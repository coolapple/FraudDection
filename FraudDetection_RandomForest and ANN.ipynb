{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week9_RandomForest_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBS0ybHVXbyA",
        "colab_type": "text"
      },
      "source": [
        "# Insurance Fraud Dection using Random Forest and Artificial Neural Network\n",
        "\n",
        "1. Show data cleaning and preparation steps\n",
        "\n",
        "2. Perform EDA on the given dataset and list out findings\n",
        "\n",
        "3. Predict the fraud projection with random tree algorithms - Use accuracy as metrics and provide best possible accuracy.\n",
        "\n",
        "4. Predict the fraud projection with artificial neural network - Use accuracy as metrics and provide best possible accuracy.\n",
        "\n",
        "5. Overall random forest achieved better accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBCedotafa9X",
        "colab_type": "text"
      },
      "source": [
        "![](https://cdn-images-1.medium.com/max/2000/1*iIx75bixLRbE45mXkH2oCg.png/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4syv3jgZYSm",
        "colab_type": "text"
      },
      "source": [
        "## Import needed dependencies :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syg4fgOvQALJ",
        "colab_type": "code",
        "outputId": "8ef2c57f-8008-4950-da61-c2d909a5ac30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTLE1A0ZbTDd",
        "colab_type": "text"
      },
      "source": [
        "## Load the preprocessed dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CXOZ-ab2Gq",
        "colab_type": "text"
      },
      "source": [
        "Download the preprocessed dataset [Download](https://drive.google.com/file/d/1rzbDYv3tYLQ7J-P3cgG7mHVwxWzgBwdr/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25vyVTxdLOW4",
        "colab_type": "code",
        "outputId": "143d0fe9-d9ac-4ceb-80b8-a5e5cd6a1ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB5raxYN6VI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_cat_column(feature_name, vocab):\n",
        "  return fc.indicator_column(\n",
        "      fc.categorical_column_with_vocabulary_list(feature_name,\n",
        "                                                 vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWdZnqW7KPrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoding categorical data\n",
        "# Encoding the Independent Variable\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "def One_hot_cat_column(df_data, feature_names):\n",
        "    df_results = pd.DataFrame()\n",
        "    for i in range(len(feature_names)):\n",
        "        #labelencoder_X = LabelEncoder()\n",
        "        #enc = labelencoder_X.fit_transform(df_data.loc[:,feature_names[i]])\n",
        "        enc = OneHotEncoder(handle_unknown='ignore')\n",
        "        enc_df = pd.DataFrame(enc.fit_transform(df_data.loc[:,[feature_names[i]]]).toarray())\n",
        "        df_results = pd.concat([enc_df, df_results], axis = 1)\n",
        "         #labelencoder_X = LabelEncoder()\n",
        "        \n",
        "\n",
        "        #enc = OneHotEncoder(handle_unknown='ignore')\n",
        "       \n",
        "\n",
        "    return df_results\n",
        "        \n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCvY3H0KuFQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Dataset/Week9/insurance_claims.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcXbDTwAuFZq",
        "colab_type": "code",
        "outputId": "17a9f271-106d-4189-bb19-e9f3c2c50906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['months_as_customer', 'age', 'policy_number', 'policy_bind_date',\n",
              "       'policy_state', 'policy_csl', 'policy_deductable',\n",
              "       'policy_annual_premium', 'umbrella_limit', 'insured_zip', 'insured_sex',\n",
              "       'insured_education_level', 'insured_occupation', 'insured_hobbies',\n",
              "       'insured_relationship', 'capital-gains', 'capital-loss',\n",
              "       'incident_date', 'incident_type', 'collision_type', 'incident_severity',\n",
              "       'authorities_contacted', 'incident_state', 'incident_city',\n",
              "       'incident_location', 'incident_hour_of_the_day',\n",
              "       'number_of_vehicles_involved', 'property_damage', 'bodily_injuries',\n",
              "       'witnesses', 'police_report_available', 'total_claim_amount',\n",
              "       'injury_claim', 'property_claim', 'vehicle_claim', 'auto_make',\n",
              "       'auto_model', 'auto_year', 'fraud_reported', '_c39'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt6-5R5_t_RH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_example = ['insured_education_level', 'insured_occupation']\n",
        "df_example_onehotcoding =  One_hot_cat_column(data, feature_names=feature_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMBf82ccur7t",
        "colab_type": "code",
        "outputId": "14bfdd8c-d6b6-4825-84d7-fce06a74ce5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df_example_onehotcoding.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3    4    5    6  ...    0    1    2    3    4    5    6\n",
              "0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
              "3  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkMENiwg7CUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataclean(datafile):\n",
        "    data = pd.read_csv(datafile)\n",
        "    data_Fraud = data[data['fraud_reported']=='Y']\n",
        "    data_NonFraud =  data[data['fraud_reported']=='N']    \n",
        "    data['fraud_reported'].value_counts()\n",
        "    df_fraud_example = pd.concat([data_Fraud, data_Fraud, data_Fraud], axis = 0)\n",
        "    data_balanced = pd.concat([data_NonFraud, df_fraud_example], axis = 0)\n",
        "    data_balanced = data_balanced.sample(frac=1).reset_index(drop=True)\n",
        "    data_balanced['fraud_reported'] = np.where(data_balanced['fraud_reported']=='Y', 1, 0)\n",
        "    data_balanced['insured_sex'] = np.where(data_balanced['insured_sex']=='MALE', 1, 0)\n",
        "\n",
        "    data_balanced['incident_date'] = pd.to_datetime(data_balanced['incident_date'])\n",
        "    data_balanced['policy_bind_date'] = pd.to_datetime(data_balanced['policy_bind_date'])\n",
        "\n",
        "    data_balanced['accd_month_since'] = round(pd.to_numeric((data_balanced['incident_date']  - data_balanced['policy_bind_date']) / np.timedelta64(1, 'M')))\n",
        "    data_balanced.drop(columns = ['policy_number', 'policy_bind_date', 'incident_date', 'incident_location', 'insured_zip', '_c39'], axis = 1, inplace=True)\n",
        "    num_columns = ['months_as_customer', 'age','accd_month_since', 'capital-gains','capital-loss', 'total_claim_amount', 'injury_claim',\n",
        "       'property_claim', 'vehicle_claim','auto_year']\n",
        "    cat_columns = [ 'policy_state', 'policy_csl','insured_education_level', 'insured_occupation', 'insured_hobbies', 'insured_relationship','insured_sex',\n",
        "                       'incident_type', 'collision_type', 'incident_severity','authorities_contacted', 'incident_state', 'incident_city', 'property_damage', \n",
        "                       'police_report_available', 'auto_make']\n",
        "    selectedcolumns = num_columns + cat_columns\n",
        "    \n",
        "    df_cat = One_hot_cat_column(data_balanced, cat_columns)\n",
        "\n",
        "    data_encoding = pd.concat([data_balanced[num_columns], df_cat], axis = 1)\n",
        "\n",
        "    #dftrain =data_encoding.iloc[:1200, :]\n",
        "    #dfeval = data_encoding.iloc[1200:,:]\n",
        "    df_train = data_encoding\n",
        "    y_train = data_balanced.loc[:, ['fraud_reported']].values\n",
        "    #y_eval = data_balanced.loc[1200:, ['fraud_reported']].values\n",
        "    #dftrain = dftrain[selectedcolumns]\n",
        "    #dfeval = dfeval[selectedcolumns]\n",
        "\n",
        "    return df_train, y_train \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yumI2UU1LX8H",
        "colab_type": "code",
        "outputId": "935fc021-f790-4930-aab0-1543b5bde880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4ArvskdLMjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datafile = \"/content/drive/My Drive/Colab Notebooks/Dataset/Week9/insurance_claims.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgMAnqkCvkVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train, y_train = dataclean(datafile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pf_uuWPXQ3e",
        "colab_type": "code",
        "outputId": "fd33ff5c-d2f9-4c0b-cb0e-86e850f41bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>months_as_customer</th>\n",
              "      <th>age</th>\n",
              "      <th>accd_month_since</th>\n",
              "      <th>capital-gains</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>total_claim_amount</th>\n",
              "      <th>injury_claim</th>\n",
              "      <th>property_claim</th>\n",
              "      <th>vehicle_claim</th>\n",
              "      <th>auto_year</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>...</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>160</td>\n",
              "      <td>33</td>\n",
              "      <td>71.0</td>\n",
              "      <td>61600</td>\n",
              "      <td>0</td>\n",
              "      <td>52800</td>\n",
              "      <td>5280</td>\n",
              "      <td>5280</td>\n",
              "      <td>42240</td>\n",
              "      <td>2006</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>281</td>\n",
              "      <td>43</td>\n",
              "      <td>259.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60190</td>\n",
              "      <td>9260</td>\n",
              "      <td>9260</td>\n",
              "      <td>41670</td>\n",
              "      <td>1999</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>156</td>\n",
              "      <td>31</td>\n",
              "      <td>221.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59000</td>\n",
              "      <td>5900</td>\n",
              "      <td>5900</td>\n",
              "      <td>47200</td>\n",
              "      <td>2013</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>148.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>75400</td>\n",
              "      <td>11600</td>\n",
              "      <td>11600</td>\n",
              "      <td>52200</td>\n",
              "      <td>2005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>222</td>\n",
              "      <td>41</td>\n",
              "      <td>195.0</td>\n",
              "      <td>37800</td>\n",
              "      <td>-50300</td>\n",
              "      <td>61290</td>\n",
              "      <td>6810</td>\n",
              "      <td>6810</td>\n",
              "      <td>47670</td>\n",
              "      <td>1995</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 116 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   months_as_customer  age  accd_month_since  capital-gains  ...    2    0    1    2\n",
              "0                 160   33              71.0          61600  ...  1.0  1.0  0.0  0.0\n",
              "1                 281   43             259.0              0  ...  0.0  0.0  0.0  1.0\n",
              "2                 156   31             221.0              0  ...  0.0  1.0  0.0  0.0\n",
              "3                  19   29             148.0              0  ...  0.0  0.0  0.0  1.0\n",
              "4                 222   41             195.0          37800  ...  0.0  0.0  0.0  1.0\n",
              "\n",
              "[5 rows x 116 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hkXCEM5XPv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = y_train #dataset['Survived']\n",
        "X = df_train #dataset.drop(['Survived'], axis = 1)\n",
        "\n",
        "# Split the dataset to trainand test data\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea71LIIVZmTt",
        "colab_type": "text"
      },
      "source": [
        "## Set the parameters for the random forest model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZWbSS6fJdAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'bootstrap': True,\n",
        "              'min_samples_leaf': 3,\n",
        "              'n_estimators': 10, \n",
        "              'min_samples_split': 10,\n",
        "              'max_features': 'sqrt',\n",
        "              'max_depth': 6,\n",
        "              'max_leaf_nodes': None}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rshkfa_hO90w",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters of Sklearn Random forest classifier[[2]](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) :\n",
        "\n",
        "*\t**bootstrap** : boolean, optional (default=True)\n",
        "\n",
        "> Whether bootstrap samples are used when building trees.\n",
        "\n",
        "*\t**min_samples_leaf** : int, float, optional (default=1)\n",
        "\n",
        "> The minimum number of samples required to be at a leaf node:\n",
        "\n",
        "> - If int, then consider min_samples_leaf as the minimum number.\n",
        "\n",
        "> - If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
        "\n",
        "* **n_estimators** : integer, optional (default=10)\n",
        "> The number of trees in the forest.\n",
        "\n",
        "* \t**min_samples_split** :  int, float, optional (default=2)\n",
        "> The minimum number of samples required to split an internal node:\n",
        "\n",
        "> - If int, then consider min_samples_split as the minimum number.\n",
        "> -\tIf float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
        "\n",
        "*\t**max_features** : int, float, string or None, optional (default=”auto”)\n",
        "> The number of features to consider when looking for the best split:\n",
        "\n",
        "> -\tIf int, then consider max_features features at each split.\n",
        "> -If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n",
        "> -\tIf “auto”, then max_features=sqrt(n_features).\n",
        "> -\tIf “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
        "> -\tIf “log2”, then max_features=log2(n_features).\n",
        "> -\tIf None, then max_features=n_features.\n",
        "\n",
        "\n",
        "*\t**max_depth** :  integer or None, optional (default=None)\n",
        "> The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "\n",
        "\n",
        "*\t**max_leaf_nodes** : int or None, optional (default=None)\n",
        "> Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
        "\n",
        "\n",
        "If you want to learn more about the rest of hyperparameters , check out  [sklearn.ensemble.RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op9F-SETZyfP",
        "colab_type": "text"
      },
      "source": [
        "## Define the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrlHxayCZxqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RF_model = RandomForestClassifier(**parameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6LYxmnZ60C",
        "colab_type": "text"
      },
      "source": [
        "## Train the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWiTrBJmaEn8",
        "colab_type": "code",
        "outputId": "774b2b86-2957-4124-acc7-dfe731a4292e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "RF_model.fit(train_X, train_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=6, max_features='sqrt',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=3, min_samples_split=10,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC4sNiZJaf0U",
        "colab_type": "text"
      },
      "source": [
        "## Test the trained model on test data :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpQcS6O1J-Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RF_predictions = RF_model.predict(test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd8y9SjiKONS",
        "colab_type": "code",
        "outputId": "9b140244-0ae0-41e0-991d-6d818f8b3264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score = accuracy_score(test_y ,RF_predictions)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8288770053475936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BctwN8hC_aN",
        "colab_type": "code",
        "outputId": "633b5660-37da-44c1-a6f2-648144630ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "scores_all = []\n",
        "treeNums =[]\n",
        "for i in range(10):\n",
        "    parameters = {'bootstrap': True,\n",
        "              'min_samples_leaf': 3,\n",
        "              'n_estimators': (i+1)*20, \n",
        "              'min_samples_split': 10,\n",
        "              'max_features': 'sqrt',\n",
        "              'max_depth': 6,\n",
        "              'max_leaf_nodes': None}\n",
        "\n",
        "    RF_model = RandomForestClassifier(**parameters)\n",
        "    RF_model.fit(train_X, train_y)\n",
        "    RF_predictions = RF_model.predict(test_X)\n",
        "    score = accuracy_score(test_y ,RF_predictions)\n",
        "    treeNums.append((i+1)*20)\n",
        "    scores_all.append(score)\n",
        "    print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8315508021390374\n",
            "0.8689839572192514\n",
            "0.8743315508021391\n",
            "0.8716577540106952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8716577540106952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8636363636363636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8716577540106952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8663101604278075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8716577540106952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8689839572192514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzD-GpLODr3W",
        "colab_type": "code",
        "outputId": "dd5b59e2-ff83-4f63-89cc-2d046cc02db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title(\"accuracy against Trees\")\n",
        "plt.plot(treeNums, scores_all)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5414d29550>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqXMOjt2EvVX",
        "colab_type": "code",
        "outputId": "8a15264b-3c42-46c0-d574-86cce767af12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "scores_all = []\n",
        "max_depth_all =[]\n",
        "scores_train =[]\n",
        "for i in range(20):\n",
        "    parameters = {'bootstrap': True,\n",
        "              'min_samples_leaf': 3,\n",
        "              'n_estimators': 100, \n",
        "              'min_samples_split': 10,\n",
        "              'max_features': 'sqrt',\n",
        "              'max_depth': i + 2,\n",
        "              'max_leaf_nodes': None}\n",
        "\n",
        "    RF_model = RandomForestClassifier(**parameters)\n",
        "    RF_model.fit(train_X, train_y)\n",
        "    RF_predictions_train = RF_model.predict(train_X)\n",
        "    scores_train.append(accuracy_score(train_y ,RF_predictions_train))\n",
        "    RF_predictions = RF_model.predict(test_X)\n",
        "    score = accuracy_score(test_y ,RF_predictions)\n",
        "    max_depth_all.append((i+2))\n",
        "    scores_all.append(score)\n",
        "    print(score)\n",
        "\n",
        "plt.title(\"accuracy against depth of Trees\")\n",
        "plt.plot(max_depth_all, scores_all)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8262032085561497\n",
            "0.820855614973262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8449197860962567\n",
            "0.8475935828877005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8716577540106952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8663101604278075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8743315508021391\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8796791443850267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8850267379679144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8903743315508021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.893048128342246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8983957219251337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8957219251336899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8983957219251337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9064171122994652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.893048128342246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9037433155080213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8983957219251337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9117647058823529\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9037433155080213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5414728f28>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lnL8pg1Gpjo",
        "colab_type": "code",
        "outputId": "6758152d-9bbf-4553-c2e7-88c9019c3791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.title(\"accuracy for both train and test\")\n",
        "plt.xlabel(\"accuracy of test set\")\n",
        "plt.ylabel(\"accuracy of train set\")\n",
        "plt.plot(scores_all,scores_train)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5414603940>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtYWSjJ5asha",
        "colab_type": "text"
      },
      "source": [
        "![](https://)We see that the model's accuracy is  86%, not bad at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCM3DYk0NO1z",
        "colab_type": "text"
      },
      "source": [
        "#Using Neural Networks:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmoxQ0K7dYU_",
        "colab_type": "text"
      },
      "source": [
        "## Define the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjHxqy9o29IR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a neural network :\n",
        "NN_model = Sequential()\n",
        "\n",
        "NN_model.add(Dense(512, input_dim = 116, activation='tanh'))\n",
        "NN_model.add(Dense(512, activation='tanh'))\n",
        "NN_model.add(Dense(256, activation='relu'))\n",
        "NN_model.add(Dense(256, activation='relu'))\n",
        "NN_model.add(Dense(256, activation='relu'))\n",
        "NN_model.add(Dense(256, activation='relu'))\n",
        "NN_model.add(Dense(128, activation='relu'))\n",
        "NN_model.add(Dense(64, activation='relu'))\n",
        "NN_model.add(Dense(1, activation='sigmoid'))\n",
        "NN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHFsE2AIPw_h",
        "colab_type": "text"
      },
      "source": [
        "Define a checkpoint callback :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEw-m-utHEC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#checkpoint_name = 'Weights-{epoch:03d}-{val_acc:.5f}.hdf5' \n",
        "#checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_accuracy', verbose = 1, save_best_only = True, mode ='max')\n",
        "#callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PapFqg3bdYVD",
        "colab_type": "text"
      },
      "source": [
        "## Train the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GkUvBi-EETO",
        "colab_type": "code",
        "outputId": "a22bb229-79e6-4f30-e091-ffa0c9ddae2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = NN_model.fit(train_X, train_y, epochs=1000, batch_size=64, validation_data= (test_X, test_y) )# validation_split = 0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1120 samples, validate on 374 samples\n",
            "Epoch 1/1000\n",
            "1120/1120 [==============================] - 1s 452us/step - loss: 0.6960 - accuracy: 0.5429 - val_loss: 0.6917 - val_accuracy: 0.4920\n",
            "Epoch 2/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.6780 - accuracy: 0.5518 - val_loss: 0.6863 - val_accuracy: 0.5481\n",
            "Epoch 3/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.6777 - accuracy: 0.5339 - val_loss: 0.6914 - val_accuracy: 0.5027\n",
            "Epoch 4/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.6739 - accuracy: 0.5670 - val_loss: 0.6853 - val_accuracy: 0.5829\n",
            "Epoch 5/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.6734 - accuracy: 0.5536 - val_loss: 0.6860 - val_accuracy: 0.5695\n",
            "Epoch 6/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.6815 - accuracy: 0.5402 - val_loss: 0.6822 - val_accuracy: 0.5882\n",
            "Epoch 7/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.6709 - accuracy: 0.5562 - val_loss: 0.6804 - val_accuracy: 0.5615\n",
            "Epoch 8/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.6692 - accuracy: 0.5768 - val_loss: 0.6752 - val_accuracy: 0.5856\n",
            "Epoch 9/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.6627 - accuracy: 0.5911 - val_loss: 0.6705 - val_accuracy: 0.6016\n",
            "Epoch 10/1000\n",
            "1120/1120 [==============================] - 0s 269us/step - loss: 0.6619 - accuracy: 0.6134 - val_loss: 0.7014 - val_accuracy: 0.5455\n",
            "Epoch 11/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.6669 - accuracy: 0.5670 - val_loss: 0.6831 - val_accuracy: 0.5668\n",
            "Epoch 12/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.6573 - accuracy: 0.5777 - val_loss: 0.7759 - val_accuracy: 0.4840\n",
            "Epoch 13/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.6672 - accuracy: 0.5786 - val_loss: 0.6825 - val_accuracy: 0.5695\n",
            "Epoch 14/1000\n",
            "1120/1120 [==============================] - 0s 271us/step - loss: 0.6589 - accuracy: 0.6045 - val_loss: 0.6769 - val_accuracy: 0.5936\n",
            "Epoch 15/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.6602 - accuracy: 0.5723 - val_loss: 0.6722 - val_accuracy: 0.5963\n",
            "Epoch 16/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.6480 - accuracy: 0.6179 - val_loss: 0.6832 - val_accuracy: 0.5802\n",
            "Epoch 17/1000\n",
            "1120/1120 [==============================] - 0s 264us/step - loss: 0.6464 - accuracy: 0.6161 - val_loss: 0.6916 - val_accuracy: 0.5802\n",
            "Epoch 18/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.6522 - accuracy: 0.6187 - val_loss: 0.6757 - val_accuracy: 0.5802\n",
            "Epoch 19/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.6446 - accuracy: 0.6143 - val_loss: 0.6582 - val_accuracy: 0.5963\n",
            "Epoch 20/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.6505 - accuracy: 0.5938 - val_loss: 0.6675 - val_accuracy: 0.5882\n",
            "Epoch 21/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.6355 - accuracy: 0.6116 - val_loss: 0.8570 - val_accuracy: 0.4893\n",
            "Epoch 22/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.6519 - accuracy: 0.5848 - val_loss: 0.6770 - val_accuracy: 0.5802\n",
            "Epoch 23/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.6422 - accuracy: 0.6357 - val_loss: 0.6614 - val_accuracy: 0.5882\n",
            "Epoch 24/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.6438 - accuracy: 0.6241 - val_loss: 0.6735 - val_accuracy: 0.5989\n",
            "Epoch 25/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.6517 - accuracy: 0.6054 - val_loss: 0.6752 - val_accuracy: 0.5936\n",
            "Epoch 26/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.6163 - accuracy: 0.6455 - val_loss: 0.6659 - val_accuracy: 0.6444\n",
            "Epoch 27/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.6381 - accuracy: 0.5938 - val_loss: 0.6723 - val_accuracy: 0.6123\n",
            "Epoch 28/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.6239 - accuracy: 0.6375 - val_loss: 0.6738 - val_accuracy: 0.6390\n",
            "Epoch 29/1000\n",
            "1120/1120 [==============================] - 0s 263us/step - loss: 0.5994 - accuracy: 0.6562 - val_loss: 0.6739 - val_accuracy: 0.5668\n",
            "Epoch 30/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.6044 - accuracy: 0.6366 - val_loss: 0.6839 - val_accuracy: 0.5535\n",
            "Epoch 31/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.6213 - accuracy: 0.6384 - val_loss: 0.6530 - val_accuracy: 0.6257\n",
            "Epoch 32/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.6411 - accuracy: 0.6134 - val_loss: 0.6712 - val_accuracy: 0.5856\n",
            "Epoch 33/1000\n",
            "1120/1120 [==============================] - 0s 258us/step - loss: 0.6128 - accuracy: 0.6429 - val_loss: 0.6679 - val_accuracy: 0.6203\n",
            "Epoch 34/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.5788 - accuracy: 0.6741 - val_loss: 0.6632 - val_accuracy: 0.5936\n",
            "Epoch 35/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.5905 - accuracy: 0.6705 - val_loss: 0.7273 - val_accuracy: 0.5481\n",
            "Epoch 36/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.5862 - accuracy: 0.6652 - val_loss: 0.6383 - val_accuracy: 0.6310\n",
            "Epoch 37/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.5858 - accuracy: 0.6527 - val_loss: 0.6720 - val_accuracy: 0.5749\n",
            "Epoch 38/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.5838 - accuracy: 0.6545 - val_loss: 0.6506 - val_accuracy: 0.6230\n",
            "Epoch 39/1000\n",
            "1120/1120 [==============================] - 0s 260us/step - loss: 0.6100 - accuracy: 0.6562 - val_loss: 0.6958 - val_accuracy: 0.5481\n",
            "Epoch 40/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.5691 - accuracy: 0.6786 - val_loss: 0.7215 - val_accuracy: 0.5882\n",
            "Epoch 41/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.5595 - accuracy: 0.6982 - val_loss: 0.6671 - val_accuracy: 0.6283\n",
            "Epoch 42/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.5568 - accuracy: 0.6875 - val_loss: 0.6499 - val_accuracy: 0.5615\n",
            "Epoch 43/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.5825 - accuracy: 0.7036 - val_loss: 0.7040 - val_accuracy: 0.5909\n",
            "Epoch 44/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.5501 - accuracy: 0.7036 - val_loss: 0.6353 - val_accuracy: 0.6444\n",
            "Epoch 45/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.5701 - accuracy: 0.6750 - val_loss: 0.6355 - val_accuracy: 0.6390\n",
            "Epoch 46/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.5516 - accuracy: 0.6991 - val_loss: 0.6983 - val_accuracy: 0.5401\n",
            "Epoch 47/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.5424 - accuracy: 0.7027 - val_loss: 0.6692 - val_accuracy: 0.6310\n",
            "Epoch 48/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.5424 - accuracy: 0.7018 - val_loss: 0.6750 - val_accuracy: 0.6444\n",
            "Epoch 49/1000\n",
            "1120/1120 [==============================] - 0s 231us/step - loss: 0.5062 - accuracy: 0.7259 - val_loss: 0.6983 - val_accuracy: 0.6043\n",
            "Epoch 50/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.5224 - accuracy: 0.6973 - val_loss: 0.6336 - val_accuracy: 0.6176\n",
            "Epoch 51/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.5531 - accuracy: 0.6848 - val_loss: 0.6630 - val_accuracy: 0.6631\n",
            "Epoch 52/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.5199 - accuracy: 0.6964 - val_loss: 0.7337 - val_accuracy: 0.6283\n",
            "Epoch 53/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.4992 - accuracy: 0.7161 - val_loss: 0.6974 - val_accuracy: 0.6203\n",
            "Epoch 54/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.4968 - accuracy: 0.7250 - val_loss: 0.7318 - val_accuracy: 0.6765\n",
            "Epoch 55/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.5673 - accuracy: 0.6714 - val_loss: 0.7079 - val_accuracy: 0.5882\n",
            "Epoch 56/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.5396 - accuracy: 0.7196 - val_loss: 0.7583 - val_accuracy: 0.6070\n",
            "Epoch 57/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.5302 - accuracy: 0.7161 - val_loss: 0.6867 - val_accuracy: 0.6337\n",
            "Epoch 58/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.5296 - accuracy: 0.6982 - val_loss: 0.6674 - val_accuracy: 0.6257\n",
            "Epoch 59/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.5411 - accuracy: 0.7018 - val_loss: 0.6528 - val_accuracy: 0.6497\n",
            "Epoch 60/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.4934 - accuracy: 0.7295 - val_loss: 0.7404 - val_accuracy: 0.6551\n",
            "Epoch 61/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.4825 - accuracy: 0.7321 - val_loss: 0.7061 - val_accuracy: 0.6604\n",
            "Epoch 62/1000\n",
            "1120/1120 [==============================] - 0s 261us/step - loss: 0.4728 - accuracy: 0.7402 - val_loss: 0.7552 - val_accuracy: 0.6551\n",
            "Epoch 63/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.5128 - accuracy: 0.7196 - val_loss: 0.6508 - val_accuracy: 0.6497\n",
            "Epoch 64/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.5201 - accuracy: 0.7018 - val_loss: 0.6613 - val_accuracy: 0.6203\n",
            "Epoch 65/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.4868 - accuracy: 0.7455 - val_loss: 1.0354 - val_accuracy: 0.6043\n",
            "Epoch 66/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.5157 - accuracy: 0.7063 - val_loss: 0.6867 - val_accuracy: 0.6551\n",
            "Epoch 67/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.5111 - accuracy: 0.7214 - val_loss: 0.7226 - val_accuracy: 0.6471\n",
            "Epoch 68/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.4839 - accuracy: 0.7330 - val_loss: 0.6816 - val_accuracy: 0.6364\n",
            "Epoch 69/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.4652 - accuracy: 0.7616 - val_loss: 0.7615 - val_accuracy: 0.6444\n",
            "Epoch 70/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.5455 - accuracy: 0.6902 - val_loss: 0.6470 - val_accuracy: 0.6176\n",
            "Epoch 71/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.5247 - accuracy: 0.6991 - val_loss: 0.7020 - val_accuracy: 0.6016\n",
            "Epoch 72/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.4969 - accuracy: 0.7188 - val_loss: 0.6852 - val_accuracy: 0.6337\n",
            "Epoch 73/1000\n",
            "1120/1120 [==============================] - 0s 258us/step - loss: 0.4727 - accuracy: 0.7455 - val_loss: 0.8251 - val_accuracy: 0.5989\n",
            "Epoch 74/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.5136 - accuracy: 0.7241 - val_loss: 0.6399 - val_accuracy: 0.6123\n",
            "Epoch 75/1000\n",
            "1120/1120 [==============================] - 0s 258us/step - loss: 0.4892 - accuracy: 0.7482 - val_loss: 1.1218 - val_accuracy: 0.5588\n",
            "Epoch 76/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.5563 - accuracy: 0.6991 - val_loss: 0.6189 - val_accuracy: 0.6364\n",
            "Epoch 77/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.5174 - accuracy: 0.6973 - val_loss: 0.6666 - val_accuracy: 0.6604\n",
            "Epoch 78/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.4535 - accuracy: 0.7527 - val_loss: 0.8016 - val_accuracy: 0.6337\n",
            "Epoch 79/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.4753 - accuracy: 0.7384 - val_loss: 0.6955 - val_accuracy: 0.6283\n",
            "Epoch 80/1000\n",
            "1120/1120 [==============================] - 0s 258us/step - loss: 0.4822 - accuracy: 0.7295 - val_loss: 0.7772 - val_accuracy: 0.6497\n",
            "Epoch 81/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.4649 - accuracy: 0.7482 - val_loss: 0.7349 - val_accuracy: 0.6444\n",
            "Epoch 82/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.4380 - accuracy: 0.7670 - val_loss: 0.7167 - val_accuracy: 0.6471\n",
            "Epoch 83/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.4688 - accuracy: 0.7366 - val_loss: 0.7325 - val_accuracy: 0.6551\n",
            "Epoch 84/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.4362 - accuracy: 0.7563 - val_loss: 0.7470 - val_accuracy: 0.6765\n",
            "Epoch 85/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.4235 - accuracy: 0.7732 - val_loss: 0.8231 - val_accuracy: 0.6364\n",
            "Epoch 86/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.4789 - accuracy: 0.7312 - val_loss: 0.6845 - val_accuracy: 0.6738\n",
            "Epoch 87/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.4501 - accuracy: 0.7598 - val_loss: 0.7980 - val_accuracy: 0.6711\n",
            "Epoch 88/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.4467 - accuracy: 0.7705 - val_loss: 0.7056 - val_accuracy: 0.6738\n",
            "Epoch 89/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.4144 - accuracy: 0.7732 - val_loss: 0.7891 - val_accuracy: 0.6738\n",
            "Epoch 90/1000\n",
            "1120/1120 [==============================] - 0s 229us/step - loss: 0.4282 - accuracy: 0.7723 - val_loss: 0.7241 - val_accuracy: 0.6417\n",
            "Epoch 91/1000\n",
            "1120/1120 [==============================] - 0s 267us/step - loss: 0.4586 - accuracy: 0.7652 - val_loss: 0.6872 - val_accuracy: 0.6390\n",
            "Epoch 92/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.4537 - accuracy: 0.7304 - val_loss: 0.7649 - val_accuracy: 0.6310\n",
            "Epoch 93/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.4395 - accuracy: 0.7563 - val_loss: 0.9581 - val_accuracy: 0.6096\n",
            "Epoch 94/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.4499 - accuracy: 0.7536 - val_loss: 0.7001 - val_accuracy: 0.6310\n",
            "Epoch 95/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.4378 - accuracy: 0.7571 - val_loss: 0.7773 - val_accuracy: 0.6390\n",
            "Epoch 96/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.4207 - accuracy: 0.7679 - val_loss: 0.8009 - val_accuracy: 0.6417\n",
            "Epoch 97/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.4025 - accuracy: 0.7804 - val_loss: 0.8488 - val_accuracy: 0.6364\n",
            "Epoch 98/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.4500 - accuracy: 0.7446 - val_loss: 0.8136 - val_accuracy: 0.6230\n",
            "Epoch 99/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.4401 - accuracy: 0.7491 - val_loss: 0.7631 - val_accuracy: 0.6711\n",
            "Epoch 100/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.4543 - accuracy: 0.7366 - val_loss: 0.7919 - val_accuracy: 0.6711\n",
            "Epoch 101/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.4449 - accuracy: 0.7500 - val_loss: 0.7838 - val_accuracy: 0.6738\n",
            "Epoch 102/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.4560 - accuracy: 0.7518 - val_loss: 0.7559 - val_accuracy: 0.6684\n",
            "Epoch 103/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.4714 - accuracy: 0.7491 - val_loss: 0.7103 - val_accuracy: 0.6176\n",
            "Epoch 104/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.5528 - accuracy: 0.6893 - val_loss: 0.6322 - val_accuracy: 0.6176\n",
            "Epoch 105/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.4935 - accuracy: 0.7232 - val_loss: 0.7018 - val_accuracy: 0.6497\n",
            "Epoch 106/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.4382 - accuracy: 0.7598 - val_loss: 0.6923 - val_accuracy: 0.6337\n",
            "Epoch 107/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.4220 - accuracy: 0.7652 - val_loss: 0.7607 - val_accuracy: 0.6471\n",
            "Epoch 108/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.4592 - accuracy: 0.7348 - val_loss: 0.6501 - val_accuracy: 0.6631\n",
            "Epoch 109/1000\n",
            "1120/1120 [==============================] - 0s 263us/step - loss: 0.4521 - accuracy: 0.7473 - val_loss: 0.8135 - val_accuracy: 0.6497\n",
            "Epoch 110/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.4316 - accuracy: 0.7634 - val_loss: 0.7984 - val_accuracy: 0.6390\n",
            "Epoch 111/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.4150 - accuracy: 0.7741 - val_loss: 0.7978 - val_accuracy: 0.6444\n",
            "Epoch 112/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.4705 - accuracy: 0.7393 - val_loss: 0.7488 - val_accuracy: 0.6765\n",
            "Epoch 113/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.4314 - accuracy: 0.7589 - val_loss: 0.8118 - val_accuracy: 0.6337\n",
            "Epoch 114/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.4314 - accuracy: 0.7670 - val_loss: 0.8207 - val_accuracy: 0.6684\n",
            "Epoch 115/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.4319 - accuracy: 0.7652 - val_loss: 0.7082 - val_accuracy: 0.6471\n",
            "Epoch 116/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.4396 - accuracy: 0.7491 - val_loss: 0.7734 - val_accuracy: 0.5963\n",
            "Epoch 117/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.4100 - accuracy: 0.7732 - val_loss: 0.8943 - val_accuracy: 0.6578\n",
            "Epoch 118/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.4476 - accuracy: 0.7527 - val_loss: 0.8166 - val_accuracy: 0.6604\n",
            "Epoch 119/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.4564 - accuracy: 0.7643 - val_loss: 0.7227 - val_accuracy: 0.6578\n",
            "Epoch 120/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.4365 - accuracy: 0.7643 - val_loss: 0.8102 - val_accuracy: 0.6444\n",
            "Epoch 121/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.4191 - accuracy: 0.7652 - val_loss: 0.9007 - val_accuracy: 0.5989\n",
            "Epoch 122/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.4525 - accuracy: 0.7446 - val_loss: 0.7902 - val_accuracy: 0.6604\n",
            "Epoch 123/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.3991 - accuracy: 0.7920 - val_loss: 0.8551 - val_accuracy: 0.6658\n",
            "Epoch 124/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.4317 - accuracy: 0.7732 - val_loss: 0.7951 - val_accuracy: 0.6203\n",
            "Epoch 125/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.4886 - accuracy: 0.7241 - val_loss: 0.7494 - val_accuracy: 0.6578\n",
            "Epoch 126/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.4387 - accuracy: 0.7616 - val_loss: 0.6978 - val_accuracy: 0.6471\n",
            "Epoch 127/1000\n",
            "1120/1120 [==============================] - 0s 279us/step - loss: 0.4362 - accuracy: 0.7589 - val_loss: 0.7552 - val_accuracy: 0.6471\n",
            "Epoch 128/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.3980 - accuracy: 0.7866 - val_loss: 0.7851 - val_accuracy: 0.6551\n",
            "Epoch 129/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.4100 - accuracy: 0.7750 - val_loss: 0.8405 - val_accuracy: 0.6337\n",
            "Epoch 130/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.4449 - accuracy: 0.7607 - val_loss: 0.7326 - val_accuracy: 0.6176\n",
            "Epoch 131/1000\n",
            "1120/1120 [==============================] - 0s 265us/step - loss: 0.4699 - accuracy: 0.7545 - val_loss: 0.7230 - val_accuracy: 0.6551\n",
            "Epoch 132/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.4085 - accuracy: 0.7723 - val_loss: 0.8934 - val_accuracy: 0.6203\n",
            "Epoch 133/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.4068 - accuracy: 0.7857 - val_loss: 0.8247 - val_accuracy: 0.6578\n",
            "Epoch 134/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.3850 - accuracy: 0.8000 - val_loss: 0.8225 - val_accuracy: 0.6631\n",
            "Epoch 135/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.3876 - accuracy: 0.7955 - val_loss: 0.9146 - val_accuracy: 0.6631\n",
            "Epoch 136/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.4007 - accuracy: 0.7768 - val_loss: 0.8123 - val_accuracy: 0.6658\n",
            "Epoch 137/1000\n",
            "1120/1120 [==============================] - 0s 263us/step - loss: 0.4421 - accuracy: 0.7643 - val_loss: 0.7390 - val_accuracy: 0.6658\n",
            "Epoch 138/1000\n",
            "1120/1120 [==============================] - 0s 261us/step - loss: 0.4310 - accuracy: 0.7563 - val_loss: 0.8488 - val_accuracy: 0.6578\n",
            "Epoch 139/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.4203 - accuracy: 0.7705 - val_loss: 0.8015 - val_accuracy: 0.6738\n",
            "Epoch 140/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.4398 - accuracy: 0.7402 - val_loss: 0.6741 - val_accuracy: 0.6791\n",
            "Epoch 141/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.4912 - accuracy: 0.7411 - val_loss: 0.6886 - val_accuracy: 0.6765\n",
            "Epoch 142/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.4270 - accuracy: 0.7679 - val_loss: 0.9434 - val_accuracy: 0.6364\n",
            "Epoch 143/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.3857 - accuracy: 0.7946 - val_loss: 0.8950 - val_accuracy: 0.6631\n",
            "Epoch 144/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.3944 - accuracy: 0.7955 - val_loss: 0.8243 - val_accuracy: 0.6791\n",
            "Epoch 145/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.3735 - accuracy: 0.8080 - val_loss: 0.9258 - val_accuracy: 0.6497\n",
            "Epoch 146/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.3742 - accuracy: 0.8071 - val_loss: 0.9106 - val_accuracy: 0.6765\n",
            "Epoch 147/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.4115 - accuracy: 0.7714 - val_loss: 0.8762 - val_accuracy: 0.6524\n",
            "Epoch 148/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.3926 - accuracy: 0.7857 - val_loss: 0.9126 - val_accuracy: 0.6604\n",
            "Epoch 149/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.4393 - accuracy: 0.7491 - val_loss: 0.7721 - val_accuracy: 0.6417\n",
            "Epoch 150/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.4311 - accuracy: 0.7804 - val_loss: 0.8152 - val_accuracy: 0.6631\n",
            "Epoch 151/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.4014 - accuracy: 0.7750 - val_loss: 0.7865 - val_accuracy: 0.6952\n",
            "Epoch 152/1000\n",
            "1120/1120 [==============================] - 0s 270us/step - loss: 0.3633 - accuracy: 0.8009 - val_loss: 0.9249 - val_accuracy: 0.6765\n",
            "Epoch 153/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.3919 - accuracy: 0.7884 - val_loss: 0.8084 - val_accuracy: 0.6765\n",
            "Epoch 154/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.3602 - accuracy: 0.8134 - val_loss: 0.8957 - val_accuracy: 0.6898\n",
            "Epoch 155/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.3865 - accuracy: 0.7812 - val_loss: 0.8314 - val_accuracy: 0.6711\n",
            "Epoch 156/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.3605 - accuracy: 0.8018 - val_loss: 0.9118 - val_accuracy: 0.6658\n",
            "Epoch 157/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.4240 - accuracy: 0.7661 - val_loss: 0.6567 - val_accuracy: 0.6604\n",
            "Epoch 158/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.4213 - accuracy: 0.7723 - val_loss: 0.8018 - val_accuracy: 0.6337\n",
            "Epoch 159/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.3762 - accuracy: 0.8027 - val_loss: 0.8002 - val_accuracy: 0.6791\n",
            "Epoch 160/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.4044 - accuracy: 0.7759 - val_loss: 0.8829 - val_accuracy: 0.6738\n",
            "Epoch 161/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3928 - accuracy: 0.7786 - val_loss: 0.9284 - val_accuracy: 0.6738\n",
            "Epoch 162/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.4066 - accuracy: 0.7884 - val_loss: 0.7677 - val_accuracy: 0.6765\n",
            "Epoch 163/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.3939 - accuracy: 0.7964 - val_loss: 0.8507 - val_accuracy: 0.6578\n",
            "Epoch 164/1000\n",
            "1120/1120 [==============================] - 0s 263us/step - loss: 0.3795 - accuracy: 0.8054 - val_loss: 0.8923 - val_accuracy: 0.6150\n",
            "Epoch 165/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.3935 - accuracy: 0.7866 - val_loss: 0.7980 - val_accuracy: 0.6872\n",
            "Epoch 166/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.3416 - accuracy: 0.8179 - val_loss: 0.9202 - val_accuracy: 0.6845\n",
            "Epoch 167/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.3825 - accuracy: 0.7973 - val_loss: 0.8318 - val_accuracy: 0.6551\n",
            "Epoch 168/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.4289 - accuracy: 0.7580 - val_loss: 0.7370 - val_accuracy: 0.6738\n",
            "Epoch 169/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.4086 - accuracy: 0.7893 - val_loss: 0.7445 - val_accuracy: 0.6497\n",
            "Epoch 170/1000\n",
            "1120/1120 [==============================] - 0s 261us/step - loss: 0.4895 - accuracy: 0.7312 - val_loss: 0.6608 - val_accuracy: 0.6551\n",
            "Epoch 171/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.4020 - accuracy: 0.7875 - val_loss: 0.8708 - val_accuracy: 0.6765\n",
            "Epoch 172/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.3751 - accuracy: 0.7911 - val_loss: 0.8868 - val_accuracy: 0.7032\n",
            "Epoch 173/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3540 - accuracy: 0.8098 - val_loss: 1.0182 - val_accuracy: 0.6818\n",
            "Epoch 174/1000\n",
            "1120/1120 [==============================] - 0s 265us/step - loss: 0.4000 - accuracy: 0.7821 - val_loss: 0.7640 - val_accuracy: 0.6658\n",
            "Epoch 175/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.3839 - accuracy: 0.7839 - val_loss: 0.8383 - val_accuracy: 0.6444\n",
            "Epoch 176/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.3744 - accuracy: 0.7866 - val_loss: 0.8362 - val_accuracy: 0.7005\n",
            "Epoch 177/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.3426 - accuracy: 0.8134 - val_loss: 1.0424 - val_accuracy: 0.6765\n",
            "Epoch 178/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.3855 - accuracy: 0.7982 - val_loss: 0.8332 - val_accuracy: 0.6791\n",
            "Epoch 179/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.3927 - accuracy: 0.7982 - val_loss: 0.8075 - val_accuracy: 0.6738\n",
            "Epoch 180/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.3916 - accuracy: 0.7804 - val_loss: 0.8331 - val_accuracy: 0.6818\n",
            "Epoch 181/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.3454 - accuracy: 0.8196 - val_loss: 0.9093 - val_accuracy: 0.6818\n",
            "Epoch 182/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3453 - accuracy: 0.8205 - val_loss: 0.8997 - val_accuracy: 0.6738\n",
            "Epoch 183/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.3565 - accuracy: 0.8071 - val_loss: 0.8814 - val_accuracy: 0.6925\n",
            "Epoch 184/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.3518 - accuracy: 0.8062 - val_loss: 0.8874 - val_accuracy: 0.6925\n",
            "Epoch 185/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.3631 - accuracy: 0.8107 - val_loss: 0.8629 - val_accuracy: 0.7005\n",
            "Epoch 186/1000\n",
            "1120/1120 [==============================] - 0s 232us/step - loss: 0.3474 - accuracy: 0.8170 - val_loss: 0.9566 - val_accuracy: 0.6631\n",
            "Epoch 187/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.3477 - accuracy: 0.8027 - val_loss: 0.9406 - val_accuracy: 0.7112\n",
            "Epoch 188/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.3672 - accuracy: 0.8071 - val_loss: 1.0323 - val_accuracy: 0.6604\n",
            "Epoch 189/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.3889 - accuracy: 0.7884 - val_loss: 0.8783 - val_accuracy: 0.6952\n",
            "Epoch 190/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.3526 - accuracy: 0.8036 - val_loss: 0.9195 - val_accuracy: 0.7005\n",
            "Epoch 191/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.3340 - accuracy: 0.8268 - val_loss: 0.9832 - val_accuracy: 0.6845\n",
            "Epoch 192/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.3725 - accuracy: 0.7991 - val_loss: 0.8201 - val_accuracy: 0.6925\n",
            "Epoch 193/1000\n",
            "1120/1120 [==============================] - 0s 262us/step - loss: 0.3846 - accuracy: 0.7902 - val_loss: 0.7951 - val_accuracy: 0.6925\n",
            "Epoch 194/1000\n",
            "1120/1120 [==============================] - 0s 262us/step - loss: 0.3249 - accuracy: 0.8143 - val_loss: 1.1027 - val_accuracy: 0.6738\n",
            "Epoch 195/1000\n",
            "1120/1120 [==============================] - 0s 260us/step - loss: 0.3709 - accuracy: 0.8009 - val_loss: 0.8989 - val_accuracy: 0.6872\n",
            "Epoch 196/1000\n",
            "1120/1120 [==============================] - 0s 272us/step - loss: 0.3578 - accuracy: 0.8071 - val_loss: 0.8980 - val_accuracy: 0.7005\n",
            "Epoch 197/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.3263 - accuracy: 0.8277 - val_loss: 0.9370 - val_accuracy: 0.6872\n",
            "Epoch 198/1000\n",
            "1120/1120 [==============================] - 0s 232us/step - loss: 0.3874 - accuracy: 0.7786 - val_loss: 0.7851 - val_accuracy: 0.6979\n",
            "Epoch 199/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.3574 - accuracy: 0.8241 - val_loss: 0.8964 - val_accuracy: 0.6952\n",
            "Epoch 200/1000\n",
            "1120/1120 [==============================] - 0s 268us/step - loss: 0.4272 - accuracy: 0.7696 - val_loss: 0.6911 - val_accuracy: 0.6898\n",
            "Epoch 201/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.3692 - accuracy: 0.8098 - val_loss: 0.9060 - val_accuracy: 0.6952\n",
            "Epoch 202/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.3516 - accuracy: 0.8062 - val_loss: 0.8124 - val_accuracy: 0.6604\n",
            "Epoch 203/1000\n",
            "1120/1120 [==============================] - 0s 261us/step - loss: 0.3774 - accuracy: 0.7964 - val_loss: 0.8312 - val_accuracy: 0.6791\n",
            "Epoch 204/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.3230 - accuracy: 0.8339 - val_loss: 1.0140 - val_accuracy: 0.6872\n",
            "Epoch 205/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.3311 - accuracy: 0.8223 - val_loss: 0.8758 - val_accuracy: 0.7005\n",
            "Epoch 206/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.3287 - accuracy: 0.8232 - val_loss: 1.0805 - val_accuracy: 0.6578\n",
            "Epoch 207/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.3491 - accuracy: 0.8241 - val_loss: 0.8616 - val_accuracy: 0.6898\n",
            "Epoch 208/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.3368 - accuracy: 0.8179 - val_loss: 0.9505 - val_accuracy: 0.7139\n",
            "Epoch 209/1000\n",
            "1120/1120 [==============================] - 0s 232us/step - loss: 0.3607 - accuracy: 0.7964 - val_loss: 0.9136 - val_accuracy: 0.6604\n",
            "Epoch 210/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3629 - accuracy: 0.7911 - val_loss: 0.8765 - val_accuracy: 0.6604\n",
            "Epoch 211/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.3287 - accuracy: 0.8196 - val_loss: 0.9349 - val_accuracy: 0.6898\n",
            "Epoch 212/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.3200 - accuracy: 0.8161 - val_loss: 1.0599 - val_accuracy: 0.6845\n",
            "Epoch 213/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.3016 - accuracy: 0.8411 - val_loss: 1.0144 - val_accuracy: 0.7112\n",
            "Epoch 214/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.3073 - accuracy: 0.8321 - val_loss: 1.0087 - val_accuracy: 0.7005\n",
            "Epoch 215/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.3565 - accuracy: 0.7946 - val_loss: 0.8661 - val_accuracy: 0.6979\n",
            "Epoch 216/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.3167 - accuracy: 0.8375 - val_loss: 1.1477 - val_accuracy: 0.7059\n",
            "Epoch 217/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.3148 - accuracy: 0.8304 - val_loss: 0.9728 - val_accuracy: 0.6578\n",
            "Epoch 218/1000\n",
            "1120/1120 [==============================] - 0s 258us/step - loss: 0.3352 - accuracy: 0.8241 - val_loss: 1.1238 - val_accuracy: 0.7112\n",
            "Epoch 219/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3449 - accuracy: 0.8196 - val_loss: 0.9083 - val_accuracy: 0.7005\n",
            "Epoch 220/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.3401 - accuracy: 0.8170 - val_loss: 0.9675 - val_accuracy: 0.6471\n",
            "Epoch 221/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.3117 - accuracy: 0.8304 - val_loss: 1.1680 - val_accuracy: 0.7005\n",
            "Epoch 222/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.3537 - accuracy: 0.8098 - val_loss: 0.8633 - val_accuracy: 0.7166\n",
            "Epoch 223/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.3085 - accuracy: 0.8286 - val_loss: 1.1403 - val_accuracy: 0.6952\n",
            "Epoch 224/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.3361 - accuracy: 0.8268 - val_loss: 0.9366 - val_accuracy: 0.7059\n",
            "Epoch 225/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.3041 - accuracy: 0.8330 - val_loss: 1.1465 - val_accuracy: 0.6738\n",
            "Epoch 226/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.3327 - accuracy: 0.8134 - val_loss: 0.9344 - val_accuracy: 0.7112\n",
            "Epoch 227/1000\n",
            "1120/1120 [==============================] - 0s 227us/step - loss: 0.2945 - accuracy: 0.8339 - val_loss: 1.0713 - val_accuracy: 0.7139\n",
            "Epoch 228/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.3359 - accuracy: 0.8259 - val_loss: 0.8696 - val_accuracy: 0.7032\n",
            "Epoch 229/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.3118 - accuracy: 0.8313 - val_loss: 0.9947 - val_accuracy: 0.6979\n",
            "Epoch 230/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.3223 - accuracy: 0.8241 - val_loss: 0.9939 - val_accuracy: 0.6791\n",
            "Epoch 231/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.3505 - accuracy: 0.8125 - val_loss: 0.9340 - val_accuracy: 0.6845\n",
            "Epoch 232/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.3297 - accuracy: 0.8232 - val_loss: 1.0098 - val_accuracy: 0.6898\n",
            "Epoch 233/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3022 - accuracy: 0.8420 - val_loss: 1.0059 - val_accuracy: 0.6925\n",
            "Epoch 234/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2843 - accuracy: 0.8571 - val_loss: 1.1929 - val_accuracy: 0.7139\n",
            "Epoch 235/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.3030 - accuracy: 0.8268 - val_loss: 0.9677 - val_accuracy: 0.7139\n",
            "Epoch 236/1000\n",
            "1120/1120 [==============================] - 0s 266us/step - loss: 0.2900 - accuracy: 0.8455 - val_loss: 1.2302 - val_accuracy: 0.7112\n",
            "Epoch 237/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.3027 - accuracy: 0.8286 - val_loss: 1.0648 - val_accuracy: 0.7032\n",
            "Epoch 238/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.3251 - accuracy: 0.8152 - val_loss: 0.9561 - val_accuracy: 0.7166\n",
            "Epoch 239/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.3254 - accuracy: 0.8188 - val_loss: 0.9818 - val_accuracy: 0.7166\n",
            "Epoch 240/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2917 - accuracy: 0.8375 - val_loss: 1.1461 - val_accuracy: 0.7219\n",
            "Epoch 241/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2754 - accuracy: 0.8384 - val_loss: 1.2618 - val_accuracy: 0.7059\n",
            "Epoch 242/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.3141 - accuracy: 0.8179 - val_loss: 1.2018 - val_accuracy: 0.6979\n",
            "Epoch 243/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.3043 - accuracy: 0.8313 - val_loss: 1.2415 - val_accuracy: 0.6898\n",
            "Epoch 244/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.4508 - accuracy: 0.7786 - val_loss: 0.6744 - val_accuracy: 0.6872\n",
            "Epoch 245/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.4777 - accuracy: 0.7554 - val_loss: 0.7602 - val_accuracy: 0.6845\n",
            "Epoch 246/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.3663 - accuracy: 0.8036 - val_loss: 0.8395 - val_accuracy: 0.7059\n",
            "Epoch 247/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.3029 - accuracy: 0.8384 - val_loss: 1.0198 - val_accuracy: 0.7059\n",
            "Epoch 248/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2959 - accuracy: 0.8446 - val_loss: 0.9856 - val_accuracy: 0.7059\n",
            "Epoch 249/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.3461 - accuracy: 0.8125 - val_loss: 0.9400 - val_accuracy: 0.7193\n",
            "Epoch 250/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.3526 - accuracy: 0.8000 - val_loss: 0.8527 - val_accuracy: 0.6791\n",
            "Epoch 251/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.3188 - accuracy: 0.8321 - val_loss: 0.9790 - val_accuracy: 0.6845\n",
            "Epoch 252/1000\n",
            "1120/1120 [==============================] - 0s 227us/step - loss: 0.2993 - accuracy: 0.8313 - val_loss: 1.0807 - val_accuracy: 0.7059\n",
            "Epoch 253/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2777 - accuracy: 0.8473 - val_loss: 1.1324 - val_accuracy: 0.7166\n",
            "Epoch 254/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2898 - accuracy: 0.8366 - val_loss: 1.1288 - val_accuracy: 0.7059\n",
            "Epoch 255/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2830 - accuracy: 0.8446 - val_loss: 1.2375 - val_accuracy: 0.6979\n",
            "Epoch 256/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.3031 - accuracy: 0.8313 - val_loss: 1.1118 - val_accuracy: 0.7139\n",
            "Epoch 257/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2885 - accuracy: 0.8357 - val_loss: 1.1197 - val_accuracy: 0.6979\n",
            "Epoch 258/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2964 - accuracy: 0.8375 - val_loss: 1.1815 - val_accuracy: 0.7139\n",
            "Epoch 259/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.3169 - accuracy: 0.8339 - val_loss: 1.0138 - val_accuracy: 0.7005\n",
            "Epoch 260/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.3290 - accuracy: 0.8188 - val_loss: 1.1063 - val_accuracy: 0.7166\n",
            "Epoch 261/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2885 - accuracy: 0.8375 - val_loss: 1.1715 - val_accuracy: 0.7139\n",
            "Epoch 262/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.3113 - accuracy: 0.8259 - val_loss: 1.1186 - val_accuracy: 0.6845\n",
            "Epoch 263/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.3269 - accuracy: 0.8241 - val_loss: 1.0101 - val_accuracy: 0.7193\n",
            "Epoch 264/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2897 - accuracy: 0.8366 - val_loss: 1.1930 - val_accuracy: 0.7273\n",
            "Epoch 265/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.4167 - accuracy: 0.7768 - val_loss: 0.7296 - val_accuracy: 0.6578\n",
            "Epoch 266/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3628 - accuracy: 0.8179 - val_loss: 0.8817 - val_accuracy: 0.6631\n",
            "Epoch 267/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.3112 - accuracy: 0.8473 - val_loss: 1.1949 - val_accuracy: 0.6497\n",
            "Epoch 268/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.3796 - accuracy: 0.8295 - val_loss: 0.8787 - val_accuracy: 0.7193\n",
            "Epoch 269/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.3470 - accuracy: 0.8170 - val_loss: 1.0480 - val_accuracy: 0.6684\n",
            "Epoch 270/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.3379 - accuracy: 0.8125 - val_loss: 0.9798 - val_accuracy: 0.7032\n",
            "Epoch 271/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.3194 - accuracy: 0.8411 - val_loss: 0.9089 - val_accuracy: 0.7032\n",
            "Epoch 272/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.3034 - accuracy: 0.8446 - val_loss: 0.9322 - val_accuracy: 0.7299\n",
            "Epoch 273/1000\n",
            "1120/1120 [==============================] - 0s 260us/step - loss: 0.2843 - accuracy: 0.8455 - val_loss: 1.1285 - val_accuracy: 0.7086\n",
            "Epoch 274/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2915 - accuracy: 0.8438 - val_loss: 1.1490 - val_accuracy: 0.7273\n",
            "Epoch 275/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2898 - accuracy: 0.8411 - val_loss: 1.0984 - val_accuracy: 0.7166\n",
            "Epoch 276/1000\n",
            "1120/1120 [==============================] - 0s 227us/step - loss: 0.2729 - accuracy: 0.8509 - val_loss: 1.1659 - val_accuracy: 0.7166\n",
            "Epoch 277/1000\n",
            "1120/1120 [==============================] - 0s 233us/step - loss: 0.3174 - accuracy: 0.8268 - val_loss: 0.9993 - val_accuracy: 0.6818\n",
            "Epoch 278/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.3040 - accuracy: 0.8402 - val_loss: 1.0458 - val_accuracy: 0.7193\n",
            "Epoch 279/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2843 - accuracy: 0.8482 - val_loss: 1.1807 - val_accuracy: 0.7166\n",
            "Epoch 280/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2922 - accuracy: 0.8491 - val_loss: 1.0457 - val_accuracy: 0.6952\n",
            "Epoch 281/1000\n",
            "1120/1120 [==============================] - 0s 262us/step - loss: 0.2886 - accuracy: 0.8455 - val_loss: 1.0598 - val_accuracy: 0.6898\n",
            "Epoch 282/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.3089 - accuracy: 0.8321 - val_loss: 1.0189 - val_accuracy: 0.6979\n",
            "Epoch 283/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2767 - accuracy: 0.8554 - val_loss: 1.2146 - val_accuracy: 0.6684\n",
            "Epoch 284/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2958 - accuracy: 0.8455 - val_loss: 0.9750 - val_accuracy: 0.7273\n",
            "Epoch 285/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2791 - accuracy: 0.8420 - val_loss: 1.1353 - val_accuracy: 0.6872\n",
            "Epoch 286/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.3243 - accuracy: 0.8250 - val_loss: 0.9748 - val_accuracy: 0.6738\n",
            "Epoch 287/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.3027 - accuracy: 0.8295 - val_loss: 0.9605 - val_accuracy: 0.7193\n",
            "Epoch 288/1000\n",
            "1120/1120 [==============================] - 0s 263us/step - loss: 0.2821 - accuracy: 0.8482 - val_loss: 1.0161 - val_accuracy: 0.7193\n",
            "Epoch 289/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2712 - accuracy: 0.8473 - val_loss: 1.1418 - val_accuracy: 0.7299\n",
            "Epoch 290/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2730 - accuracy: 0.8491 - val_loss: 1.2182 - val_accuracy: 0.7005\n",
            "Epoch 291/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2715 - accuracy: 0.8536 - val_loss: 1.1471 - val_accuracy: 0.7299\n",
            "Epoch 292/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.4494 - accuracy: 0.7723 - val_loss: 0.6982 - val_accuracy: 0.6390\n",
            "Epoch 293/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.4480 - accuracy: 0.7607 - val_loss: 0.7186 - val_accuracy: 0.6791\n",
            "Epoch 294/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.3378 - accuracy: 0.8241 - val_loss: 1.1130 - val_accuracy: 0.7166\n",
            "Epoch 295/1000\n",
            "1120/1120 [==============================] - 0s 274us/step - loss: 0.3252 - accuracy: 0.8259 - val_loss: 1.1333 - val_accuracy: 0.6791\n",
            "Epoch 296/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.3168 - accuracy: 0.8259 - val_loss: 0.9523 - val_accuracy: 0.7299\n",
            "Epoch 297/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2696 - accuracy: 0.8464 - val_loss: 1.3243 - val_accuracy: 0.7326\n",
            "Epoch 298/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2782 - accuracy: 0.8536 - val_loss: 1.1634 - val_accuracy: 0.7219\n",
            "Epoch 299/1000\n",
            "1120/1120 [==============================] - 0s 270us/step - loss: 0.2794 - accuracy: 0.8527 - val_loss: 1.0677 - val_accuracy: 0.7193\n",
            "Epoch 300/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.3057 - accuracy: 0.8268 - val_loss: 0.9450 - val_accuracy: 0.7299\n",
            "Epoch 301/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.3226 - accuracy: 0.8161 - val_loss: 0.9865 - val_accuracy: 0.7059\n",
            "Epoch 302/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2921 - accuracy: 0.8455 - val_loss: 1.2249 - val_accuracy: 0.7086\n",
            "Epoch 303/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.3020 - accuracy: 0.8375 - val_loss: 1.0136 - val_accuracy: 0.7353\n",
            "Epoch 304/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2971 - accuracy: 0.8357 - val_loss: 1.0087 - val_accuracy: 0.7112\n",
            "Epoch 305/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.3280 - accuracy: 0.8295 - val_loss: 0.9207 - val_accuracy: 0.7032\n",
            "Epoch 306/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.2703 - accuracy: 0.8625 - val_loss: 1.2660 - val_accuracy: 0.7273\n",
            "Epoch 307/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2519 - accuracy: 0.8643 - val_loss: 1.2501 - val_accuracy: 0.7460\n",
            "Epoch 308/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2610 - accuracy: 0.8580 - val_loss: 1.2044 - val_accuracy: 0.7273\n",
            "Epoch 309/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2411 - accuracy: 0.8759 - val_loss: 1.5053 - val_accuracy: 0.7032\n",
            "Epoch 310/1000\n",
            "1120/1120 [==============================] - 0s 270us/step - loss: 0.3615 - accuracy: 0.8080 - val_loss: 1.0321 - val_accuracy: 0.6818\n",
            "Epoch 311/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.3752 - accuracy: 0.7946 - val_loss: 0.8143 - val_accuracy: 0.7166\n",
            "Epoch 312/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2816 - accuracy: 0.8545 - val_loss: 1.2131 - val_accuracy: 0.6952\n",
            "Epoch 313/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2855 - accuracy: 0.8375 - val_loss: 1.1251 - val_accuracy: 0.7166\n",
            "Epoch 314/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.3222 - accuracy: 0.8366 - val_loss: 0.9148 - val_accuracy: 0.7219\n",
            "Epoch 315/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2747 - accuracy: 0.8661 - val_loss: 1.0842 - val_accuracy: 0.7059\n",
            "Epoch 316/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.3039 - accuracy: 0.8411 - val_loss: 0.9745 - val_accuracy: 0.6952\n",
            "Epoch 317/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2953 - accuracy: 0.8384 - val_loss: 0.9804 - val_accuracy: 0.6952\n",
            "Epoch 318/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2585 - accuracy: 0.8545 - val_loss: 1.3826 - val_accuracy: 0.7166\n",
            "Epoch 319/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2953 - accuracy: 0.8366 - val_loss: 1.2344 - val_accuracy: 0.7086\n",
            "Epoch 320/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.3337 - accuracy: 0.8134 - val_loss: 0.9580 - val_accuracy: 0.7032\n",
            "Epoch 321/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2990 - accuracy: 0.8330 - val_loss: 1.0274 - val_accuracy: 0.6872\n",
            "Epoch 322/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2909 - accuracy: 0.8411 - val_loss: 0.9614 - val_accuracy: 0.7246\n",
            "Epoch 323/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2634 - accuracy: 0.8402 - val_loss: 1.1005 - val_accuracy: 0.7112\n",
            "Epoch 324/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2722 - accuracy: 0.8438 - val_loss: 1.0747 - val_accuracy: 0.7139\n",
            "Epoch 325/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.3159 - accuracy: 0.8321 - val_loss: 0.9634 - val_accuracy: 0.7166\n",
            "Epoch 326/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2877 - accuracy: 0.8429 - val_loss: 1.1688 - val_accuracy: 0.7299\n",
            "Epoch 327/1000\n",
            "1120/1120 [==============================] - 0s 229us/step - loss: 0.2918 - accuracy: 0.8393 - val_loss: 1.0604 - val_accuracy: 0.7219\n",
            "Epoch 328/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2969 - accuracy: 0.8402 - val_loss: 1.0707 - val_accuracy: 0.7086\n",
            "Epoch 329/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2457 - accuracy: 0.8670 - val_loss: 1.5188 - val_accuracy: 0.7032\n",
            "Epoch 330/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2698 - accuracy: 0.8438 - val_loss: 1.1866 - val_accuracy: 0.7086\n",
            "Epoch 331/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2616 - accuracy: 0.8545 - val_loss: 1.4861 - val_accuracy: 0.7005\n",
            "Epoch 332/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2666 - accuracy: 0.8518 - val_loss: 1.2964 - val_accuracy: 0.7139\n",
            "Epoch 333/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2498 - accuracy: 0.8625 - val_loss: 1.3744 - val_accuracy: 0.7273\n",
            "Epoch 334/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2632 - accuracy: 0.8571 - val_loss: 1.2903 - val_accuracy: 0.7219\n",
            "Epoch 335/1000\n",
            "1120/1120 [==============================] - 0s 229us/step - loss: 0.2945 - accuracy: 0.8482 - val_loss: 1.0387 - val_accuracy: 0.7166\n",
            "Epoch 336/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.3824 - accuracy: 0.7866 - val_loss: 0.7348 - val_accuracy: 0.6952\n",
            "Epoch 337/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3356 - accuracy: 0.8196 - val_loss: 1.0293 - val_accuracy: 0.7219\n",
            "Epoch 338/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2773 - accuracy: 0.8562 - val_loss: 1.0765 - val_accuracy: 0.7273\n",
            "Epoch 339/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2629 - accuracy: 0.8598 - val_loss: 1.1292 - val_accuracy: 0.7219\n",
            "Epoch 340/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2526 - accuracy: 0.8679 - val_loss: 1.1525 - val_accuracy: 0.7326\n",
            "Epoch 341/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2547 - accuracy: 0.8634 - val_loss: 1.1977 - val_accuracy: 0.6845\n",
            "Epoch 342/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2780 - accuracy: 0.8491 - val_loss: 1.1263 - val_accuracy: 0.7380\n",
            "Epoch 343/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.2731 - accuracy: 0.8571 - val_loss: 1.1264 - val_accuracy: 0.7326\n",
            "Epoch 344/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2682 - accuracy: 0.8598 - val_loss: 1.2747 - val_accuracy: 0.7299\n",
            "Epoch 345/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2661 - accuracy: 0.8536 - val_loss: 1.1323 - val_accuracy: 0.7246\n",
            "Epoch 346/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2957 - accuracy: 0.8393 - val_loss: 1.0407 - val_accuracy: 0.7246\n",
            "Epoch 347/1000\n",
            "1120/1120 [==============================] - 0s 273us/step - loss: 0.3222 - accuracy: 0.8232 - val_loss: 0.8739 - val_accuracy: 0.7193\n",
            "Epoch 348/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2614 - accuracy: 0.8589 - val_loss: 1.1895 - val_accuracy: 0.6952\n",
            "Epoch 349/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2561 - accuracy: 0.8518 - val_loss: 1.2942 - val_accuracy: 0.7246\n",
            "Epoch 350/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2519 - accuracy: 0.8598 - val_loss: 1.3477 - val_accuracy: 0.7326\n",
            "Epoch 351/1000\n",
            "1120/1120 [==============================] - 0s 231us/step - loss: 0.2669 - accuracy: 0.8455 - val_loss: 1.1563 - val_accuracy: 0.7059\n",
            "Epoch 352/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2741 - accuracy: 0.8482 - val_loss: 1.3055 - val_accuracy: 0.6979\n",
            "Epoch 353/1000\n",
            "1120/1120 [==============================] - 0s 261us/step - loss: 0.2813 - accuracy: 0.8482 - val_loss: 1.1600 - val_accuracy: 0.7086\n",
            "Epoch 354/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2476 - accuracy: 0.8625 - val_loss: 1.2178 - val_accuracy: 0.7326\n",
            "Epoch 355/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2463 - accuracy: 0.8598 - val_loss: 1.2818 - val_accuracy: 0.6925\n",
            "Epoch 356/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2530 - accuracy: 0.8607 - val_loss: 1.2023 - val_accuracy: 0.7299\n",
            "Epoch 357/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.3153 - accuracy: 0.8438 - val_loss: 0.9366 - val_accuracy: 0.7406\n",
            "Epoch 358/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2838 - accuracy: 0.8589 - val_loss: 1.0532 - val_accuracy: 0.7032\n",
            "Epoch 359/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2677 - accuracy: 0.8518 - val_loss: 1.1568 - val_accuracy: 0.7112\n",
            "Epoch 360/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2458 - accuracy: 0.8634 - val_loss: 1.1811 - val_accuracy: 0.7086\n",
            "Epoch 361/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2826 - accuracy: 0.8304 - val_loss: 0.9901 - val_accuracy: 0.7059\n",
            "Epoch 362/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2833 - accuracy: 0.8464 - val_loss: 1.1536 - val_accuracy: 0.6791\n",
            "Epoch 363/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3251 - accuracy: 0.8134 - val_loss: 0.8810 - val_accuracy: 0.7059\n",
            "Epoch 364/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2677 - accuracy: 0.8527 - val_loss: 1.1183 - val_accuracy: 0.7059\n",
            "Epoch 365/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2491 - accuracy: 0.8589 - val_loss: 1.2200 - val_accuracy: 0.7139\n",
            "Epoch 366/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2633 - accuracy: 0.8527 - val_loss: 1.2234 - val_accuracy: 0.6925\n",
            "Epoch 367/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.3633 - accuracy: 0.8161 - val_loss: 0.8415 - val_accuracy: 0.6684\n",
            "Epoch 368/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.3135 - accuracy: 0.8259 - val_loss: 0.9972 - val_accuracy: 0.7059\n",
            "Epoch 369/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2965 - accuracy: 0.8348 - val_loss: 0.9642 - val_accuracy: 0.7273\n",
            "Epoch 370/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2914 - accuracy: 0.8420 - val_loss: 1.0013 - val_accuracy: 0.7406\n",
            "Epoch 371/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2607 - accuracy: 0.8554 - val_loss: 1.0673 - val_accuracy: 0.7246\n",
            "Epoch 372/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2457 - accuracy: 0.8598 - val_loss: 1.2578 - val_accuracy: 0.7086\n",
            "Epoch 373/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2325 - accuracy: 0.8723 - val_loss: 1.3179 - val_accuracy: 0.7219\n",
            "Epoch 374/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2642 - accuracy: 0.8625 - val_loss: 1.2366 - val_accuracy: 0.7139\n",
            "Epoch 375/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.3044 - accuracy: 0.8455 - val_loss: 0.8254 - val_accuracy: 0.7326\n",
            "Epoch 376/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2434 - accuracy: 0.8536 - val_loss: 1.0160 - val_accuracy: 0.7139\n",
            "Epoch 377/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2300 - accuracy: 0.8634 - val_loss: 1.1902 - val_accuracy: 0.7246\n",
            "Epoch 378/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2362 - accuracy: 0.8723 - val_loss: 1.2484 - val_accuracy: 0.7166\n",
            "Epoch 379/1000\n",
            "1120/1120 [==============================] - 0s 274us/step - loss: 0.2289 - accuracy: 0.8714 - val_loss: 1.2956 - val_accuracy: 0.7219\n",
            "Epoch 380/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2827 - accuracy: 0.8509 - val_loss: 1.0709 - val_accuracy: 0.7193\n",
            "Epoch 381/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2868 - accuracy: 0.8482 - val_loss: 0.9685 - val_accuracy: 0.7219\n",
            "Epoch 382/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2669 - accuracy: 0.8562 - val_loss: 1.0995 - val_accuracy: 0.7112\n",
            "Epoch 383/1000\n",
            "1120/1120 [==============================] - 0s 263us/step - loss: 0.2552 - accuracy: 0.8509 - val_loss: 1.0725 - val_accuracy: 0.7273\n",
            "Epoch 384/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2494 - accuracy: 0.8634 - val_loss: 1.2450 - val_accuracy: 0.7246\n",
            "Epoch 385/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2443 - accuracy: 0.8625 - val_loss: 1.2135 - val_accuracy: 0.7380\n",
            "Epoch 386/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2307 - accuracy: 0.8723 - val_loss: 1.4107 - val_accuracy: 0.7326\n",
            "Epoch 387/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2725 - accuracy: 0.8598 - val_loss: 1.2262 - val_accuracy: 0.7139\n",
            "Epoch 388/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2854 - accuracy: 0.8500 - val_loss: 1.0077 - val_accuracy: 0.7433\n",
            "Epoch 389/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2486 - accuracy: 0.8616 - val_loss: 1.2061 - val_accuracy: 0.7086\n",
            "Epoch 390/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.3098 - accuracy: 0.8411 - val_loss: 1.0253 - val_accuracy: 0.7246\n",
            "Epoch 391/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2802 - accuracy: 0.8536 - val_loss: 1.0045 - val_accuracy: 0.7299\n",
            "Epoch 392/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2484 - accuracy: 0.8625 - val_loss: 1.0609 - val_accuracy: 0.7219\n",
            "Epoch 393/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2446 - accuracy: 0.8741 - val_loss: 1.1643 - val_accuracy: 0.7166\n",
            "Epoch 394/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2484 - accuracy: 0.8580 - val_loss: 1.1633 - val_accuracy: 0.7299\n",
            "Epoch 395/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2418 - accuracy: 0.8607 - val_loss: 1.2949 - val_accuracy: 0.7353\n",
            "Epoch 396/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2276 - accuracy: 0.8687 - val_loss: 1.2549 - val_accuracy: 0.7139\n",
            "Epoch 397/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2304 - accuracy: 0.8696 - val_loss: 1.2853 - val_accuracy: 0.7139\n",
            "Epoch 398/1000\n",
            "1120/1120 [==============================] - 0s 268us/step - loss: 0.2360 - accuracy: 0.8661 - val_loss: 1.2908 - val_accuracy: 0.7139\n",
            "Epoch 399/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2489 - accuracy: 0.8598 - val_loss: 1.1556 - val_accuracy: 0.7193\n",
            "Epoch 400/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2461 - accuracy: 0.8652 - val_loss: 1.3161 - val_accuracy: 0.7086\n",
            "Epoch 401/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2625 - accuracy: 0.8571 - val_loss: 1.2816 - val_accuracy: 0.6925\n",
            "Epoch 402/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.3117 - accuracy: 0.8321 - val_loss: 1.0664 - val_accuracy: 0.7086\n",
            "Epoch 403/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.3375 - accuracy: 0.8170 - val_loss: 1.0273 - val_accuracy: 0.7139\n",
            "Epoch 404/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2965 - accuracy: 0.8420 - val_loss: 0.9858 - val_accuracy: 0.7326\n",
            "Epoch 405/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2533 - accuracy: 0.8473 - val_loss: 1.4745 - val_accuracy: 0.7567\n",
            "Epoch 406/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2367 - accuracy: 0.8696 - val_loss: 1.4620 - val_accuracy: 0.6979\n",
            "Epoch 407/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2682 - accuracy: 0.8545 - val_loss: 1.0561 - val_accuracy: 0.7326\n",
            "Epoch 408/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2552 - accuracy: 0.8545 - val_loss: 1.0451 - val_accuracy: 0.7219\n",
            "Epoch 409/1000\n",
            "1120/1120 [==============================] - 0s 282us/step - loss: 0.2279 - accuracy: 0.8723 - val_loss: 1.3690 - val_accuracy: 0.7219\n",
            "Epoch 410/1000\n",
            "1120/1120 [==============================] - 0s 269us/step - loss: 0.2355 - accuracy: 0.8687 - val_loss: 1.2785 - val_accuracy: 0.7219\n",
            "Epoch 411/1000\n",
            "1120/1120 [==============================] - 0s 277us/step - loss: 0.2647 - accuracy: 0.8500 - val_loss: 1.3892 - val_accuracy: 0.6898\n",
            "Epoch 412/1000\n",
            "1120/1120 [==============================] - 0s 275us/step - loss: 0.3232 - accuracy: 0.8134 - val_loss: 1.1222 - val_accuracy: 0.6631\n",
            "Epoch 413/1000\n",
            "1120/1120 [==============================] - 0s 269us/step - loss: 0.3444 - accuracy: 0.8170 - val_loss: 1.0465 - val_accuracy: 0.6872\n",
            "Epoch 414/1000\n",
            "1120/1120 [==============================] - 0s 267us/step - loss: 0.2984 - accuracy: 0.8384 - val_loss: 1.3364 - val_accuracy: 0.7059\n",
            "Epoch 415/1000\n",
            "1120/1120 [==============================] - 0s 285us/step - loss: 0.2795 - accuracy: 0.8598 - val_loss: 1.4412 - val_accuracy: 0.6818\n",
            "Epoch 416/1000\n",
            "1120/1120 [==============================] - 0s 269us/step - loss: 0.2978 - accuracy: 0.8348 - val_loss: 1.0167 - val_accuracy: 0.7460\n",
            "Epoch 417/1000\n",
            "1120/1120 [==============================] - 0s 267us/step - loss: 0.2511 - accuracy: 0.8670 - val_loss: 1.1608 - val_accuracy: 0.7353\n",
            "Epoch 418/1000\n",
            "1120/1120 [==============================] - 0s 262us/step - loss: 0.2323 - accuracy: 0.8696 - val_loss: 1.4380 - val_accuracy: 0.7246\n",
            "Epoch 419/1000\n",
            "1120/1120 [==============================] - 0s 281us/step - loss: 0.2205 - accuracy: 0.8732 - val_loss: 1.7443 - val_accuracy: 0.7139\n",
            "Epoch 420/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2441 - accuracy: 0.8625 - val_loss: 1.3443 - val_accuracy: 0.7353\n",
            "Epoch 421/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2434 - accuracy: 0.8670 - val_loss: 1.5499 - val_accuracy: 0.6952\n",
            "Epoch 422/1000\n",
            "1120/1120 [==============================] - 0s 269us/step - loss: 0.2788 - accuracy: 0.8330 - val_loss: 1.1399 - val_accuracy: 0.7112\n",
            "Epoch 423/1000\n",
            "1120/1120 [==============================] - 0s 265us/step - loss: 0.2856 - accuracy: 0.8429 - val_loss: 1.2782 - val_accuracy: 0.6979\n",
            "Epoch 424/1000\n",
            "1120/1120 [==============================] - 0s 264us/step - loss: 0.3105 - accuracy: 0.8304 - val_loss: 0.8761 - val_accuracy: 0.7246\n",
            "Epoch 425/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2543 - accuracy: 0.8705 - val_loss: 1.3152 - val_accuracy: 0.7166\n",
            "Epoch 426/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2542 - accuracy: 0.8625 - val_loss: 1.2041 - val_accuracy: 0.7032\n",
            "Epoch 427/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2626 - accuracy: 0.8634 - val_loss: 1.1303 - val_accuracy: 0.6898\n",
            "Epoch 428/1000\n",
            "1120/1120 [==============================] - 0s 258us/step - loss: 0.2438 - accuracy: 0.8670 - val_loss: 1.2081 - val_accuracy: 0.7166\n",
            "Epoch 429/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2405 - accuracy: 0.8607 - val_loss: 1.2583 - val_accuracy: 0.7406\n",
            "Epoch 430/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2896 - accuracy: 0.8554 - val_loss: 1.1227 - val_accuracy: 0.6979\n",
            "Epoch 431/1000\n",
            "1120/1120 [==============================] - 0s 261us/step - loss: 0.2820 - accuracy: 0.8384 - val_loss: 1.2489 - val_accuracy: 0.7166\n",
            "Epoch 432/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2562 - accuracy: 0.8661 - val_loss: 1.2694 - val_accuracy: 0.7246\n",
            "Epoch 433/1000\n",
            "1120/1120 [==============================] - 0s 265us/step - loss: 0.2682 - accuracy: 0.8518 - val_loss: 1.1343 - val_accuracy: 0.7540\n",
            "Epoch 434/1000\n",
            "1120/1120 [==============================] - 0s 263us/step - loss: 0.2842 - accuracy: 0.8554 - val_loss: 1.0874 - val_accuracy: 0.7246\n",
            "Epoch 435/1000\n",
            "1120/1120 [==============================] - 0s 261us/step - loss: 0.2374 - accuracy: 0.8741 - val_loss: 1.4953 - val_accuracy: 0.7193\n",
            "Epoch 436/1000\n",
            "1120/1120 [==============================] - 0s 261us/step - loss: 0.2735 - accuracy: 0.8554 - val_loss: 1.2411 - val_accuracy: 0.7086\n",
            "Epoch 437/1000\n",
            "1120/1120 [==============================] - 0s 269us/step - loss: 0.2851 - accuracy: 0.8536 - val_loss: 1.0400 - val_accuracy: 0.7273\n",
            "Epoch 438/1000\n",
            "1120/1120 [==============================] - 0s 263us/step - loss: 0.2442 - accuracy: 0.8643 - val_loss: 1.3397 - val_accuracy: 0.7326\n",
            "Epoch 439/1000\n",
            "1120/1120 [==============================] - 0s 266us/step - loss: 0.2422 - accuracy: 0.8705 - val_loss: 1.3418 - val_accuracy: 0.7353\n",
            "Epoch 440/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2346 - accuracy: 0.8661 - val_loss: 1.3080 - val_accuracy: 0.7273\n",
            "Epoch 441/1000\n",
            "1120/1120 [==============================] - 0s 276us/step - loss: 0.2344 - accuracy: 0.8723 - val_loss: 1.4242 - val_accuracy: 0.7540\n",
            "Epoch 442/1000\n",
            "1120/1120 [==============================] - 0s 264us/step - loss: 0.2739 - accuracy: 0.8554 - val_loss: 1.1752 - val_accuracy: 0.6898\n",
            "Epoch 443/1000\n",
            "1120/1120 [==============================] - 0s 264us/step - loss: 0.2672 - accuracy: 0.8518 - val_loss: 1.2747 - val_accuracy: 0.6979\n",
            "Epoch 444/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2471 - accuracy: 0.8679 - val_loss: 1.4068 - val_accuracy: 0.7487\n",
            "Epoch 445/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2490 - accuracy: 0.8705 - val_loss: 1.4079 - val_accuracy: 0.7273\n",
            "Epoch 446/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2376 - accuracy: 0.8786 - val_loss: 1.5187 - val_accuracy: 0.7299\n",
            "Epoch 447/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2256 - accuracy: 0.8777 - val_loss: 1.7229 - val_accuracy: 0.7299\n",
            "Epoch 448/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2556 - accuracy: 0.8643 - val_loss: 1.3903 - val_accuracy: 0.7086\n",
            "Epoch 449/1000\n",
            "1120/1120 [==============================] - 0s 261us/step - loss: 0.2489 - accuracy: 0.8625 - val_loss: 1.4773 - val_accuracy: 0.7380\n",
            "Epoch 450/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2389 - accuracy: 0.8759 - val_loss: 1.5840 - val_accuracy: 0.7326\n",
            "Epoch 451/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2502 - accuracy: 0.8670 - val_loss: 1.3432 - val_accuracy: 0.7299\n",
            "Epoch 452/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.3136 - accuracy: 0.8277 - val_loss: 1.1556 - val_accuracy: 0.7139\n",
            "Epoch 453/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2752 - accuracy: 0.8643 - val_loss: 1.2003 - val_accuracy: 0.7273\n",
            "Epoch 454/1000\n",
            "1120/1120 [==============================] - 0s 271us/step - loss: 0.2554 - accuracy: 0.8571 - val_loss: 1.2790 - val_accuracy: 0.7513\n",
            "Epoch 455/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2539 - accuracy: 0.8625 - val_loss: 1.2302 - val_accuracy: 0.7380\n",
            "Epoch 456/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2282 - accuracy: 0.8750 - val_loss: 1.3730 - val_accuracy: 0.7513\n",
            "Epoch 457/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2266 - accuracy: 0.8795 - val_loss: 1.4249 - val_accuracy: 0.7513\n",
            "Epoch 458/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2226 - accuracy: 0.8777 - val_loss: 1.5243 - val_accuracy: 0.7460\n",
            "Epoch 459/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.2248 - accuracy: 0.8786 - val_loss: 1.7213 - val_accuracy: 0.7166\n",
            "Epoch 460/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.3815 - accuracy: 0.8170 - val_loss: 0.7652 - val_accuracy: 0.7005\n",
            "Epoch 461/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.3025 - accuracy: 0.8429 - val_loss: 1.1813 - val_accuracy: 0.7112\n",
            "Epoch 462/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2668 - accuracy: 0.8589 - val_loss: 1.4416 - val_accuracy: 0.7112\n",
            "Epoch 463/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2744 - accuracy: 0.8696 - val_loss: 1.3441 - val_accuracy: 0.7193\n",
            "Epoch 464/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2888 - accuracy: 0.8473 - val_loss: 1.0146 - val_accuracy: 0.7326\n",
            "Epoch 465/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2835 - accuracy: 0.8393 - val_loss: 1.0864 - val_accuracy: 0.7513\n",
            "Epoch 466/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2595 - accuracy: 0.8500 - val_loss: 1.6629 - val_accuracy: 0.6979\n",
            "Epoch 467/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2484 - accuracy: 0.8616 - val_loss: 1.2047 - val_accuracy: 0.7433\n",
            "Epoch 468/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2391 - accuracy: 0.8741 - val_loss: 1.6342 - val_accuracy: 0.6979\n",
            "Epoch 469/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2731 - accuracy: 0.8518 - val_loss: 1.4355 - val_accuracy: 0.6979\n",
            "Epoch 470/1000\n",
            "1120/1120 [==============================] - 0s 228us/step - loss: 0.3084 - accuracy: 0.8429 - val_loss: 1.0591 - val_accuracy: 0.7219\n",
            "Epoch 471/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2634 - accuracy: 0.8616 - val_loss: 1.1994 - val_accuracy: 0.7326\n",
            "Epoch 472/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2642 - accuracy: 0.8554 - val_loss: 1.1195 - val_accuracy: 0.7299\n",
            "Epoch 473/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2511 - accuracy: 0.8652 - val_loss: 1.2804 - val_accuracy: 0.7299\n",
            "Epoch 474/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2398 - accuracy: 0.8705 - val_loss: 1.3344 - val_accuracy: 0.7460\n",
            "Epoch 475/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2440 - accuracy: 0.8661 - val_loss: 1.3890 - val_accuracy: 0.7406\n",
            "Epoch 476/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2309 - accuracy: 0.8679 - val_loss: 1.6497 - val_accuracy: 0.7246\n",
            "Epoch 477/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.3124 - accuracy: 0.8527 - val_loss: 1.1028 - val_accuracy: 0.7059\n",
            "Epoch 478/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3852 - accuracy: 0.7946 - val_loss: 0.6885 - val_accuracy: 0.7086\n",
            "Epoch 479/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.3087 - accuracy: 0.8429 - val_loss: 0.9443 - val_accuracy: 0.6979\n",
            "Epoch 480/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2653 - accuracy: 0.8562 - val_loss: 1.0632 - val_accuracy: 0.7193\n",
            "Epoch 481/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2607 - accuracy: 0.8616 - val_loss: 1.1243 - val_accuracy: 0.7193\n",
            "Epoch 482/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2455 - accuracy: 0.8607 - val_loss: 1.2573 - val_accuracy: 0.7353\n",
            "Epoch 483/1000\n",
            "1120/1120 [==============================] - 0s 265us/step - loss: 0.2436 - accuracy: 0.8625 - val_loss: 1.2587 - val_accuracy: 0.7326\n",
            "Epoch 484/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2230 - accuracy: 0.8750 - val_loss: 1.2852 - val_accuracy: 0.7299\n",
            "Epoch 485/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2269 - accuracy: 0.8723 - val_loss: 1.2064 - val_accuracy: 0.7139\n",
            "Epoch 486/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2261 - accuracy: 0.8759 - val_loss: 1.2963 - val_accuracy: 0.7246\n",
            "Epoch 487/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2279 - accuracy: 0.8723 - val_loss: 1.3919 - val_accuracy: 0.7380\n",
            "Epoch 488/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2384 - accuracy: 0.8580 - val_loss: 1.2136 - val_accuracy: 0.7219\n",
            "Epoch 489/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2488 - accuracy: 0.8554 - val_loss: 1.2125 - val_accuracy: 0.7219\n",
            "Epoch 490/1000\n",
            "1120/1120 [==============================] - 0s 269us/step - loss: 0.2821 - accuracy: 0.8420 - val_loss: 1.1680 - val_accuracy: 0.7299\n",
            "Epoch 491/1000\n",
            "1120/1120 [==============================] - 0s 258us/step - loss: 0.2400 - accuracy: 0.8687 - val_loss: 1.3778 - val_accuracy: 0.7299\n",
            "Epoch 492/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2306 - accuracy: 0.8661 - val_loss: 1.4969 - val_accuracy: 0.7219\n",
            "Epoch 493/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2272 - accuracy: 0.8786 - val_loss: 1.5214 - val_accuracy: 0.7193\n",
            "Epoch 494/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2343 - accuracy: 0.8696 - val_loss: 1.4404 - val_accuracy: 0.7513\n",
            "Epoch 495/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2374 - accuracy: 0.8705 - val_loss: 1.5382 - val_accuracy: 0.7005\n",
            "Epoch 496/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2447 - accuracy: 0.8670 - val_loss: 1.4198 - val_accuracy: 0.7219\n",
            "Epoch 497/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.2368 - accuracy: 0.8670 - val_loss: 1.6247 - val_accuracy: 0.7433\n",
            "Epoch 498/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2626 - accuracy: 0.8661 - val_loss: 1.2647 - val_accuracy: 0.7273\n",
            "Epoch 499/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2652 - accuracy: 0.8625 - val_loss: 1.2551 - val_accuracy: 0.7433\n",
            "Epoch 500/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2407 - accuracy: 0.8652 - val_loss: 1.3301 - val_accuracy: 0.7166\n",
            "Epoch 501/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.3359 - accuracy: 0.8366 - val_loss: 1.1870 - val_accuracy: 0.6631\n",
            "Epoch 502/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3792 - accuracy: 0.8071 - val_loss: 0.8531 - val_accuracy: 0.6845\n",
            "Epoch 503/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2818 - accuracy: 0.8438 - val_loss: 1.0485 - val_accuracy: 0.7193\n",
            "Epoch 504/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2971 - accuracy: 0.8536 - val_loss: 0.8740 - val_accuracy: 0.7219\n",
            "Epoch 505/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2423 - accuracy: 0.8696 - val_loss: 1.0436 - val_accuracy: 0.7433\n",
            "Epoch 506/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2220 - accuracy: 0.8777 - val_loss: 1.2551 - val_accuracy: 0.7299\n",
            "Epoch 507/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2557 - accuracy: 0.8634 - val_loss: 1.2676 - val_accuracy: 0.6979\n",
            "Epoch 508/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2936 - accuracy: 0.8357 - val_loss: 0.8933 - val_accuracy: 0.7193\n",
            "Epoch 509/1000\n",
            "1120/1120 [==============================] - 0s 275us/step - loss: 0.2746 - accuracy: 0.8446 - val_loss: 0.9784 - val_accuracy: 0.7433\n",
            "Epoch 510/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2255 - accuracy: 0.8705 - val_loss: 1.1704 - val_accuracy: 0.7353\n",
            "Epoch 511/1000\n",
            "1120/1120 [==============================] - 0s 231us/step - loss: 0.2358 - accuracy: 0.8732 - val_loss: 1.1446 - val_accuracy: 0.7246\n",
            "Epoch 512/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2521 - accuracy: 0.8571 - val_loss: 1.1047 - val_accuracy: 0.7433\n",
            "Epoch 513/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2267 - accuracy: 0.8741 - val_loss: 1.2475 - val_accuracy: 0.7380\n",
            "Epoch 514/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2256 - accuracy: 0.8777 - val_loss: 1.3739 - val_accuracy: 0.7353\n",
            "Epoch 515/1000\n",
            "1120/1120 [==============================] - 0s 231us/step - loss: 0.2320 - accuracy: 0.8696 - val_loss: 1.2965 - val_accuracy: 0.7406\n",
            "Epoch 516/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2223 - accuracy: 0.8732 - val_loss: 1.4321 - val_accuracy: 0.7487\n",
            "Epoch 517/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2426 - accuracy: 0.8679 - val_loss: 1.4248 - val_accuracy: 0.7166\n",
            "Epoch 518/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2650 - accuracy: 0.8491 - val_loss: 1.1935 - val_accuracy: 0.7299\n",
            "Epoch 519/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2330 - accuracy: 0.8705 - val_loss: 1.5295 - val_accuracy: 0.7326\n",
            "Epoch 520/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2304 - accuracy: 0.8777 - val_loss: 1.3870 - val_accuracy: 0.7273\n",
            "Epoch 521/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2272 - accuracy: 0.8795 - val_loss: 1.5228 - val_accuracy: 0.7326\n",
            "Epoch 522/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2511 - accuracy: 0.8562 - val_loss: 1.1805 - val_accuracy: 0.7246\n",
            "Epoch 523/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2713 - accuracy: 0.8571 - val_loss: 1.2475 - val_accuracy: 0.7005\n",
            "Epoch 524/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.3509 - accuracy: 0.8250 - val_loss: 1.0146 - val_accuracy: 0.6791\n",
            "Epoch 525/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.3028 - accuracy: 0.8509 - val_loss: 0.9836 - val_accuracy: 0.7353\n",
            "Epoch 526/1000\n",
            "1120/1120 [==============================] - 0s 233us/step - loss: 0.2449 - accuracy: 0.8607 - val_loss: 1.4270 - val_accuracy: 0.7166\n",
            "Epoch 527/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2515 - accuracy: 0.8580 - val_loss: 1.0298 - val_accuracy: 0.7273\n",
            "Epoch 528/1000\n",
            "1120/1120 [==============================] - 0s 266us/step - loss: 0.3006 - accuracy: 0.8580 - val_loss: 0.8171 - val_accuracy: 0.7166\n",
            "Epoch 529/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2679 - accuracy: 0.8580 - val_loss: 0.8625 - val_accuracy: 0.7513\n",
            "Epoch 530/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2575 - accuracy: 0.8545 - val_loss: 1.0112 - val_accuracy: 0.7380\n",
            "Epoch 531/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.2355 - accuracy: 0.8687 - val_loss: 1.1718 - val_accuracy: 0.7353\n",
            "Epoch 532/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2251 - accuracy: 0.8714 - val_loss: 1.4284 - val_accuracy: 0.7406\n",
            "Epoch 533/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2665 - accuracy: 0.8473 - val_loss: 1.0040 - val_accuracy: 0.7326\n",
            "Epoch 534/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2727 - accuracy: 0.8687 - val_loss: 1.0990 - val_accuracy: 0.7326\n",
            "Epoch 535/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2452 - accuracy: 0.8670 - val_loss: 1.1907 - val_accuracy: 0.7433\n",
            "Epoch 536/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2794 - accuracy: 0.8571 - val_loss: 1.0185 - val_accuracy: 0.7433\n",
            "Epoch 537/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2583 - accuracy: 0.8580 - val_loss: 1.2689 - val_accuracy: 0.7112\n",
            "Epoch 538/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2551 - accuracy: 0.8616 - val_loss: 1.2462 - val_accuracy: 0.7139\n",
            "Epoch 539/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2646 - accuracy: 0.8652 - val_loss: 1.2482 - val_accuracy: 0.7112\n",
            "Epoch 540/1000\n",
            "1120/1120 [==============================] - 0s 232us/step - loss: 0.2742 - accuracy: 0.8473 - val_loss: 1.0793 - val_accuracy: 0.7166\n",
            "Epoch 541/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2738 - accuracy: 0.8500 - val_loss: 1.1463 - val_accuracy: 0.7166\n",
            "Epoch 542/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2281 - accuracy: 0.8795 - val_loss: 1.3292 - val_accuracy: 0.7326\n",
            "Epoch 543/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2232 - accuracy: 0.8750 - val_loss: 1.3368 - val_accuracy: 0.7299\n",
            "Epoch 544/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2343 - accuracy: 0.8723 - val_loss: 1.2946 - val_accuracy: 0.7460\n",
            "Epoch 545/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2436 - accuracy: 0.8571 - val_loss: 1.1955 - val_accuracy: 0.7299\n",
            "Epoch 546/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2297 - accuracy: 0.8679 - val_loss: 1.5640 - val_accuracy: 0.7166\n",
            "Epoch 547/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2530 - accuracy: 0.8536 - val_loss: 1.3907 - val_accuracy: 0.7166\n",
            "Epoch 548/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2439 - accuracy: 0.8562 - val_loss: 1.3056 - val_accuracy: 0.7433\n",
            "Epoch 549/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2252 - accuracy: 0.8804 - val_loss: 1.4558 - val_accuracy: 0.7406\n",
            "Epoch 550/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2376 - accuracy: 0.8696 - val_loss: 1.2074 - val_accuracy: 0.7487\n",
            "Epoch 551/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2643 - accuracy: 0.8652 - val_loss: 1.1277 - val_accuracy: 0.7353\n",
            "Epoch 552/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2356 - accuracy: 0.8643 - val_loss: 1.3169 - val_accuracy: 0.7380\n",
            "Epoch 553/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2343 - accuracy: 0.8714 - val_loss: 1.2045 - val_accuracy: 0.7246\n",
            "Epoch 554/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.4542 - accuracy: 0.7875 - val_loss: 0.6848 - val_accuracy: 0.7112\n",
            "Epoch 555/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.5327 - accuracy: 0.6946 - val_loss: 0.6229 - val_accuracy: 0.6417\n",
            "Epoch 556/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.4486 - accuracy: 0.7902 - val_loss: 0.7656 - val_accuracy: 0.6872\n",
            "Epoch 557/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.3351 - accuracy: 0.8268 - val_loss: 0.8296 - val_accuracy: 0.6791\n",
            "Epoch 558/1000\n",
            "1120/1120 [==============================] - 0s 258us/step - loss: 0.3042 - accuracy: 0.8330 - val_loss: 1.0901 - val_accuracy: 0.7086\n",
            "Epoch 559/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2695 - accuracy: 0.8491 - val_loss: 1.0663 - val_accuracy: 0.7406\n",
            "Epoch 560/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2429 - accuracy: 0.8696 - val_loss: 1.4078 - val_accuracy: 0.7299\n",
            "Epoch 561/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2363 - accuracy: 0.8705 - val_loss: 1.4122 - val_accuracy: 0.7353\n",
            "Epoch 562/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2284 - accuracy: 0.8759 - val_loss: 1.7014 - val_accuracy: 0.7112\n",
            "Epoch 563/1000\n",
            "1120/1120 [==============================] - 0s 233us/step - loss: 0.2719 - accuracy: 0.8491 - val_loss: 1.2806 - val_accuracy: 0.7005\n",
            "Epoch 564/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2939 - accuracy: 0.8545 - val_loss: 1.0547 - val_accuracy: 0.7193\n",
            "Epoch 565/1000\n",
            "1120/1120 [==============================] - 0s 231us/step - loss: 0.2446 - accuracy: 0.8607 - val_loss: 1.1990 - val_accuracy: 0.7193\n",
            "Epoch 566/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.2490 - accuracy: 0.8625 - val_loss: 1.2490 - val_accuracy: 0.7326\n",
            "Epoch 567/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2348 - accuracy: 0.8607 - val_loss: 1.3072 - val_accuracy: 0.7139\n",
            "Epoch 568/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2317 - accuracy: 0.8705 - val_loss: 1.3113 - val_accuracy: 0.7460\n",
            "Epoch 569/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2238 - accuracy: 0.8723 - val_loss: 1.5080 - val_accuracy: 0.7487\n",
            "Epoch 570/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2257 - accuracy: 0.8705 - val_loss: 1.7534 - val_accuracy: 0.6952\n",
            "Epoch 571/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.3299 - accuracy: 0.8313 - val_loss: 0.9737 - val_accuracy: 0.7246\n",
            "Epoch 572/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2805 - accuracy: 0.8482 - val_loss: 1.1236 - val_accuracy: 0.7380\n",
            "Epoch 573/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2410 - accuracy: 0.8607 - val_loss: 1.3598 - val_accuracy: 0.7193\n",
            "Epoch 574/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2507 - accuracy: 0.8643 - val_loss: 1.0856 - val_accuracy: 0.7326\n",
            "Epoch 575/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2334 - accuracy: 0.8598 - val_loss: 1.2932 - val_accuracy: 0.7246\n",
            "Epoch 576/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2346 - accuracy: 0.8562 - val_loss: 1.1415 - val_accuracy: 0.7299\n",
            "Epoch 577/1000\n",
            "1120/1120 [==============================] - 0s 230us/step - loss: 0.2266 - accuracy: 0.8679 - val_loss: 1.3500 - val_accuracy: 0.7353\n",
            "Epoch 578/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2483 - accuracy: 0.8679 - val_loss: 1.2945 - val_accuracy: 0.7273\n",
            "Epoch 579/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2303 - accuracy: 0.8705 - val_loss: 1.2799 - val_accuracy: 0.7086\n",
            "Epoch 580/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2780 - accuracy: 0.8554 - val_loss: 1.6763 - val_accuracy: 0.6283\n",
            "Epoch 581/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.6740 - accuracy: 0.6357 - val_loss: 0.7137 - val_accuracy: 0.4840\n",
            "Epoch 582/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.5309 - accuracy: 0.6375 - val_loss: 0.6923 - val_accuracy: 0.6845\n",
            "Epoch 583/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.3323 - accuracy: 0.8250 - val_loss: 0.9173 - val_accuracy: 0.7112\n",
            "Epoch 584/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.3165 - accuracy: 0.8339 - val_loss: 0.8274 - val_accuracy: 0.7112\n",
            "Epoch 585/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2604 - accuracy: 0.8571 - val_loss: 1.0690 - val_accuracy: 0.7219\n",
            "Epoch 586/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2668 - accuracy: 0.8527 - val_loss: 1.1184 - val_accuracy: 0.7406\n",
            "Epoch 587/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2943 - accuracy: 0.8580 - val_loss: 0.7984 - val_accuracy: 0.7353\n",
            "Epoch 588/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.4064 - accuracy: 0.7616 - val_loss: 0.7519 - val_accuracy: 0.6738\n",
            "Epoch 589/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.3031 - accuracy: 0.8446 - val_loss: 1.0068 - val_accuracy: 0.7193\n",
            "Epoch 590/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2602 - accuracy: 0.8580 - val_loss: 1.0962 - val_accuracy: 0.7246\n",
            "Epoch 591/1000\n",
            "1120/1120 [==============================] - 0s 232us/step - loss: 0.2521 - accuracy: 0.8679 - val_loss: 1.0346 - val_accuracy: 0.7086\n",
            "Epoch 592/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2723 - accuracy: 0.8589 - val_loss: 0.9465 - val_accuracy: 0.7139\n",
            "Epoch 593/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2454 - accuracy: 0.8687 - val_loss: 1.2025 - val_accuracy: 0.7326\n",
            "Epoch 594/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2394 - accuracy: 0.8759 - val_loss: 1.1672 - val_accuracy: 0.7380\n",
            "Epoch 595/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2249 - accuracy: 0.8634 - val_loss: 1.3226 - val_accuracy: 0.7032\n",
            "Epoch 596/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2358 - accuracy: 0.8714 - val_loss: 1.1183 - val_accuracy: 0.7513\n",
            "Epoch 597/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2573 - accuracy: 0.8634 - val_loss: 1.1157 - val_accuracy: 0.7193\n",
            "Epoch 598/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2280 - accuracy: 0.8741 - val_loss: 1.2534 - val_accuracy: 0.7353\n",
            "Epoch 599/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2185 - accuracy: 0.8741 - val_loss: 1.4589 - val_accuracy: 0.7326\n",
            "Epoch 600/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2408 - accuracy: 0.8634 - val_loss: 1.1954 - val_accuracy: 0.7219\n",
            "Epoch 601/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2186 - accuracy: 0.8768 - val_loss: 1.4041 - val_accuracy: 0.7193\n",
            "Epoch 602/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2477 - accuracy: 0.8589 - val_loss: 1.2411 - val_accuracy: 0.7005\n",
            "Epoch 603/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2591 - accuracy: 0.8625 - val_loss: 1.1850 - val_accuracy: 0.7086\n",
            "Epoch 604/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2663 - accuracy: 0.8527 - val_loss: 1.1293 - val_accuracy: 0.7353\n",
            "Epoch 605/1000\n",
            "1120/1120 [==============================] - 0s 261us/step - loss: 0.2508 - accuracy: 0.8616 - val_loss: 1.1331 - val_accuracy: 0.7166\n",
            "Epoch 606/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2225 - accuracy: 0.8777 - val_loss: 1.3117 - val_accuracy: 0.7139\n",
            "Epoch 607/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2156 - accuracy: 0.8741 - val_loss: 1.3336 - val_accuracy: 0.7166\n",
            "Epoch 608/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2150 - accuracy: 0.8714 - val_loss: 1.3535 - val_accuracy: 0.7353\n",
            "Epoch 609/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2132 - accuracy: 0.8768 - val_loss: 1.4541 - val_accuracy: 0.7166\n",
            "Epoch 610/1000\n",
            "1120/1120 [==============================] - 0s 233us/step - loss: 0.2315 - accuracy: 0.8652 - val_loss: 1.3220 - val_accuracy: 0.7406\n",
            "Epoch 611/1000\n",
            "1120/1120 [==============================] - 0s 260us/step - loss: 0.2245 - accuracy: 0.8741 - val_loss: 1.4623 - val_accuracy: 0.7059\n",
            "Epoch 612/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2359 - accuracy: 0.8518 - val_loss: 1.3440 - val_accuracy: 0.7219\n",
            "Epoch 613/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2435 - accuracy: 0.8607 - val_loss: 1.3699 - val_accuracy: 0.7353\n",
            "Epoch 614/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2662 - accuracy: 0.8411 - val_loss: 1.1014 - val_accuracy: 0.7273\n",
            "Epoch 615/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.2434 - accuracy: 0.8679 - val_loss: 1.4150 - val_accuracy: 0.7620\n",
            "Epoch 616/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2683 - accuracy: 0.8545 - val_loss: 1.2116 - val_accuracy: 0.7353\n",
            "Epoch 617/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2459 - accuracy: 0.8696 - val_loss: 1.3672 - val_accuracy: 0.7299\n",
            "Epoch 618/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2412 - accuracy: 0.8696 - val_loss: 1.5169 - val_accuracy: 0.7433\n",
            "Epoch 619/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2242 - accuracy: 0.8777 - val_loss: 1.6036 - val_accuracy: 0.7460\n",
            "Epoch 620/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2524 - accuracy: 0.8562 - val_loss: 1.3385 - val_accuracy: 0.7299\n",
            "Epoch 621/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2416 - accuracy: 0.8723 - val_loss: 1.5336 - val_accuracy: 0.7353\n",
            "Epoch 622/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2434 - accuracy: 0.8687 - val_loss: 1.4226 - val_accuracy: 0.7406\n",
            "Epoch 623/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2427 - accuracy: 0.8616 - val_loss: 1.4150 - val_accuracy: 0.7353\n",
            "Epoch 624/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2534 - accuracy: 0.8554 - val_loss: 1.3258 - val_accuracy: 0.7594\n",
            "Epoch 625/1000\n",
            "1120/1120 [==============================] - 0s 262us/step - loss: 0.2638 - accuracy: 0.8598 - val_loss: 1.3963 - val_accuracy: 0.7139\n",
            "Epoch 626/1000\n",
            "1120/1120 [==============================] - 0s 266us/step - loss: 0.2528 - accuracy: 0.8527 - val_loss: 1.3836 - val_accuracy: 0.7299\n",
            "Epoch 627/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2418 - accuracy: 0.8670 - val_loss: 1.3378 - val_accuracy: 0.7406\n",
            "Epoch 628/1000\n",
            "1120/1120 [==============================] - 0s 262us/step - loss: 0.2574 - accuracy: 0.8536 - val_loss: 1.3668 - val_accuracy: 0.7353\n",
            "Epoch 629/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2788 - accuracy: 0.8607 - val_loss: 1.0768 - val_accuracy: 0.7246\n",
            "Epoch 630/1000\n",
            "1120/1120 [==============================] - 0s 264us/step - loss: 0.2433 - accuracy: 0.8679 - val_loss: 1.2184 - val_accuracy: 0.7353\n",
            "Epoch 631/1000\n",
            "1120/1120 [==============================] - 0s 260us/step - loss: 0.2450 - accuracy: 0.8696 - val_loss: 1.2914 - val_accuracy: 0.7353\n",
            "Epoch 632/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2696 - accuracy: 0.8491 - val_loss: 1.1910 - val_accuracy: 0.7299\n",
            "Epoch 633/1000\n",
            "1120/1120 [==============================] - 0s 263us/step - loss: 0.2406 - accuracy: 0.8670 - val_loss: 1.2640 - val_accuracy: 0.7246\n",
            "Epoch 634/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2534 - accuracy: 0.8696 - val_loss: 1.2059 - val_accuracy: 0.7246\n",
            "Epoch 635/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2465 - accuracy: 0.8714 - val_loss: 1.1901 - val_accuracy: 0.7193\n",
            "Epoch 636/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2300 - accuracy: 0.8741 - val_loss: 1.1872 - val_accuracy: 0.7460\n",
            "Epoch 637/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2554 - accuracy: 0.8616 - val_loss: 1.1868 - val_accuracy: 0.7353\n",
            "Epoch 638/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.3194 - accuracy: 0.8330 - val_loss: 0.9768 - val_accuracy: 0.7433\n",
            "Epoch 639/1000\n",
            "1120/1120 [==============================] - 0s 267us/step - loss: 0.2523 - accuracy: 0.8679 - val_loss: 1.2385 - val_accuracy: 0.7326\n",
            "Epoch 640/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2400 - accuracy: 0.8732 - val_loss: 1.2543 - val_accuracy: 0.7326\n",
            "Epoch 641/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2351 - accuracy: 0.8741 - val_loss: 1.3364 - val_accuracy: 0.7594\n",
            "Epoch 642/1000\n",
            "1120/1120 [==============================] - 0s 231us/step - loss: 0.2236 - accuracy: 0.8786 - val_loss: 1.4716 - val_accuracy: 0.7299\n",
            "Epoch 643/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2327 - accuracy: 0.8741 - val_loss: 1.3401 - val_accuracy: 0.7433\n",
            "Epoch 644/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2514 - accuracy: 0.8536 - val_loss: 1.2517 - val_accuracy: 0.7353\n",
            "Epoch 645/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2274 - accuracy: 0.8777 - val_loss: 1.3686 - val_accuracy: 0.7406\n",
            "Epoch 646/1000\n",
            "1120/1120 [==============================] - 0s 229us/step - loss: 0.2282 - accuracy: 0.8723 - val_loss: 1.4514 - val_accuracy: 0.7219\n",
            "Epoch 647/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2307 - accuracy: 0.8652 - val_loss: 1.4371 - val_accuracy: 0.7299\n",
            "Epoch 648/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2710 - accuracy: 0.8598 - val_loss: 1.2088 - val_accuracy: 0.7193\n",
            "Epoch 649/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2402 - accuracy: 0.8687 - val_loss: 1.3360 - val_accuracy: 0.7487\n",
            "Epoch 650/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2263 - accuracy: 0.8741 - val_loss: 1.4164 - val_accuracy: 0.7299\n",
            "Epoch 651/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2373 - accuracy: 0.8714 - val_loss: 1.3926 - val_accuracy: 0.7620\n",
            "Epoch 652/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2231 - accuracy: 0.8714 - val_loss: 1.4608 - val_accuracy: 0.7460\n",
            "Epoch 653/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2270 - accuracy: 0.8786 - val_loss: 1.5220 - val_accuracy: 0.7353\n",
            "Epoch 654/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2777 - accuracy: 0.8446 - val_loss: 1.3223 - val_accuracy: 0.7353\n",
            "Epoch 655/1000\n",
            "1120/1120 [==============================] - 0s 262us/step - loss: 0.2541 - accuracy: 0.8670 - val_loss: 1.3133 - val_accuracy: 0.7273\n",
            "Epoch 656/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2304 - accuracy: 0.8795 - val_loss: 1.4455 - val_accuracy: 0.7246\n",
            "Epoch 657/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2459 - accuracy: 0.8687 - val_loss: 1.3360 - val_accuracy: 0.7433\n",
            "Epoch 658/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2405 - accuracy: 0.8723 - val_loss: 1.3503 - val_accuracy: 0.7353\n",
            "Epoch 659/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2272 - accuracy: 0.8786 - val_loss: 1.5660 - val_accuracy: 0.7193\n",
            "Epoch 660/1000\n",
            "1120/1120 [==============================] - 0s 228us/step - loss: 0.2494 - accuracy: 0.8696 - val_loss: 1.3092 - val_accuracy: 0.7567\n",
            "Epoch 661/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2368 - accuracy: 0.8661 - val_loss: 1.4560 - val_accuracy: 0.7353\n",
            "Epoch 662/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2195 - accuracy: 0.8777 - val_loss: 1.5362 - val_accuracy: 0.7166\n",
            "Epoch 663/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2217 - accuracy: 0.8714 - val_loss: 1.6563 - val_accuracy: 0.7112\n",
            "Epoch 664/1000\n",
            "1120/1120 [==============================] - 0s 230us/step - loss: 0.2294 - accuracy: 0.8705 - val_loss: 1.5134 - val_accuracy: 0.7112\n",
            "Epoch 665/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2433 - accuracy: 0.8670 - val_loss: 1.3491 - val_accuracy: 0.7460\n",
            "Epoch 666/1000\n",
            "1120/1120 [==============================] - 0s 264us/step - loss: 0.2365 - accuracy: 0.8714 - val_loss: 1.3550 - val_accuracy: 0.7086\n",
            "Epoch 667/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2442 - accuracy: 0.8625 - val_loss: 1.3887 - val_accuracy: 0.7219\n",
            "Epoch 668/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2338 - accuracy: 0.8759 - val_loss: 1.4678 - val_accuracy: 0.7487\n",
            "Epoch 669/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2266 - accuracy: 0.8732 - val_loss: 1.5690 - val_accuracy: 0.7326\n",
            "Epoch 670/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2292 - accuracy: 0.8768 - val_loss: 1.6050 - val_accuracy: 0.7299\n",
            "Epoch 671/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2387 - accuracy: 0.8714 - val_loss: 1.3819 - val_accuracy: 0.7353\n",
            "Epoch 672/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2346 - accuracy: 0.8750 - val_loss: 1.3569 - val_accuracy: 0.7326\n",
            "Epoch 673/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2200 - accuracy: 0.8795 - val_loss: 1.5124 - val_accuracy: 0.7326\n",
            "Epoch 674/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2485 - accuracy: 0.8643 - val_loss: 1.1573 - val_accuracy: 0.7380\n",
            "Epoch 675/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2340 - accuracy: 0.8652 - val_loss: 1.4532 - val_accuracy: 0.7193\n",
            "Epoch 676/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2219 - accuracy: 0.8741 - val_loss: 1.4195 - val_accuracy: 0.7353\n",
            "Epoch 677/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2225 - accuracy: 0.8813 - val_loss: 1.5404 - val_accuracy: 0.7353\n",
            "Epoch 678/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2180 - accuracy: 0.8795 - val_loss: 1.5562 - val_accuracy: 0.7540\n",
            "Epoch 679/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2137 - accuracy: 0.8786 - val_loss: 1.5970 - val_accuracy: 0.7433\n",
            "Epoch 680/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2236 - accuracy: 0.8777 - val_loss: 1.5004 - val_accuracy: 0.7166\n",
            "Epoch 681/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2520 - accuracy: 0.8661 - val_loss: 1.6640 - val_accuracy: 0.7005\n",
            "Epoch 682/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2694 - accuracy: 0.8509 - val_loss: 1.3006 - val_accuracy: 0.7273\n",
            "Epoch 683/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2817 - accuracy: 0.8438 - val_loss: 1.1943 - val_accuracy: 0.7112\n",
            "Epoch 684/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2344 - accuracy: 0.8750 - val_loss: 1.2912 - val_accuracy: 0.7353\n",
            "Epoch 685/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2194 - accuracy: 0.8759 - val_loss: 1.3698 - val_accuracy: 0.7353\n",
            "Epoch 686/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2122 - accuracy: 0.8804 - val_loss: 1.4985 - val_accuracy: 0.7380\n",
            "Epoch 687/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2150 - accuracy: 0.8821 - val_loss: 1.5086 - val_accuracy: 0.7326\n",
            "Epoch 688/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2402 - accuracy: 0.8696 - val_loss: 1.3015 - val_accuracy: 0.7193\n",
            "Epoch 689/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2362 - accuracy: 0.8723 - val_loss: 1.4677 - val_accuracy: 0.7353\n",
            "Epoch 690/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2168 - accuracy: 0.8795 - val_loss: 1.5884 - val_accuracy: 0.7380\n",
            "Epoch 691/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2472 - accuracy: 0.8625 - val_loss: 1.2009 - val_accuracy: 0.7406\n",
            "Epoch 692/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2225 - accuracy: 0.8768 - val_loss: 1.5299 - val_accuracy: 0.7460\n",
            "Epoch 693/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2192 - accuracy: 0.8741 - val_loss: 1.3685 - val_accuracy: 0.7273\n",
            "Epoch 694/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2076 - accuracy: 0.8839 - val_loss: 1.6461 - val_accuracy: 0.7246\n",
            "Epoch 695/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.3213 - accuracy: 0.8393 - val_loss: 1.0152 - val_accuracy: 0.6364\n",
            "Epoch 696/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.3432 - accuracy: 0.8286 - val_loss: 0.7811 - val_accuracy: 0.7032\n",
            "Epoch 697/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2416 - accuracy: 0.8687 - val_loss: 1.1855 - val_accuracy: 0.7487\n",
            "Epoch 698/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2283 - accuracy: 0.8696 - val_loss: 1.2627 - val_accuracy: 0.7353\n",
            "Epoch 699/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2150 - accuracy: 0.8821 - val_loss: 1.4055 - val_accuracy: 0.7460\n",
            "Epoch 700/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2169 - accuracy: 0.8786 - val_loss: 1.4112 - val_accuracy: 0.7326\n",
            "Epoch 701/1000\n",
            "1120/1120 [==============================] - 0s 258us/step - loss: 0.2279 - accuracy: 0.8759 - val_loss: 1.3272 - val_accuracy: 0.7540\n",
            "Epoch 702/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2223 - accuracy: 0.8732 - val_loss: 1.3815 - val_accuracy: 0.7219\n",
            "Epoch 703/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2493 - accuracy: 0.8625 - val_loss: 1.1581 - val_accuracy: 0.7406\n",
            "Epoch 704/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2244 - accuracy: 0.8714 - val_loss: 1.3881 - val_accuracy: 0.7406\n",
            "Epoch 705/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2142 - accuracy: 0.8750 - val_loss: 1.4365 - val_accuracy: 0.7487\n",
            "Epoch 706/1000\n",
            "1120/1120 [==============================] - 0s 233us/step - loss: 0.2375 - accuracy: 0.8625 - val_loss: 1.3032 - val_accuracy: 0.7406\n",
            "Epoch 707/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2309 - accuracy: 0.8741 - val_loss: 1.5514 - val_accuracy: 0.7326\n",
            "Epoch 708/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2288 - accuracy: 0.8759 - val_loss: 1.4505 - val_accuracy: 0.7166\n",
            "Epoch 709/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2422 - accuracy: 0.8554 - val_loss: 1.6894 - val_accuracy: 0.7299\n",
            "Epoch 710/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2366 - accuracy: 0.8679 - val_loss: 1.6444 - val_accuracy: 0.7406\n",
            "Epoch 711/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2239 - accuracy: 0.8741 - val_loss: 1.7362 - val_accuracy: 0.7219\n",
            "Epoch 712/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3402 - accuracy: 0.8268 - val_loss: 1.1175 - val_accuracy: 0.6551\n",
            "Epoch 713/1000\n",
            "1120/1120 [==============================] - 0s 266us/step - loss: 0.4719 - accuracy: 0.7437 - val_loss: 0.8594 - val_accuracy: 0.6738\n",
            "Epoch 714/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.3270 - accuracy: 0.8286 - val_loss: 1.2474 - val_accuracy: 0.7353\n",
            "Epoch 715/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.3505 - accuracy: 0.8205 - val_loss: 0.9616 - val_accuracy: 0.7139\n",
            "Epoch 716/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2786 - accuracy: 0.8562 - val_loss: 1.0515 - val_accuracy: 0.7353\n",
            "Epoch 717/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2459 - accuracy: 0.8696 - val_loss: 1.1644 - val_accuracy: 0.7433\n",
            "Epoch 718/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.2269 - accuracy: 0.8813 - val_loss: 1.4124 - val_accuracy: 0.7513\n",
            "Epoch 719/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2631 - accuracy: 0.8562 - val_loss: 1.2466 - val_accuracy: 0.7326\n",
            "Epoch 720/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2251 - accuracy: 0.8830 - val_loss: 1.6340 - val_accuracy: 0.7433\n",
            "Epoch 721/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2831 - accuracy: 0.8571 - val_loss: 0.9225 - val_accuracy: 0.6952\n",
            "Epoch 722/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.3174 - accuracy: 0.8304 - val_loss: 1.0397 - val_accuracy: 0.7005\n",
            "Epoch 723/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2520 - accuracy: 0.8643 - val_loss: 1.2375 - val_accuracy: 0.7326\n",
            "Epoch 724/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2246 - accuracy: 0.8750 - val_loss: 1.1741 - val_accuracy: 0.7406\n",
            "Epoch 725/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2356 - accuracy: 0.8661 - val_loss: 1.0858 - val_accuracy: 0.7193\n",
            "Epoch 726/1000\n",
            "1120/1120 [==============================] - 0s 268us/step - loss: 0.2203 - accuracy: 0.8732 - val_loss: 1.2000 - val_accuracy: 0.7353\n",
            "Epoch 727/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2174 - accuracy: 0.8732 - val_loss: 1.2822 - val_accuracy: 0.7326\n",
            "Epoch 728/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2219 - accuracy: 0.8714 - val_loss: 1.3394 - val_accuracy: 0.7219\n",
            "Epoch 729/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2518 - accuracy: 0.8571 - val_loss: 1.0790 - val_accuracy: 0.7540\n",
            "Epoch 730/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2269 - accuracy: 0.8714 - val_loss: 1.4200 - val_accuracy: 0.7139\n",
            "Epoch 731/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2349 - accuracy: 0.8687 - val_loss: 1.4675 - val_accuracy: 0.7086\n",
            "Epoch 732/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2208 - accuracy: 0.8723 - val_loss: 1.5636 - val_accuracy: 0.7193\n",
            "Epoch 733/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2051 - accuracy: 0.8804 - val_loss: 1.7385 - val_accuracy: 0.7433\n",
            "Epoch 734/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2151 - accuracy: 0.8839 - val_loss: 1.5825 - val_accuracy: 0.7005\n",
            "Epoch 735/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2347 - accuracy: 0.8616 - val_loss: 1.5605 - val_accuracy: 0.7193\n",
            "Epoch 736/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2714 - accuracy: 0.8500 - val_loss: 1.2786 - val_accuracy: 0.7032\n",
            "Epoch 737/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2317 - accuracy: 0.8723 - val_loss: 1.3613 - val_accuracy: 0.7193\n",
            "Epoch 738/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2176 - accuracy: 0.8723 - val_loss: 1.4513 - val_accuracy: 0.7139\n",
            "Epoch 739/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2129 - accuracy: 0.8795 - val_loss: 1.6310 - val_accuracy: 0.7166\n",
            "Epoch 740/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2130 - accuracy: 0.8759 - val_loss: 1.7589 - val_accuracy: 0.7112\n",
            "Epoch 741/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2247 - accuracy: 0.8714 - val_loss: 1.5039 - val_accuracy: 0.7193\n",
            "Epoch 742/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2222 - accuracy: 0.8741 - val_loss: 1.6320 - val_accuracy: 0.7005\n",
            "Epoch 743/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2266 - accuracy: 0.8661 - val_loss: 1.6504 - val_accuracy: 0.7005\n",
            "Epoch 744/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2351 - accuracy: 0.8661 - val_loss: 1.4771 - val_accuracy: 0.7166\n",
            "Epoch 745/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2244 - accuracy: 0.8777 - val_loss: 1.4855 - val_accuracy: 0.7139\n",
            "Epoch 746/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2196 - accuracy: 0.8705 - val_loss: 1.6305 - val_accuracy: 0.7166\n",
            "Epoch 747/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2722 - accuracy: 0.8616 - val_loss: 1.0443 - val_accuracy: 0.6952\n",
            "Epoch 748/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2783 - accuracy: 0.8616 - val_loss: 0.9408 - val_accuracy: 0.7273\n",
            "Epoch 749/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2318 - accuracy: 0.8732 - val_loss: 1.2773 - val_accuracy: 0.7166\n",
            "Epoch 750/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2410 - accuracy: 0.8705 - val_loss: 1.1663 - val_accuracy: 0.7139\n",
            "Epoch 751/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2481 - accuracy: 0.8580 - val_loss: 1.1264 - val_accuracy: 0.7567\n",
            "Epoch 752/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2273 - accuracy: 0.8795 - val_loss: 1.5005 - val_accuracy: 0.7353\n",
            "Epoch 753/1000\n",
            "1120/1120 [==============================] - 0s 230us/step - loss: 0.2426 - accuracy: 0.8589 - val_loss: 1.4082 - val_accuracy: 0.7139\n",
            "Epoch 754/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2870 - accuracy: 0.8446 - val_loss: 1.0699 - val_accuracy: 0.7219\n",
            "Epoch 755/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2578 - accuracy: 0.8687 - val_loss: 1.1808 - val_accuracy: 0.7326\n",
            "Epoch 756/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2274 - accuracy: 0.8750 - val_loss: 1.4173 - val_accuracy: 0.7406\n",
            "Epoch 757/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2292 - accuracy: 0.8777 - val_loss: 1.4240 - val_accuracy: 0.7460\n",
            "Epoch 758/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2315 - accuracy: 0.8732 - val_loss: 1.2642 - val_accuracy: 0.7460\n",
            "Epoch 759/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2183 - accuracy: 0.8804 - val_loss: 1.5345 - val_accuracy: 0.7380\n",
            "Epoch 760/1000\n",
            "1120/1120 [==============================] - 0s 232us/step - loss: 0.2187 - accuracy: 0.8813 - val_loss: 1.5441 - val_accuracy: 0.7433\n",
            "Epoch 761/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2201 - accuracy: 0.8795 - val_loss: 1.5460 - val_accuracy: 0.7433\n",
            "Epoch 762/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2196 - accuracy: 0.8750 - val_loss: 1.6073 - val_accuracy: 0.7487\n",
            "Epoch 763/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2312 - accuracy: 0.8652 - val_loss: 1.5202 - val_accuracy: 0.7166\n",
            "Epoch 764/1000\n",
            "1120/1120 [==============================] - 0s 233us/step - loss: 0.2936 - accuracy: 0.8446 - val_loss: 1.0757 - val_accuracy: 0.7005\n",
            "Epoch 765/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.3677 - accuracy: 0.8241 - val_loss: 0.7695 - val_accuracy: 0.7433\n",
            "Epoch 766/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2635 - accuracy: 0.8696 - val_loss: 1.2102 - val_accuracy: 0.7433\n",
            "Epoch 767/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2692 - accuracy: 0.8643 - val_loss: 1.1820 - val_accuracy: 0.7299\n",
            "Epoch 768/1000\n",
            "1120/1120 [==============================] - 0s 226us/step - loss: 0.2725 - accuracy: 0.8598 - val_loss: 1.1144 - val_accuracy: 0.7246\n",
            "Epoch 769/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2259 - accuracy: 0.8759 - val_loss: 1.3453 - val_accuracy: 0.7406\n",
            "Epoch 770/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2228 - accuracy: 0.8830 - val_loss: 1.2536 - val_accuracy: 0.7380\n",
            "Epoch 771/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2567 - accuracy: 0.8634 - val_loss: 1.1144 - val_accuracy: 0.7353\n",
            "Epoch 772/1000\n",
            "1120/1120 [==============================] - 0s 229us/step - loss: 0.2485 - accuracy: 0.8607 - val_loss: 1.2026 - val_accuracy: 0.7139\n",
            "Epoch 773/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2494 - accuracy: 0.8714 - val_loss: 1.2526 - val_accuracy: 0.7487\n",
            "Epoch 774/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2571 - accuracy: 0.8661 - val_loss: 1.0681 - val_accuracy: 0.7246\n",
            "Epoch 775/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2673 - accuracy: 0.8536 - val_loss: 1.0427 - val_accuracy: 0.7166\n",
            "Epoch 776/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2911 - accuracy: 0.8482 - val_loss: 1.0198 - val_accuracy: 0.7460\n",
            "Epoch 777/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2820 - accuracy: 0.8455 - val_loss: 1.1684 - val_accuracy: 0.7460\n",
            "Epoch 778/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2361 - accuracy: 0.8768 - val_loss: 1.2775 - val_accuracy: 0.7460\n",
            "Epoch 779/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2357 - accuracy: 0.8804 - val_loss: 1.2225 - val_accuracy: 0.7219\n",
            "Epoch 780/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2332 - accuracy: 0.8714 - val_loss: 1.2184 - val_accuracy: 0.7540\n",
            "Epoch 781/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2358 - accuracy: 0.8732 - val_loss: 1.3628 - val_accuracy: 0.7353\n",
            "Epoch 782/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2258 - accuracy: 0.8750 - val_loss: 1.2716 - val_accuracy: 0.7460\n",
            "Epoch 783/1000\n",
            "1120/1120 [==============================] - 0s 232us/step - loss: 0.2169 - accuracy: 0.8795 - val_loss: 1.3444 - val_accuracy: 0.7299\n",
            "Epoch 784/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2262 - accuracy: 0.8768 - val_loss: 1.2726 - val_accuracy: 0.7433\n",
            "Epoch 785/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2295 - accuracy: 0.8830 - val_loss: 1.2098 - val_accuracy: 0.7567\n",
            "Epoch 786/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2189 - accuracy: 0.8795 - val_loss: 1.3781 - val_accuracy: 0.7299\n",
            "Epoch 787/1000\n",
            "1120/1120 [==============================] - 0s 228us/step - loss: 0.2274 - accuracy: 0.8786 - val_loss: 1.1523 - val_accuracy: 0.7380\n",
            "Epoch 788/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2350 - accuracy: 0.8839 - val_loss: 1.2462 - val_accuracy: 0.7326\n",
            "Epoch 789/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2347 - accuracy: 0.8723 - val_loss: 1.3922 - val_accuracy: 0.7299\n",
            "Epoch 790/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2371 - accuracy: 0.8777 - val_loss: 1.1610 - val_accuracy: 0.7299\n",
            "Epoch 791/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2363 - accuracy: 0.8732 - val_loss: 1.1973 - val_accuracy: 0.7219\n",
            "Epoch 792/1000\n",
            "1120/1120 [==============================] - 0s 232us/step - loss: 0.2295 - accuracy: 0.8732 - val_loss: 1.3426 - val_accuracy: 0.7380\n",
            "Epoch 793/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2242 - accuracy: 0.8813 - val_loss: 1.3207 - val_accuracy: 0.7406\n",
            "Epoch 794/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2193 - accuracy: 0.8777 - val_loss: 1.5147 - val_accuracy: 0.7326\n",
            "Epoch 795/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2177 - accuracy: 0.8786 - val_loss: 1.5174 - val_accuracy: 0.7433\n",
            "Epoch 796/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2145 - accuracy: 0.8750 - val_loss: 1.4844 - val_accuracy: 0.7353\n",
            "Epoch 797/1000\n",
            "1120/1120 [==============================] - 0s 223us/step - loss: 0.2141 - accuracy: 0.8813 - val_loss: 1.4887 - val_accuracy: 0.7326\n",
            "Epoch 798/1000\n",
            "1120/1120 [==============================] - 0s 225us/step - loss: 0.2283 - accuracy: 0.8696 - val_loss: 1.3893 - val_accuracy: 0.7299\n",
            "Epoch 799/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.2184 - accuracy: 0.8795 - val_loss: 1.5469 - val_accuracy: 0.7727\n",
            "Epoch 800/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2231 - accuracy: 0.8714 - val_loss: 1.4672 - val_accuracy: 0.7460\n",
            "Epoch 801/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2193 - accuracy: 0.8768 - val_loss: 1.5231 - val_accuracy: 0.7380\n",
            "Epoch 802/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2149 - accuracy: 0.8795 - val_loss: 1.4826 - val_accuracy: 0.7406\n",
            "Epoch 803/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2231 - accuracy: 0.8777 - val_loss: 1.4938 - val_accuracy: 0.7273\n",
            "Epoch 804/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2231 - accuracy: 0.8777 - val_loss: 1.4259 - val_accuracy: 0.7299\n",
            "Epoch 805/1000\n",
            "1120/1120 [==============================] - 0s 224us/step - loss: 0.2188 - accuracy: 0.8830 - val_loss: 1.5438 - val_accuracy: 0.7273\n",
            "Epoch 806/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2339 - accuracy: 0.8571 - val_loss: 1.3688 - val_accuracy: 0.7433\n",
            "Epoch 807/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2212 - accuracy: 0.8750 - val_loss: 1.5759 - val_accuracy: 0.7326\n",
            "Epoch 808/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2182 - accuracy: 0.8795 - val_loss: 1.4479 - val_accuracy: 0.7353\n",
            "Epoch 809/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2532 - accuracy: 0.8714 - val_loss: 1.4521 - val_accuracy: 0.7433\n",
            "Epoch 810/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2964 - accuracy: 0.8366 - val_loss: 0.9578 - val_accuracy: 0.7380\n",
            "Epoch 811/1000\n",
            "1120/1120 [==============================] - 0s 228us/step - loss: 0.2505 - accuracy: 0.8670 - val_loss: 1.2597 - val_accuracy: 0.7380\n",
            "Epoch 812/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2239 - accuracy: 0.8777 - val_loss: 1.2947 - val_accuracy: 0.7380\n",
            "Epoch 813/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2090 - accuracy: 0.8821 - val_loss: 1.3810 - val_accuracy: 0.7299\n",
            "Epoch 814/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2096 - accuracy: 0.8768 - val_loss: 1.3726 - val_accuracy: 0.7487\n",
            "Epoch 815/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2128 - accuracy: 0.8723 - val_loss: 1.2982 - val_accuracy: 0.7406\n",
            "Epoch 816/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2196 - accuracy: 0.8741 - val_loss: 1.4539 - val_accuracy: 0.7326\n",
            "Epoch 817/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2042 - accuracy: 0.8821 - val_loss: 1.7401 - val_accuracy: 0.7273\n",
            "Epoch 818/1000\n",
            "1120/1120 [==============================] - 0s 232us/step - loss: 0.2079 - accuracy: 0.8786 - val_loss: 1.9700 - val_accuracy: 0.7433\n",
            "Epoch 819/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2164 - accuracy: 0.8679 - val_loss: 1.8818 - val_accuracy: 0.7380\n",
            "Epoch 820/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2438 - accuracy: 0.8696 - val_loss: 1.3658 - val_accuracy: 0.7112\n",
            "Epoch 821/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2537 - accuracy: 0.8679 - val_loss: 1.3126 - val_accuracy: 0.7086\n",
            "Epoch 822/1000\n",
            "1120/1120 [==============================] - 0s 229us/step - loss: 0.2301 - accuracy: 0.8741 - val_loss: 1.2402 - val_accuracy: 0.7353\n",
            "Epoch 823/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2354 - accuracy: 0.8714 - val_loss: 1.2416 - val_accuracy: 0.7139\n",
            "Epoch 824/1000\n",
            "1120/1120 [==============================] - 0s 262us/step - loss: 0.2437 - accuracy: 0.8714 - val_loss: 1.0096 - val_accuracy: 0.7513\n",
            "Epoch 825/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.3028 - accuracy: 0.8321 - val_loss: 1.0744 - val_accuracy: 0.7273\n",
            "Epoch 826/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2658 - accuracy: 0.8473 - val_loss: 1.0565 - val_accuracy: 0.7326\n",
            "Epoch 827/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2792 - accuracy: 0.8491 - val_loss: 1.2397 - val_accuracy: 0.6738\n",
            "Epoch 828/1000\n",
            "1120/1120 [==============================] - 0s 231us/step - loss: 0.3519 - accuracy: 0.8000 - val_loss: 0.8765 - val_accuracy: 0.7406\n",
            "Epoch 829/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2532 - accuracy: 0.8687 - val_loss: 1.2480 - val_accuracy: 0.7086\n",
            "Epoch 830/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2193 - accuracy: 0.8768 - val_loss: 1.3881 - val_accuracy: 0.7193\n",
            "Epoch 831/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2201 - accuracy: 0.8768 - val_loss: 1.3397 - val_accuracy: 0.7059\n",
            "Epoch 832/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2139 - accuracy: 0.8750 - val_loss: 1.3521 - val_accuracy: 0.7112\n",
            "Epoch 833/1000\n",
            "1120/1120 [==============================] - 0s 265us/step - loss: 0.2169 - accuracy: 0.8750 - val_loss: 1.3798 - val_accuracy: 0.7460\n",
            "Epoch 834/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2082 - accuracy: 0.8821 - val_loss: 1.4597 - val_accuracy: 0.7487\n",
            "Epoch 835/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2045 - accuracy: 0.8830 - val_loss: 1.5878 - val_accuracy: 0.7299\n",
            "Epoch 836/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2245 - accuracy: 0.8777 - val_loss: 1.2320 - val_accuracy: 0.7246\n",
            "Epoch 837/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2633 - accuracy: 0.8464 - val_loss: 1.1265 - val_accuracy: 0.7112\n",
            "Epoch 838/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2474 - accuracy: 0.8571 - val_loss: 1.1198 - val_accuracy: 0.7326\n",
            "Epoch 839/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2257 - accuracy: 0.8705 - val_loss: 1.3467 - val_accuracy: 0.7299\n",
            "Epoch 840/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2596 - accuracy: 0.8536 - val_loss: 1.2447 - val_accuracy: 0.6979\n",
            "Epoch 841/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2762 - accuracy: 0.8491 - val_loss: 1.0740 - val_accuracy: 0.7406\n",
            "Epoch 842/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.2345 - accuracy: 0.8732 - val_loss: 1.1886 - val_accuracy: 0.7460\n",
            "Epoch 843/1000\n",
            "1120/1120 [==============================] - 0s 267us/step - loss: 0.2433 - accuracy: 0.8473 - val_loss: 1.1813 - val_accuracy: 0.7246\n",
            "Epoch 844/1000\n",
            "1120/1120 [==============================] - 0s 263us/step - loss: 0.2312 - accuracy: 0.8643 - val_loss: 1.2745 - val_accuracy: 0.7193\n",
            "Epoch 845/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2217 - accuracy: 0.8696 - val_loss: 1.4884 - val_accuracy: 0.7193\n",
            "Epoch 846/1000\n",
            "1120/1120 [==============================] - 0s 263us/step - loss: 0.2321 - accuracy: 0.8687 - val_loss: 1.5474 - val_accuracy: 0.7246\n",
            "Epoch 847/1000\n",
            "1120/1120 [==============================] - 0s 264us/step - loss: 0.2676 - accuracy: 0.8625 - val_loss: 1.1127 - val_accuracy: 0.7433\n",
            "Epoch 848/1000\n",
            "1120/1120 [==============================] - 0s 262us/step - loss: 0.2603 - accuracy: 0.8679 - val_loss: 1.2052 - val_accuracy: 0.7380\n",
            "Epoch 849/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2491 - accuracy: 0.8652 - val_loss: 1.3578 - val_accuracy: 0.7380\n",
            "Epoch 850/1000\n",
            "1120/1120 [==============================] - 0s 276us/step - loss: 0.3042 - accuracy: 0.8313 - val_loss: 1.0662 - val_accuracy: 0.7406\n",
            "Epoch 851/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2419 - accuracy: 0.8768 - val_loss: 1.3059 - val_accuracy: 0.7487\n",
            "Epoch 852/1000\n",
            "1120/1120 [==============================] - 0s 231us/step - loss: 0.2327 - accuracy: 0.8786 - val_loss: 1.2759 - val_accuracy: 0.7487\n",
            "Epoch 853/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2363 - accuracy: 0.8598 - val_loss: 1.0033 - val_accuracy: 0.7433\n",
            "Epoch 854/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2187 - accuracy: 0.8750 - val_loss: 1.1373 - val_accuracy: 0.7380\n",
            "Epoch 855/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2252 - accuracy: 0.8759 - val_loss: 1.2382 - val_accuracy: 0.7433\n",
            "Epoch 856/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2272 - accuracy: 0.8804 - val_loss: 1.3511 - val_accuracy: 0.7406\n",
            "Epoch 857/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2260 - accuracy: 0.8777 - val_loss: 1.1328 - val_accuracy: 0.7433\n",
            "Epoch 858/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2242 - accuracy: 0.8714 - val_loss: 1.3258 - val_accuracy: 0.7246\n",
            "Epoch 859/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2308 - accuracy: 0.8670 - val_loss: 1.1528 - val_accuracy: 0.7513\n",
            "Epoch 860/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2249 - accuracy: 0.8813 - val_loss: 1.4719 - val_accuracy: 0.7380\n",
            "Epoch 861/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2288 - accuracy: 0.8732 - val_loss: 1.2900 - val_accuracy: 0.7380\n",
            "Epoch 862/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2429 - accuracy: 0.8696 - val_loss: 1.2225 - val_accuracy: 0.7219\n",
            "Epoch 863/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2254 - accuracy: 0.8804 - val_loss: 1.2620 - val_accuracy: 0.7380\n",
            "Epoch 864/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2172 - accuracy: 0.8723 - val_loss: 1.4968 - val_accuracy: 0.7219\n",
            "Epoch 865/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2202 - accuracy: 0.8768 - val_loss: 1.3486 - val_accuracy: 0.7567\n",
            "Epoch 866/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2147 - accuracy: 0.8768 - val_loss: 1.4293 - val_accuracy: 0.7380\n",
            "Epoch 867/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2127 - accuracy: 0.8795 - val_loss: 1.4358 - val_accuracy: 0.7487\n",
            "Epoch 868/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2330 - accuracy: 0.8652 - val_loss: 1.2728 - val_accuracy: 0.7433\n",
            "Epoch 869/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2170 - accuracy: 0.8786 - val_loss: 1.3604 - val_accuracy: 0.7406\n",
            "Epoch 870/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2137 - accuracy: 0.8759 - val_loss: 1.4506 - val_accuracy: 0.7460\n",
            "Epoch 871/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2058 - accuracy: 0.8804 - val_loss: 1.5081 - val_accuracy: 0.7380\n",
            "Epoch 872/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2933 - accuracy: 0.8518 - val_loss: 1.1333 - val_accuracy: 0.7246\n",
            "Epoch 873/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2835 - accuracy: 0.8491 - val_loss: 1.1817 - val_accuracy: 0.7353\n",
            "Epoch 874/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2183 - accuracy: 0.8813 - val_loss: 1.4864 - val_accuracy: 0.7460\n",
            "Epoch 875/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2219 - accuracy: 0.8634 - val_loss: 1.3443 - val_accuracy: 0.7326\n",
            "Epoch 876/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2503 - accuracy: 0.8625 - val_loss: 1.2802 - val_accuracy: 0.7139\n",
            "Epoch 877/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2586 - accuracy: 0.8670 - val_loss: 1.2126 - val_accuracy: 0.7326\n",
            "Epoch 878/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2390 - accuracy: 0.8759 - val_loss: 1.3178 - val_accuracy: 0.7193\n",
            "Epoch 879/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2422 - accuracy: 0.8589 - val_loss: 1.2538 - val_accuracy: 0.7246\n",
            "Epoch 880/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2122 - accuracy: 0.8821 - val_loss: 1.4844 - val_accuracy: 0.7326\n",
            "Epoch 881/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2123 - accuracy: 0.8866 - val_loss: 1.4690 - val_accuracy: 0.7433\n",
            "Epoch 882/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2272 - accuracy: 0.8696 - val_loss: 1.3324 - val_accuracy: 0.7353\n",
            "Epoch 883/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.2187 - accuracy: 0.8750 - val_loss: 1.4038 - val_accuracy: 0.7326\n",
            "Epoch 884/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2104 - accuracy: 0.8866 - val_loss: 1.4005 - val_accuracy: 0.7513\n",
            "Epoch 885/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2330 - accuracy: 0.8768 - val_loss: 1.3253 - val_accuracy: 0.7219\n",
            "Epoch 886/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2564 - accuracy: 0.8634 - val_loss: 1.1599 - val_accuracy: 0.7273\n",
            "Epoch 887/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2290 - accuracy: 0.8759 - val_loss: 1.1987 - val_accuracy: 0.7513\n",
            "Epoch 888/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2402 - accuracy: 0.8634 - val_loss: 1.4318 - val_accuracy: 0.7193\n",
            "Epoch 889/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2441 - accuracy: 0.8714 - val_loss: 1.4173 - val_accuracy: 0.7246\n",
            "Epoch 890/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2582 - accuracy: 0.8625 - val_loss: 1.2459 - val_accuracy: 0.7353\n",
            "Epoch 891/1000\n",
            "1120/1120 [==============================] - 0s 235us/step - loss: 0.2317 - accuracy: 0.8687 - val_loss: 1.4123 - val_accuracy: 0.7112\n",
            "Epoch 892/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2255 - accuracy: 0.8759 - val_loss: 1.4450 - val_accuracy: 0.7513\n",
            "Epoch 893/1000\n",
            "1120/1120 [==============================] - 0s 258us/step - loss: 0.2551 - accuracy: 0.8607 - val_loss: 2.0469 - val_accuracy: 0.6578\n",
            "Epoch 894/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.5000 - accuracy: 0.7563 - val_loss: 0.6404 - val_accuracy: 0.6711\n",
            "Epoch 895/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.3221 - accuracy: 0.8402 - val_loss: 1.0950 - val_accuracy: 0.7219\n",
            "Epoch 896/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2208 - accuracy: 0.8679 - val_loss: 1.2905 - val_accuracy: 0.7647\n",
            "Epoch 897/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.2116 - accuracy: 0.8866 - val_loss: 1.4922 - val_accuracy: 0.7139\n",
            "Epoch 898/1000\n",
            "1120/1120 [==============================] - 0s 253us/step - loss: 0.2080 - accuracy: 0.8777 - val_loss: 1.4699 - val_accuracy: 0.7273\n",
            "Epoch 899/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2064 - accuracy: 0.8804 - val_loss: 1.4934 - val_accuracy: 0.7406\n",
            "Epoch 900/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2062 - accuracy: 0.8804 - val_loss: 1.5512 - val_accuracy: 0.7273\n",
            "Epoch 901/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2038 - accuracy: 0.8795 - val_loss: 1.5327 - val_accuracy: 0.7460\n",
            "Epoch 902/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2032 - accuracy: 0.8839 - val_loss: 1.6623 - val_accuracy: 0.7112\n",
            "Epoch 903/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2537 - accuracy: 0.8562 - val_loss: 1.3120 - val_accuracy: 0.7193\n",
            "Epoch 904/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2271 - accuracy: 0.8750 - val_loss: 1.8541 - val_accuracy: 0.7139\n",
            "Epoch 905/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2428 - accuracy: 0.8616 - val_loss: 1.4819 - val_accuracy: 0.7460\n",
            "Epoch 906/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2110 - accuracy: 0.8813 - val_loss: 1.9262 - val_accuracy: 0.7299\n",
            "Epoch 907/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2443 - accuracy: 0.8687 - val_loss: 1.1291 - val_accuracy: 0.7273\n",
            "Epoch 908/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2195 - accuracy: 0.8786 - val_loss: 1.3398 - val_accuracy: 0.7353\n",
            "Epoch 909/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2027 - accuracy: 0.8866 - val_loss: 1.5604 - val_accuracy: 0.7513\n",
            "Epoch 910/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2082 - accuracy: 0.8786 - val_loss: 1.3969 - val_accuracy: 0.7246\n",
            "Epoch 911/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2049 - accuracy: 0.8813 - val_loss: 1.3558 - val_accuracy: 0.7326\n",
            "Epoch 912/1000\n",
            "1120/1120 [==============================] - 0s 232us/step - loss: 0.2171 - accuracy: 0.8750 - val_loss: 1.4746 - val_accuracy: 0.7193\n",
            "Epoch 913/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2095 - accuracy: 0.8741 - val_loss: 1.5696 - val_accuracy: 0.7246\n",
            "Epoch 914/1000\n",
            "1120/1120 [==============================] - 0s 229us/step - loss: 0.2093 - accuracy: 0.8786 - val_loss: 1.5220 - val_accuracy: 0.7086\n",
            "Epoch 915/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2158 - accuracy: 0.8759 - val_loss: 1.3978 - val_accuracy: 0.7406\n",
            "Epoch 916/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2119 - accuracy: 0.8777 - val_loss: 1.7529 - val_accuracy: 0.7326\n",
            "Epoch 917/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2260 - accuracy: 0.8696 - val_loss: 1.4416 - val_accuracy: 0.7246\n",
            "Epoch 918/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2161 - accuracy: 0.8750 - val_loss: 1.7531 - val_accuracy: 0.7032\n",
            "Epoch 919/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2704 - accuracy: 0.8598 - val_loss: 1.0590 - val_accuracy: 0.7166\n",
            "Epoch 920/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2247 - accuracy: 0.8759 - val_loss: 1.5923 - val_accuracy: 0.7059\n",
            "Epoch 921/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2349 - accuracy: 0.8741 - val_loss: 1.4840 - val_accuracy: 0.7193\n",
            "Epoch 922/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2715 - accuracy: 0.8571 - val_loss: 1.0535 - val_accuracy: 0.7326\n",
            "Epoch 923/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2264 - accuracy: 0.8732 - val_loss: 1.4753 - val_accuracy: 0.7380\n",
            "Epoch 924/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2121 - accuracy: 0.8768 - val_loss: 1.5134 - val_accuracy: 0.7460\n",
            "Epoch 925/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2204 - accuracy: 0.8732 - val_loss: 1.3964 - val_accuracy: 0.7193\n",
            "Epoch 926/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2195 - accuracy: 0.8705 - val_loss: 1.5237 - val_accuracy: 0.7487\n",
            "Epoch 927/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2084 - accuracy: 0.8795 - val_loss: 1.6170 - val_accuracy: 0.7246\n",
            "Epoch 928/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2309 - accuracy: 0.8786 - val_loss: 1.2704 - val_accuracy: 0.7326\n",
            "Epoch 929/1000\n",
            "1120/1120 [==============================] - 0s 234us/step - loss: 0.2144 - accuracy: 0.8741 - val_loss: 1.4131 - val_accuracy: 0.7299\n",
            "Epoch 930/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2093 - accuracy: 0.8777 - val_loss: 1.5569 - val_accuracy: 0.7139\n",
            "Epoch 931/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2211 - accuracy: 0.8670 - val_loss: 1.2090 - val_accuracy: 0.7326\n",
            "Epoch 932/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2278 - accuracy: 0.8795 - val_loss: 1.5340 - val_accuracy: 0.7112\n",
            "Epoch 933/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2301 - accuracy: 0.8652 - val_loss: 1.5000 - val_accuracy: 0.7139\n",
            "Epoch 934/1000\n",
            "1120/1120 [==============================] - 0s 260us/step - loss: 0.2360 - accuracy: 0.8705 - val_loss: 1.2418 - val_accuracy: 0.7005\n",
            "Epoch 935/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2263 - accuracy: 0.8750 - val_loss: 1.4826 - val_accuracy: 0.6845\n",
            "Epoch 936/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2331 - accuracy: 0.8732 - val_loss: 1.4116 - val_accuracy: 0.7246\n",
            "Epoch 937/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2088 - accuracy: 0.8813 - val_loss: 1.5138 - val_accuracy: 0.7273\n",
            "Epoch 938/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2022 - accuracy: 0.8821 - val_loss: 1.5712 - val_accuracy: 0.7246\n",
            "Epoch 939/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2013 - accuracy: 0.8848 - val_loss: 1.6561 - val_accuracy: 0.7299\n",
            "Epoch 940/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2044 - accuracy: 0.8813 - val_loss: 1.6304 - val_accuracy: 0.7380\n",
            "Epoch 941/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2038 - accuracy: 0.8750 - val_loss: 1.5008 - val_accuracy: 0.7273\n",
            "Epoch 942/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2025 - accuracy: 0.8839 - val_loss: 1.4698 - val_accuracy: 0.7380\n",
            "Epoch 943/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2013 - accuracy: 0.8875 - val_loss: 1.6833 - val_accuracy: 0.7246\n",
            "Epoch 944/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2108 - accuracy: 0.8777 - val_loss: 1.5409 - val_accuracy: 0.7273\n",
            "Epoch 945/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.2122 - accuracy: 0.8768 - val_loss: 1.6282 - val_accuracy: 0.7487\n",
            "Epoch 946/1000\n",
            "1120/1120 [==============================] - 0s 249us/step - loss: 0.2071 - accuracy: 0.8759 - val_loss: 1.8533 - val_accuracy: 0.7139\n",
            "Epoch 947/1000\n",
            "1120/1120 [==============================] - 0s 261us/step - loss: 0.2017 - accuracy: 0.8902 - val_loss: 1.6458 - val_accuracy: 0.7139\n",
            "Epoch 948/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2017 - accuracy: 0.8866 - val_loss: 1.7334 - val_accuracy: 0.7246\n",
            "Epoch 949/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2162 - accuracy: 0.8732 - val_loss: 1.5873 - val_accuracy: 0.7273\n",
            "Epoch 950/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2073 - accuracy: 0.8821 - val_loss: 1.6512 - val_accuracy: 0.7380\n",
            "Epoch 951/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2360 - accuracy: 0.8652 - val_loss: 1.5393 - val_accuracy: 0.6925\n",
            "Epoch 952/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2562 - accuracy: 0.8634 - val_loss: 1.1024 - val_accuracy: 0.7460\n",
            "Epoch 953/1000\n",
            "1120/1120 [==============================] - 0s 236us/step - loss: 0.2317 - accuracy: 0.8714 - val_loss: 1.2813 - val_accuracy: 0.7193\n",
            "Epoch 954/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2471 - accuracy: 0.8652 - val_loss: 1.0932 - val_accuracy: 0.7273\n",
            "Epoch 955/1000\n",
            "1120/1120 [==============================] - 0s 233us/step - loss: 0.2241 - accuracy: 0.8804 - val_loss: 1.3403 - val_accuracy: 0.7299\n",
            "Epoch 956/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2236 - accuracy: 0.8786 - val_loss: 1.2334 - val_accuracy: 0.7326\n",
            "Epoch 957/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2249 - accuracy: 0.8768 - val_loss: 1.3317 - val_accuracy: 0.7273\n",
            "Epoch 958/1000\n",
            "1120/1120 [==============================] - 0s 247us/step - loss: 0.2253 - accuracy: 0.8732 - val_loss: 1.4015 - val_accuracy: 0.7299\n",
            "Epoch 959/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2363 - accuracy: 0.8741 - val_loss: 1.3643 - val_accuracy: 0.7460\n",
            "Epoch 960/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2468 - accuracy: 0.8679 - val_loss: 1.0587 - val_accuracy: 0.7326\n",
            "Epoch 961/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2294 - accuracy: 0.8768 - val_loss: 2.0980 - val_accuracy: 0.7005\n",
            "Epoch 962/1000\n",
            "1120/1120 [==============================] - 0s 231us/step - loss: 0.4462 - accuracy: 0.8027 - val_loss: 0.6698 - val_accuracy: 0.6898\n",
            "Epoch 963/1000\n",
            "1120/1120 [==============================] - 0s 265us/step - loss: 0.2927 - accuracy: 0.8598 - val_loss: 1.0709 - val_accuracy: 0.7193\n",
            "Epoch 964/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2363 - accuracy: 0.8830 - val_loss: 0.8978 - val_accuracy: 0.7460\n",
            "Epoch 965/1000\n",
            "1120/1120 [==============================] - 0s 257us/step - loss: 0.2292 - accuracy: 0.8759 - val_loss: 1.3631 - val_accuracy: 0.7273\n",
            "Epoch 966/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2223 - accuracy: 0.8768 - val_loss: 1.2589 - val_accuracy: 0.7433\n",
            "Epoch 967/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2140 - accuracy: 0.8821 - val_loss: 1.5372 - val_accuracy: 0.7166\n",
            "Epoch 968/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2612 - accuracy: 0.8536 - val_loss: 0.8784 - val_accuracy: 0.7487\n",
            "Epoch 969/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2232 - accuracy: 0.8857 - val_loss: 1.1260 - val_accuracy: 0.7460\n",
            "Epoch 970/1000\n",
            "1120/1120 [==============================] - 0s 242us/step - loss: 0.2173 - accuracy: 0.8759 - val_loss: 1.2766 - val_accuracy: 0.7540\n",
            "Epoch 971/1000\n",
            "1120/1120 [==============================] - 0s 274us/step - loss: 0.2170 - accuracy: 0.8786 - val_loss: 1.1606 - val_accuracy: 0.7353\n",
            "Epoch 972/1000\n",
            "1120/1120 [==============================] - 0s 252us/step - loss: 0.2169 - accuracy: 0.8741 - val_loss: 1.2311 - val_accuracy: 0.7433\n",
            "Epoch 973/1000\n",
            "1120/1120 [==============================] - 0s 243us/step - loss: 0.2099 - accuracy: 0.8830 - val_loss: 1.4918 - val_accuracy: 0.7166\n",
            "Epoch 974/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2078 - accuracy: 0.8839 - val_loss: 1.5035 - val_accuracy: 0.7594\n",
            "Epoch 975/1000\n",
            "1120/1120 [==============================] - 0s 258us/step - loss: 0.2156 - accuracy: 0.8687 - val_loss: 1.4826 - val_accuracy: 0.7353\n",
            "Epoch 976/1000\n",
            "1120/1120 [==============================] - 0s 259us/step - loss: 0.2244 - accuracy: 0.8804 - val_loss: 1.3017 - val_accuracy: 0.7326\n",
            "Epoch 977/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.2950 - accuracy: 0.8384 - val_loss: 1.1395 - val_accuracy: 0.6898\n",
            "Epoch 978/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2869 - accuracy: 0.8509 - val_loss: 1.0351 - val_accuracy: 0.7433\n",
            "Epoch 979/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2161 - accuracy: 0.8777 - val_loss: 1.4943 - val_accuracy: 0.7433\n",
            "Epoch 980/1000\n",
            "1120/1120 [==============================] - 0s 256us/step - loss: 0.2180 - accuracy: 0.8759 - val_loss: 1.3466 - val_accuracy: 0.7487\n",
            "Epoch 981/1000\n",
            "1120/1120 [==============================] - 0s 254us/step - loss: 0.2141 - accuracy: 0.8821 - val_loss: 1.4604 - val_accuracy: 0.7299\n",
            "Epoch 982/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2108 - accuracy: 0.8857 - val_loss: 1.6401 - val_accuracy: 0.7406\n",
            "Epoch 983/1000\n",
            "1120/1120 [==============================] - 0s 250us/step - loss: 0.2124 - accuracy: 0.8750 - val_loss: 1.6067 - val_accuracy: 0.7701\n",
            "Epoch 984/1000\n",
            "1120/1120 [==============================] - 0s 238us/step - loss: 0.2204 - accuracy: 0.8705 - val_loss: 1.5536 - val_accuracy: 0.7406\n",
            "Epoch 985/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2187 - accuracy: 0.8741 - val_loss: 1.6118 - val_accuracy: 0.7326\n",
            "Epoch 986/1000\n",
            "1120/1120 [==============================] - 0s 239us/step - loss: 0.2297 - accuracy: 0.8741 - val_loss: 1.5701 - val_accuracy: 0.7326\n",
            "Epoch 987/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2389 - accuracy: 0.8696 - val_loss: 1.3230 - val_accuracy: 0.7353\n",
            "Epoch 988/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2172 - accuracy: 0.8821 - val_loss: 1.6463 - val_accuracy: 0.7460\n",
            "Epoch 989/1000\n",
            "1120/1120 [==============================] - 0s 245us/step - loss: 0.2084 - accuracy: 0.8848 - val_loss: 1.9086 - val_accuracy: 0.7273\n",
            "Epoch 990/1000\n",
            "1120/1120 [==============================] - 0s 237us/step - loss: 0.2169 - accuracy: 0.8732 - val_loss: 1.4220 - val_accuracy: 0.7433\n",
            "Epoch 991/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2121 - accuracy: 0.8750 - val_loss: 1.7278 - val_accuracy: 0.7353\n",
            "Epoch 992/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2124 - accuracy: 0.8786 - val_loss: 1.8607 - val_accuracy: 0.7353\n",
            "Epoch 993/1000\n",
            "1120/1120 [==============================] - 0s 248us/step - loss: 0.2841 - accuracy: 0.8562 - val_loss: 1.3595 - val_accuracy: 0.6551\n",
            "Epoch 994/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.4495 - accuracy: 0.7795 - val_loss: 0.6245 - val_accuracy: 0.6684\n",
            "Epoch 995/1000\n",
            "1120/1120 [==============================] - 0s 246us/step - loss: 0.3392 - accuracy: 0.8375 - val_loss: 0.8477 - val_accuracy: 0.7460\n",
            "Epoch 996/1000\n",
            "1120/1120 [==============================] - 0s 251us/step - loss: 0.2207 - accuracy: 0.8839 - val_loss: 1.3434 - val_accuracy: 0.7166\n",
            "Epoch 997/1000\n",
            "1120/1120 [==============================] - 0s 244us/step - loss: 0.2619 - accuracy: 0.8687 - val_loss: 1.1164 - val_accuracy: 0.7166\n",
            "Epoch 998/1000\n",
            "1120/1120 [==============================] - 0s 240us/step - loss: 0.2375 - accuracy: 0.8750 - val_loss: 1.0373 - val_accuracy: 0.7406\n",
            "Epoch 999/1000\n",
            "1120/1120 [==============================] - 0s 241us/step - loss: 0.2274 - accuracy: 0.8750 - val_loss: 1.1330 - val_accuracy: 0.7380\n",
            "Epoch 1000/1000\n",
            "1120/1120 [==============================] - 0s 255us/step - loss: 0.2105 - accuracy: 0.8839 - val_loss: 1.2134 - val_accuracy: 0.7433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTePkjbGNb97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load wights file of the best model :\n",
        "wights_file = './Weights-016-0.88060.hdf5' # choose the best checkpoint \n",
        "NN_model.load_weights(wights_file) # load it\n",
        "NN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX9HwH3-Jv1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_train_history(history, title):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  \n",
        "  epochs = range(len(loss))\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "  plt.title(title)\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsCgsGZBJz0n",
        "colab_type": "code",
        "outputId": "ceb15aaa-0b1c-4f7c-f763-116ae0639551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plot_train_history(history, \"Train and validation loss\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydd5gUxdbG37OBJWcQBREMqCB5CVdEAZFkQBEVRBETggFzwgCmq35iAkVFRa4J5KIiXkGMCIqogIhECYIuKCAZlgV2t74/aoqu7qnurp7p2ZlZ6vc880xPh+rqnu63T586dYoYYzAYDAZD6SUj2RUwGAwGQ2IxQm8wGAylHCP0BoPBUMoxQm8wGAylHCP0BoPBUMoxQm8wGAylHCP0hoRARDOI6IoUqMdIIno7AeVOIKJHI9MdiWilzrox7msPER0b6/Ye5a4joq5hl2tIPYzQGw4RERTxKSaifdLvAUHKYoz1ZIz9J1F1TSUYY3MYYyeGURYRzSKiaxzlV2SMrQ2jfMPhSVayK2BIHRhjFcU0Ea0DcA1j7AvnekSUxRgrLMm6GQyG2DEWvcEXIupERHlEdDcR/Q3gDSKqRkT/I6ItRLQ9Ml1P2uaQZUpEg4joWyIaFVn3dyLq6bG/e4hoDRHtJqJlRHSBtMyzLCJqSETfRLb9HEBNj/0sJ6JzpN9ZkeNpFfn9XyL6m4h2EtFsImridX6k3y2JaGGkDu8BKCstcz1vRPQYgI4AXoi8Rb0Qmc+I6PjIdBUiejOy/Xoiup+IMmI5z45jyCGi54hoY+TzHBHlRJbVjNRzBxFtI6I50j7vJqINkWNdSURn6uzPULIYoTfoUgdAdQDHABgMfu28EfldH8A+AC94bN8OwEpw4f0/AK8TEbmsuwZc8KoAeAjA20R0pGZZ7wJYEFn2CACvdoKJAPpLv7sD+IcxtjDyewaAEwDUBrAQwDseZQEAiKgMgKkA3gI/X/8FcKG0iut5Y4zdB2AOgBsj7pobFbsYA35ejgVwBoCBAK6Ulgc5zzL3AWgPoAWA5gDaArg/sux2AHkAagE4AsBwAIyITgRwI4A2jLFK4Odvnca+DCUNY8x8zCfqA37Ddo1MdwJwAEBZj/VbANgu/Z4F7voBgEEAVkvLygNgAOpo1mURgN5+ZYELZyGACtLydwG87VLu8QB2Aygf+f0OgAdd1q0a2U+VyO8JAB6Vzk9eZPp0ABsBkLTtXLFukPMmzWORumZG/ofG0rLrAMyK5Tw7/uM1AHpJy7oDWBeZfhjARwCOV5y/zQC6AshO9jVrPu4fY9EbdNnCGCsQP4ioPBG9EnEf7AIwG0BVIsp02f5vMcEYy49MVlStSEQDiWhRxFWwA8ApsLtg3Mo6Clw090rrrnc7IMbYagDLAZxLROUBnAf+YAARZRLRExEX0i5YlqqrKyjCUQA2sIgSOusQw3mTqQkg23FM6wHUlX5rn2dFvZ3lHhWZfgrAagCfEdFaIronUv5qALcAGAlgMxFNIqKjYEg5jNAbdHGmOb0dwIkA2jHGKoNbsgCg4yZwhYiOAfAquEugBmOsKoAlmuX+BaAaEVWQ5tX32Ua4b3oDWBYRLwC4NDKvK7irpIGookYd6jrcJXId/M6bVzrZfwAcBHf7yGVv8KmTDhsV5W4EAMbYbsbY7YyxY8EfhrcJXzxj7F3G2GmRbRmAJ0OoiyFkjNAbYqUSuH95BxFVBzAipHIrgAvGFgAgoivBLXpfGGPrAcwH8BARlSGi0wCc67PZJADdAAxFxJqPUAnAfgBbwV0g/9as//fg7qNhRJRNRH3A/d1yuV7nbRO4/z0KxlgRgMkAHiOiSpGH4m0AwugnMBHA/URUi4hqAnhQlEtE5xDR8ZGH104ARQCKiehEIuoSabQtiBxXcQh1MYSMEXpDrDwHoBy4lTkPwKdhFMoYWwbgaXDB3ASgKYDvAhRxKXiD5DZwEX3TZ39/RfZ1KoD3pEVvgrsvNgBYBn6MOvU/AKAPuL98G4BLAHwgreJ33p4H0DcSNTNasYubAOwFsBbAt+APp/E6dfPhUfCH5GIAv4I3PotOXicA+ALAHvBzNZYx9jWAHABPRI7lb/BG63tDqIshZMjuSjQYDAZDacNY9AaDwVDKMUJvMBgMpRwj9AaDwVDKMUJvMBgMpZyUTGpWs2ZN1qBBg2RXw2AwGNKGBQsW/MMYq6ValpJC36BBA8yfPz/Z1TAYDIa0gYhce4Eb143BYDCUcozQGwwGQynHCL3BYDCUclLSR6/i4MGDyMvLQ0FBgf/KhqRStmxZ1KtXD9nZ2cmuisFgQBoJfV5eHipVqoQGDRpAbxwFQzJgjGHr1q3Iy8tDw4YNk10dg8GANHLdFBQUoEaNGkbkUxwiQo0aNcybl8GQQqSN0AMwIp8mmP/JYEgt0kroDQaDIW3YtAmYOjXZtQBghF6LrVu3okWLFmjRogXq1KmDunXrHvp94MABz23nz5+PYcOG+e7j1FNPDaWus2bNwjnnnBNKWQaDIQ7OOgu44AJg375k1yR9GmOTSY0aNbBo0SIAwMiRI1GxYkXccccdh5YXFhYiK0t9KnNzc5Gbm+u7j7lz54ZTWYPBkBqsjoxKWZz8QbeMRR8jgwYNwpAhQ9CuXTvcdddd+PHHH/Gvf/0LLVu2xKmnnoqVK1cCsFvYI0eOxFVXXYVOnTrh2GOPxejR1gBCFStWPLR+p06d0LdvX5x00kkYMGAAxOAw06dPx0knnYTWrVtj2LBhvpb7tm3bcP7556NZs2Zo3749Fi9eDAD45ptvDr2RtGzZErt378Zff/2F008/HS1atMApp5yCOXPmhH7ODAZDckhLi/6WW4CIgR0aLVoAzz0XbJu8vDzMnTsXmZmZ2LVrF+bMmYOsrCx88cUXGD58ON5///2obVasWIGvv/4au3fvxoknnoihQ4dGxZv//PPPWLp0KY466ih06NAB3333HXJzc3Hddddh9uzZaNiwIfr37+9bvxEjRqBly5aYOnUqvvrqKwwcOBCLFi3CqFGj8OKLL6JDhw7Ys2cPypYti3HjxqF79+647777UFRUhPz8/GAnw2AwpCxpKfSpwkUXXYTMzEwAwM6dO3HFFVdg1apVICIcPHhQuc3ZZ5+NnJwc5OTkoHbt2ti0aRPq1atnW6dt27aH5rVo0QLr1q1DxYoVceyxxx6KTe/fvz/GjRvnWb9vv/320MOmS5cu2Lp1K3bt2oUOHTrgtttuw4ABA9CnTx/Uq1cPbdq0wVVXXYWDBw/i/PPPR4sWLeI6NwZD2rF9O1CtWvjlpkAUWloKfVDLO1FUqFDh0PQDDzyAzp0748MPP8S6devQqVMn5TY5OTmHpjMzM1FYWBjTOvFwzz334Oyzz8b06dPRoUMHzJw5E6effjpmz56NTz75BIMGDcJtt92GgQMHhrpfgyFlWbAAyM0F3n0X0HhbTjeMjz4kdu7cibp16wIAJkyYEHr5J554ItauXYt169YBAN577z3fbTp27Ih33nkHAPf916xZE5UrV8aaNWvQtGlT3H333WjTpg1WrFiB9evX44gjjsC1116La665BgsXLgz9GAyGlOXnn/n3F18ktx4JwlfoiehoIvqaiJYR0VIiulmxDhHRaCJaTUSLiaiVtOwKIloV+VwR9gGkCnfddRfuvfdetGzZMnQLHADKlSuHsWPHokePHmjdujUqVaqEKlWqeG4zcuRILFiwAM2aNcM999yD//znPwCA5557DqeccgqaNWuG7Oxs9OzZE7NmzULz5s3RsmVLvPfee7j55qi/2WAovUQCHlLBzZIQGGOeHwBHAmgVma4E4DcAjR3r9AIwAwABaA/gh8j86gDWRr6rRaar+e2zdevWzMmyZcui5h1u7N69mzHGWHFxMRs6dCh75plnklwjd8z/ZUgrXnmFMYCxa64Jr8yyZXmZe/eGV6YHAOYzF031tegZY38xxhZGpncDWA6grmO13gDejOxvHoCqRHQkgO4APmeMbWOMbQfwOYAesT6UDndeffVVtGjRAk2aNMHOnTtx3XXXJbtKBkPpIhEWvXhbSCKBGmOJqAGAlgB+cCyqC+BP6XdeZJ7bfFXZgwEMBoD69esHqdZhw6233opbb7012dUwGEofKSDGiUS7MZaIKgJ4H8AtjLFdYVeEMTaOMZbLGMutVUs5vq3BYDAkhkT66FPgIaIl9ESUDS7y7zDGPlCssgHA0dLvepF5bvMNBoMhOLt3JyalQCKEPgUEXqATdUMAXgewnDH2jMtq0wAMjETftAewkzH2F4CZALoRUTUiqgagW2SewWAwBCM/H6hcGbjrrsTt4zC26DsAuBxAFyJaFPn0IqIhRDQkss508Iia1QBeBXA9ADDGtgF4BMBPkc/DkXkGg8EQDJEFcvz45NYjDdGJuvmWMUaMsWaMsRaRz3TG2MuMsZcj6zDG2A2MseMYY00ZY/Ol7cczxo6PfN5I5MEkks6dO2PmTPvLyHPPPYehQ4e6btOpUyfMn89PRa9evbBjx46odUaOHIlRo0Z57nvq1KlYtmzZod8PPvggvgihY4dJaWxIK4S17ZMaPCYSaXWniUVvAM8tM2nSJNu8SZMmaSUXA3jmyapVq8a0b6fQP/zww+jatWtMZRkMaYvwzSdS6A9XH72B07dvX3zyySeHBhpZt24dNm7ciI4dO2Lo0KHIzc1FkyZNMGLECOX2DRo0wD///AMAeOyxx9CoUSOcdtpph9IZAzxOvk2bNmjevDkuvPBC5OfnY+7cuZg2bRruvPNOtGjRAmvWrMGgQYMwZcoUAMCXX36Jli1bomnTprjqqquwf//+Q/sbMWIEWrVqhaZNm2LFihWex2dSGhtSnqIi/u2SMDAunEJ/8CDw738DYYx9nAKCn5ZJzZKRp7h69epo27YtZsyYgd69e2PSpEm4+OKLQUR47LHHUL16dRQVFeHMM8/E4sWL0axZM2U5CxYswKRJk7Bo0SIUFhaiVatWaN26NQCgT58+uPbaawEA999/P15//XXcdNNNOO+883DOOeegb9++trIKCgowaNAgfPnll2jUqBEGDhyIl156CbfccgsAoGbNmli4cCHGjh2LUaNG4bXXXnM9PpPS2JDyCKFPJELoX34ZuO8+vs8HHkj8fhOMsegDILtvZLfN5MmT0apVK7Rs2RJLly61uVmczJkzBxdccAHKly+PypUr47zzzju0bMmSJejYsSOaNm2Kd955B0uXLvWsz8qVK9GwYUM0atQIAHDFFVdg9uzZh5b36dMHANC6detDydDc+Pbbb3H55ZcDUKc0Hj16NHbs2IGsrCy0adMGb7zxBkaOHIlff/0VlSpV8izbYAiFRAq90+res4d/790bftkyd91VIonU0tOiT1Ke4t69e+PWW2/FwoULkZ+fj9atW+P333/HqFGj8NNPP6FatWoYNGgQCmJ83Rs0aBCmTp2K5s2bY8KECZg1a1Zc9RXpjuNJdWxSGhtShpIQeqePPtFJzp56in8S7N4xFn0AKlasiM6dO+Oqq646ZM3v2rULFSpUQJUqVbBp0ybMmDHDs4zTTz8dU6dOxb59+7B79258/PHHh5bt3r0bRx55JA4ePHgovTAAVKpUCbt3744q68QTT8S6deuwOjI25VtvvYUzzjgjpmMzKY0NKU9JCn2Ywmt89OlH//79ccEFFxxy4YjUvieddBKOPvpodOjQwXP7Vq1a4ZJLLkHz5s1Ru3ZttGnT5tCyRx55BO3atUOtWrXQrl27Q+Ler18/XHvttRg9evShRlgAKFu2LN544w1cdNFFKCwsRJs2bTBkyJCofeogxrNt1qwZypcvb0tp/PXXXyMjIwNNmjRBz549MWnSJDz11FPIzs5GxYoV8eabb8a0T4MhECXpow8jCicFBP4Qbmktk/kxaYrTH/N/GUJn2TKe9vdQ150QefZZXu6wYfz3o4/y38OHx15mdjYvY+tW93VCPB7Ek6bYYDAYUoJ0dd3o8NNPCS3eCL3BYEgPSlLoRUilrutm/36+7ssvu5ftNb9HYofpSCuhZ6nk8zK4Yv4nQ0JIRNZKJ7FG3WyLpPB66KHoZW73g3w8WYltLk0boS9btiy2bt1qRCTFYYxh69atKFu2bLKrYihtlGQcfUmUIx9PgoU+baJu6tWrh7y8PGzZsiXZVTH4ULZsWdSrVy/Z1TCUNlI5jt5rPTfxN0IfTXZ2Nho2bJjsahgMhmSRykKvciv5WffGdWMwGAwOSiKOPlZE3VTingIWvRF6g8GQHpRkhym3327EUjdj0RsMBoODZLhuMjQl0qtuOhZ9drbefmLE9zFCROMBnANgM2PsFMXyOwEMkMo7GUAtxtg2IloHYDeAIgCFjLHcsCpuMBgOM1LZR69KGphmPvoJAFyj+RljT7HIEIMA7gXwDbOPC9s5styIvMFQ2ikoSMwIUEBqh1emu4+eMTYbgO6A3v0BTIyrRgbD4cQvvwD//W+yaxEe5coBkfERQidZPvriYmDYMEDKKBtFvD76BLtuQvPRE1F5cMv/fWk2A/AZES0gosFh7ctgiOKHH4AJE5Jdi+C0aAFcfHGyaxEu69cnptxE9oz1ct1MnQqMGQNcdpn79l7jPaSDRR+AcwF853DbnMYYawWgJ4AbiOh0t42JaDARzSei+aZTlCEw7dsDV16Z7FqUbt55B3jwQfUyIuDmmxO7/2T46AFAMRZEFKJusnb5uYPSVOj7weG2YYxtiHxvBvAhgLZuGzPGxjHGchljubVq1QqxWgZDhK+/toaIKywE/vwzufVJNy67DHjkEfflo0cndv/J8NET6TXIynX7+2+9suU3lMxM/33EQShCT0RVAJwB4CNpXgUiqiSmAXQDsCSM/RkMgcnLA7p0Aa64gv++806gfn1g8+bk1sugTyrH0cuuG916yuslWOh1wisnAugEoCYR5QEYASAbABhjIifnBQA+Y4zJI+keAeBD4icqC8C7jLFPw6u6wRAA8fotBm7/5BP+vX07ULt2cupTvnzJ7zedSZbrRge5bk4LXseiT/DYtL5Czxjrr7HOBPAwTHneWgDNY62YwRAq4qYSN5S4MRPsG1WSnw9UrgzcckvJ7zsRlET6YCC14+jluh08aC9TZ5sEC73pGWs4PHDeyOJVO8GvzEpEO4FXuF6qsXat+zKviJMwSZaPXofVq61pIfR+ZZegRW+E3nB4IG420aVdiEYyhN7txl+1Cnj/ffWyZPPee+7LSlroEymKqrJ19jdYih53Cr0bJWjRp02aYoMhLpyuG/E7MxPYtYu7Ukoa580tOholY3CdLVv4uaheXb3c6/yUBqGXz7k8Hcu+jEVvMCQJp+tGiMZHHwFVqgDz5iWvLiVJUREwcWK0X712baBGDfftKlVyX6ZrwcZLSQg9kT2E1C+8kjFgyhT7vBS06I3QGw4PhLA5XTdffMG/f/yRj/t53nn2Ti+JrEsyhH7MGODSS4H//Md7vSVLgHXrrN9eQq+y6BPR/lAS542InyNdZs0CLrrIPm/YMPtvnZ6xCcYIveHwwK0xVrBvHzB2LPDxx8Bzz5VsXUqSDRv4t9/DrGlTQB7RzVnXggLrHKqE3itdQKwIYdRNHRwEWYyDWNqqY3e+HTqFvqgIGDQI+Pln/f3EiRF6w+GBm0UvbsJ77vEW4L17gf37w61LMoj1IeO0PsuVA3pEktq6+ei/+SbYPnTrkGjXjVPovfbn1ZjvZsn/+Sd/oxo61L6fBGKE3nB4IMTI2Rgr44zMkalYEWjWLJy6JNN143zg6aJyM3z5Jf92E/pOnYLtQ7cOibToiezHo/qPVqzgPa0B/majW7ZAlanSCL3BEAJOoVflD/cT4N9+C6cuJREm6IYsaPv360f4ePmTdRsfv/8+vreiRAq9QEfoTz4ZOPpoPr1vX/B9qB6MRugNhhBwioS42VRhdYkUErku8d7cV17Js3YGQTzM/vgDKFsWeP11vfBIL6HX2X7VKuDUU4HbbtOrp1cdEh1e6Sf0MjpC73yYJkHoTRy9oXTBmPqmETeX00cvr1tSLpWwfPSx5N8XoiNy/nz4IbB8uX2d77+P3i4WoZf91yJ5nNwAGRQ3i37NGh7nH0/WW/lNxyu+fcYM++9YLHrVG5AReoMhAMXF6gYyIUbff28XsnS26GNBiJgY7i8nB/hUyjW4cye3vJ3EIvRyHiGxTjy5hdzO2/HHAxUqWKklYsGrkVqe16uXfVmaWPTGdWMoXbhZy7JQPfOMNR3ERx8WyWyMFccrhL5MGR5RJMjPV2/n5Z5x89GrhD5oyolXXuHnacsW73FZ5WOIB1VSs5kz3dd3Cn3XrsAxx3jvwwi9wRAnbkIv31x+VmXYN90llwDjx1u/k2XRP/ww7ysAWEJftqxd3N0s91gsejm6ROwjqEX/+uv8e+1ab6GPF7cyH3sMePNN922cD8YGDaIHRzcWvcEQMttcxrGXby63gZgT1ZFp8mTg6qut38my6B991JoWlmhOjn2oPDdBd/qmZZzCJpCtd+FWCSr0ooziYuu8ebVxFBe710cH53/iNTDNxRdHDzOYk+MfWaQS+gTnNzJCbyhddOumnu82PqeO6ybsDk4l2PXdhnysQnhzcuyx4G7W+ccfu5frFksun+c5c/i320PWSeXKfEQw0cu0uFjPoh8yhB9TUESZW7fqbzNlilroY7HojdAbDAFY4jJapZvrRqcx9t57vff5++/BHgZhx4PLudB1EULvFN5YMlH6CT1jwAcf8OmqVfXK3L2bj/ErKCqyzpvXuX71Vb3ynYj/PmiDrtPiL1Mm2qI/91ze4U6gatNIcG9p3yuNiMYT0WYiUt5BRNSJiHYS0aLI50FpWQ8iWklEq4nonjArbkgDCgqS291fRsdH72bR/9//uZe7YgVw7LFA69b6dQnbdXP22d7LDxwA3n1XbdE76xCm0Au3y4EDwKZNfDqWcETA3aJ3u76CWsixCv20adb0++/zh7dz3ytW2BuLU9SinwCgh886cxhjLSKfhwGAiDIBvAigJ4DGAPoTUeN4KmtIIw4c4PlQ7rwz2TXh6PjoVekBRFd3N/78k38vWqRfl7BdN34RJw8+CAwYYN+vcC9MmmRfNxb/tptPWjxQ5f36pQxYuxaoUyd6vptF7xbxE6twOl0xQcjK4teOn3GjEvpkW/SMsdkAXFq4PGkLYDVjbC1j7ACASQB6x1COIZUpKgJuuAFYv94+X1husb5Kh40sYG6uG2eaBID3IPUilrjwsC16P5FYs8Z9mchmKWjVyrsslbvJTbyzs/n5k4XNr65z5ljWv4ybRe/2YHIT+goVgOuuc18/nlj8zExvoV+7lqdvTlGLXod/EdEvRDSDiJpE5tUF8Ke0Tl5knqE0MXcuD9kbONA+X4hYMkZLEkydyiNeALvV6SbOwjqUBfivv7z3EctQhGFY9HIZfuXpJN7SReWacCt/5UoeUz5qlDXPT+i9wmODCL1cTnExb9j99FMeDjluXPT6oszZs73r50VWlvd1n5vL0zePHu1d3wQQRs/YhQCOYYztIaJeAKYCOCFoIUQ0GMBgAKhfv34I1TKUCOKCdl7Y4oLfs4dfxInubariggv498UXu1v0Miqh37nTex/xCL3bOXFL4yAjuyxKUuiBaFHyK1/ueRuLWwPgx6ty3egI/Z49vGF3/nxr3nff8XQMAwfah0mMZ1jEgwet/1Ql9Nu3828x2I1Mqlv0jLFdjLE9kenpALKJqCaADQCOllatF5nnVs44xlguYyy3Vjw5Kwwli1vsuXzhrlgR/n4//dQ9wkaFbNHLPno/142fz1oWa92bVeW6cRuz1A1Z6FXitHevJSw6Qq8b9siYvtCXLcu/5QbYWIX+wAG1Ra/jo1+7Nrrs004DbroJuOKK6PVj5Z9/rOshqIWebB+9H0RUh4hfsUTUNlLmVgA/ATiBiBoSURkA/QBMcy/JkJa4Cb184SbCWunZk4+CpItOelwhGrJ4+wm9/Hage7OqLPAg52j9evvxqPbbqJE10LeO0AtR9kMl9M5zK5KtiRGqwhZ6UQ8xX4XYz7RpQMuW0fUQzJplLy8ecnK8xzvwItkWPRFNBPA9gBOJKI+IriaiIUQ0JLJKXwBLiOgXAKMB9GOcQgA3ApgJYDmAyYyxpYk5DEPS0LHoUyHEUhYjlWAAlmgEsehV2S9l5PJ//92+npsV77zphwwBbr6ZT8+axbvZDx9uLVc9ODZu5N8HDug95OIRemekSteuwFFHWW8JYQj9zp1q48FP6H/5xXt/ou7xCG2TJsB77/FUF16uGy+S7aNnjPX3Wf4CgBdclk0HMD22qhnSgmRZ9EGRxU4WE5UbQLbo/UTSOdao0wUiL+/QgQuwytfsJfSvvMK/n38e6NyZT0+XbisvH31ODlCzpvcxAPEJ/Y4d9t/Z2fx6EOcxDKG//nre1iKXk5HhHpevO5KWqj4tWwZLp5yZadUtVtdNsi16g8ETtwvUGfWQSGrU8F9HtvxkYZQbClWNsX4WvZ+IyfNEL0oxz+3NQueml+vlN8LTP//4l1emjPsyZ18C54PF2WBdtqw9r7vsOvK7FtweWtWrqx/QbkKvm7dIrOfWhqOD/H+VVh+94TBHx3WTaIveLZGZIC/PLnZu4i3m6/ro9+8HzjjD+j1hAh9ucOhQnlYXsN/AzkFPdC16r7o6y4kVLz++eIsQOMV4xw6rPQDgHeXkQbbDsOhbt7Y/0EQ5bmmVg4yNe+aZwEsvWb+DCr1MivrozcAjhvjQcd0k20d/9NH23249SVXWpJfrxtm55oYbrOn8fOA//4lN6HXwC/sMildqAmcuHZXQV6tmPXCzs/mxqs5nEKGvVMnuQ5eF3mnRO99Iggj9V1/Zf8czOEqK+uiNRW+Ij0T76B9/HLj88ti3V+H2BuAWpy0ScTldRF4uE3HMqmHpRCcs3cZYv/mAu2WrS58+6vktWkTPkxuCAf4wlJN2AdGDbAuCCH2nTvbtVEIvjtuZsTKe3sdBLfp69axp46M3lEp0XDfx9AQdPhx4++3Yt1ch4suduPW8LFeOR1Q4GzW93DqqV3ghAj/8wL/lkYh0hF40yqpYtcp9mQ7ly6vnq6JWnGktirMyLq4AACAASURBVIujxVF23TjX9UIWemcbhsp1I+fVl4lnWMigQv/WW9a0+N/lzJt+tG2b/Kgbg8ETHYs+kfnXn346+Da6Fn2XLvyGPeYYdQ4TL4veS+iFmAVtjJUbjp20aBGfVRhPz+Xi4mh3RywWfdu2wE8/Wb/l81NcHN0Yu2MH8PLL/LczaiiI68ZJUNeN/KYn9nf//cH2Zyx6Q0ojC/3nn/NQsx077Dd0PN3K/bjjDv11Tz6Zf7sJvWiQFHUXVlmZMmqh97LoVa/wznny9jo3eiLFIB6hV4WVBvXRL1hgiXxGBk+u9sQT1vJvvuF5leRyhg61tnFz3YQl9CNG6G0r9leunPsy1f6Mj96QFhDx8TWLi3nKXlmU5MiUZDJkCO+t6Sb0u3bxb6egisZF58147rnu+/Ky6MU8lc9ZtX+/+QK/6CMv4rXoVa4b5wPeK7Njbq41XbUqF35V+4CAMXsklVNY43HdqIRe9yHrJfRu7QWZmcaiN6Q4skUv5x9PdqSNijJl+E3lFnUjhN5Zd7c8417pf4MK/cyZ1rRO3wQVbdt6L/ciEULvtOizs6OPYckSe1sFYBfaiy5S77OoyL1dQZQr6hGUMMIrgwi9Tg77ODFCb4iPWIR+2zZg3bqEVy2KnBxv/6ts0cv1z8riD4ggN6NK6J2ROLLQ9+0bvZ4Tv85bXg8eFbJQhi30KteNykXx1FPROf/l/2jyZJ5SwUl+vr3++fn2kNMePXgDtdfoYG7EI/TOEFodiIxFb0hxZKEXKXsLC70v3OOPtxJelSTCondDCGlxcXT++qBWl7jhZfERZaqEXsbt3PmNdhWEGjXsWUVjFfrt23mPX53GWCFoRFbbiur/EHl6vOq2Z4/dal67Nno82s6drRHAguAUepGjSAdRV2dsPuAu/kTA998D//63/n4CYoTeEB8qofez6N3CG8NAtoyd+Fn0AsbsPUXFyEEbNkSPpOWGELVGjax5QujFDR902D6/QcB103tffDHvuVuhgjUvlrz6AHDOOfzb+dBSuW7ktAgiWkrnAaNyeezZ45/Dx5mDRxfnNdKggf62Xq4i+QH+wAPR29x3n/5+AmKE3hCcYcOAe+/l0/LFqyv0XuzbF/sN6oefRS8oLrb3FBUWPcAzFepAFP1QKCqyu4V0LPrHH7emVeu/IOUTPO88q/5eZGTYk46JebHw44/823leVULPWPQ8nf/DzaLPzub/zaWXqreLNaw3jMZYP+Te2mENKemBEXpDcMaMsULf3Cz6WH2O7dvz7vRu/Pe/3jfGlCnqMUEBfYu+uFht0QP+A3HLqNInPP+8JcRu6RXkUbucvVCd9OrF/di1alnC5pd/XhxLGEIv3DNOwXYLr3S6c3SE3s2iP3DAiojyqpsXcu9bQRg+ej+IeMrpBx80Qm9IA8TNHMR1I5g4MXre4sXe28jJp9xQjQkK6Fv0TtdNVlZw6zAjQy0048ZZ58ZtIGoh9Kp9NmwIPPec9Ts7m0emVKgQ3VtU5rXXgCuv5NNnnWXVUa5vPDi3d/PRO99KVA9eOX7erW579vCysrPd/1MdoVelZz79dHuSNiB8iz4jg4cdP/SQEXpDGiDnFAkq9Fdf7b5s8GBu7cj7ueOOYF3LncRj0esM3iHj1TNU162lctWUKWMXBmF9ZmTwDmtbt6qF/uqrgfHjecPpoEHWNgKdB+CNN7ovc4qVm3g52yVU+1WFagpEL1Qh9KIzW6yohL5WLX4eY0FXtOX1jNAbUh7ZopfDK3UsoH373F0hr74KPPKI9buwMLZ0BzJBLPqHHrJ+Z2Wp3SFHHulehsp6Bfi58Xs7EOdOtb3zQSVEsbCQJ0s7+2zvTJRyg60skCL7o1tj9skn2xuWnQixEj1U3cTLeUwqkVaFagJAx45WT1gd140OKqEPI3tlkPVSQeiJaDwRbSYi5UjMRDSAiBYT0a9ENJeImkvL1kXmLyKi+artDWmOyqIvLIy2WouLeTifHG0A2FP7ehHUdaKKaAli0U+Thjd2E3qvwTqI1HXQedsRQq96I8jKUlv04o1jyRL9TJay2PgJtHN9J+K/P/VUe32cyEK/ciXwzDPR67ilHO7cGahbl0/Lrpt4hN6ZOgEomcbYEhZ6nUfXBPChAt90Wf47gDMYY9uJqCeAcQDaScs7M8Y0hrgxlAirVvGUsl7WqC6MWX5fv8bY4mLuG3bGgv/2m96+gubLad8+ep6uRa+KDJHF86yz+MDkXuGRY8bwjxMd142fRa8SevEg2rvX26KXkc+FSvCceAlSTg4wb56VT0gMvOJEPqbpLqOMOi16kdqhdm3+H5Ypw48zqOsmJyf6AaSy6GMNNQXS13XDGJsNwDWJBmNsLmNMBEbPA1DPbV1DCtCoER+4OQzy860bx89HX1SktvJ0b1K/4fKcqMb8zMnRs8zkdAQAF1c5EuiLL4Bnnw1eJyCYRR/EdSO/cczXfHkOatH7xYi3awdUrsx/q952unWzH5NbBzA3oRf++YoVY3PdLFoEHHGE974Af4v+/ffd95GiFn3YPvqrAcyQfjMAnxHRAiIaHPK+DMlGvvh1hF6FrvXkFzKoQ5kyeuK8cqX9d2am1ZVeCBkQvMMTEKyPgaquYuBtuW6A/SGq25NT9Wagu74fTrF85x1ukcvXjMpto6qLeJsTDyPGuBswqOvGGUkDAFWq+NfdiddA6rEIfbwRTzq7C6sgIuoMLvR3S7NPY4y1AtATwA1EdLrH9oOJaD4Rzd/i9tpnCI8PP+TfU6YA11wTWxnO0ZO8XDdFRWqh0BV6XXeEF2XK6Imz86GSlcVHEerc2S4MsVr0YTbGqli4kAvg33/r10tHxHV7fap+y30R/HAKvShLXCvbt/O3qo8+4r2FYxX6Zs3UidHiaYxNV9eNDkTUDMBrAHozxg7FJTHGNkS+NwP4EIBrej3G2DjGWC5jLLeWbldugz7vvWe/oESek4suAl5/PbYy5Xh1P4vezYoV26xezfOVuBHvUHkAtwh1hN7pYhJ1LFPG/sAJw6JXZTn0a4z1Y+NGnvfF6aaIhVgTnyVC6MX2F15oLcvP1zcWnAN8FBZa5/P66+3reSHfR85e3LqinW6uGyKqD+ADAJczxn6T5lcgokpiGkA3AMrIHUMJMGGC/bdX71MnX33FL+jbbrP7J++805qWG1Wd2R8Bb9fNgQPACSfwXp5uhG3Rt2qlv5248bOz7Q+cWLrYO4VedgUJgjTGqtixI3iDolvbRaVK1vJUseibNrUv94p+8qKw0NqXKpWHDk7Xj250jptF7zWKWBz4mgdENBFAJwA1iSgPwAgA2QDAGHsZwIMAagAYS7zChYyxXABHAPgwMi8LwLuMscQchcEf58WrK/Q7dwJnnsm7is+axRshVSxaZKXJDeq6ERkenb5xmS++0KuvF7LQd+zIXRw6LF3Kv7Oz43/gOKNuqlQBNm1Sr6t6i9Gx6Hfs0E9w5oec+CxRQt+zJzBDatpzCrfTondeyzoRQ8IdLNfrrLOssuT5fo2xXmKu+/B3s+ivvNIaPD5EfK8axlh/n+XXAIhy8jLG1gJoHr2FISk4RaNiRfvvoiIuYsuW8Wx9tWvz+XJ8th+7d1tlOS36vDy1oGVk6DW0hpHZT45t9xq0wsnmzfw7Ozv+vOEqoXci9tGjR/QyuTG2Qwf1Pg4csITkrbf0Qmndxv4VriWi+IReTgzn5PXX7ZFgbg3DQpRjEXpnpss33wT69VMPuK7zMP3+e/WA7PEKvVdDbxyYnrGHC87UAc4b8cAB/prerh3QuHH0ekEEThUrLuLtnbiFXSYK0RPXmbvcSeXKwPLlfFoMSBJPsitBZqZdDGSLWSDOtdivjCxCTheGjDjOyy7jb2R+iLJ69uRvZ61b89/igZhI101ODlCnjvXb+RAOw6J3lnXccfz/FEaGXIafRU/E+2lcfnn0eroRVW6uG1WbTQgYoT9ccV6Qcvpdkedj9WrrBgzislC5bv5x6TM3Y4a3YIWNEE9VqJ1Mdrbl3hJvKiqhHz0auP9+77J+/916Q3IOYKLyL6sequIcySKkWk8M6BJ0/NimTXk0yxVXAM2bW20YsvDEEwboJfTOsFen28npow9D6EUZwm0oP/jj6TClK/TGojeEwnvvcTcMoBYE5zxV7LWcdCyo0Is86cKS93LPhBEjr4twYfkJfZkyltCL+qmEqkIFe/I1FVlZ1kPCOSShrtCLzkKAt2X92mvedfFCFjuxj1ijQ5zH4NWD1Sn0TjeL06J3WtxBhF78/8J1KYRedqHFE14Zr+vGWPSGQPTrZ1npqovPzxUzaxYwe7b++jJFRcAHH/BpcdOEETUTK127RqcE9nPdZGdzAWrShGd9BNwzLfq5dLKyLEHXsegFxx1nTQuh93sohiUUqpz18bhuvIQ+O9seSup0Z/lZ9EGsYHEdCmEXLi4/ode9/uN13RiL3qDFtm32CIY//nBPl+vFiy/GXge5bCGCYcTBA+pwRD8yMy3xOPdc/u1ntQkBXrLEeitRCZVOaJ/8MJCH03PbXohKmzbWPCH08gNTJT5BhZ4IGDJEPR/grqBy5fh4pkGE3nl95eSoz58IFxUNxqr8QGH66AXiQf/oo9xd1V+KOUmkRX/88fzbWPSGuOjTxx6PfswxaqH3s1CmTIm9DvLF7nbTfP010KVLsHJr1oweOFoH+VinTOFtEH5uCJUAe6XU9SpPdt0cOGAXQZVIqQYeEUKfn++9r6BCUVysHsxF7KNiRb7PCy7w9tH7XU9uQi/O81dfAW+/rc55H6+PXnVOxIP/yCN5HxO/togwLPp77rHqbnz0hrhQxaK7CX2iBunWEfry5YM37uXk8BvUzYrWsa7LlOH+eTexFHVSuWPcLFLAW3zlOu/fbz8/cp3F24pK6IUFKrtuVOITllCUlI9eHH/9+sCAAd5lxeqj76+IEE9Ub1Qviz4zU329GIs+RVi+HLj22tgHGy5JVBewm+tGpEEIG3l/XjHRQaMbhIi5PSCcue4FKkH0E3rVQ0NVX+FK8XpoZWUBV13Fp+vWdXfdnH8+/161il9z8vXWqhV/7X/iCavuquMKIwQUCC70fj56P4veCz+L3sudV6+evZPfihXA//7nv0+AP3xGjNBbV+Bl0Wdm2ttqBMaiTxEuvphHM4gollRGdTO5WfSqOO0wkHPAyMIjdwDKyIjNohfbei0/4QTgtNOs+UEakoVoqOLbVfsVoZd+x3LDDXyM0Fq17GIgi5Yoo2tX3pdBFvrGjfkDoHNn7/3EExooE3QA8Vh99EGE3s1H3769e+P6vffaHwQnnshH4dJh/Xpg5Ei9dQW6Qm8aY1OQeHtDhk1BAY+qWb8+elkqCP3HH1vT8mv2GWdY084wQx10hb5sWR7fLtC16KtWtcpw9hx226+4Mf1cAUT8JnfmAfLKYlhUxAfy+Oor/TEE4mlIlBF1kesUpo++XmTYCp03ED+LPjPTMsKcy8J6w9HF660/I0PdpqNKOx0yRujTkRkzeJz8zTdHLwviuhExxP36hVs/+QEkC49zvNKgo0bpCj3gH52jOk9yN30doa9bF7j0Uu86OfdZWOjeecxZxowZ/DicVnyfPvwBICeVE4QlFCrXTZCyvXz0N95o9dEIIvRuFj1gXWflygFDh0bPjxddY88r5bcs9G4P+wQZlUbo0xFxwYuLYvNm4Msv7ctk/Cz6BDUAAbDfyE6hD5rP3U/o5ddieV+6Fr2cGVJH6K+7zpqn07iXkWENbi2QLUCRKVJGdY5q1uQWrGqw7rAt+rCE3mnRi3rqDtYur6s6RrfyEmQhu1KxIr8X5TdKQUaGdY3K7k0j9AYl4sIQVkGXLtyv65aPZOvW6HmMWZ2IYolD1sXNos/MDG7R+zXGiuWMcUtY9M7VFXq5m75KdJ2iIR+bW53kh6hqn0Loq1YFWraMXh60o1lJW/SiLaR7d/t85zl3tsmIc6fzJqRj0Yt1nMNFhvXgC0KXLsBNN0XPN0JvCIQz6kKk0d20CdiwIXr9gQOj5xUX81C/WHzlQZAbyeS0A/G4btzEzPnAEgNV6948QS16HaGXQ1hV64hzf8UV6u11hf6PP4AFC0reon/kEe6KcrosxDk/+mhrnvwmKgRP59rz89EDPF3F8OG8PUNOoVDSrhsviKw3XPlNzQi9QYlbeN2RR6pHPVq3LnoeY1zoc3JiGxJPF5FkC7B3M8/I0AtXlSMkhMWuEmEgWui9whBV1nWPHtb5U7mzvITezXUj10m1jhA651iwAt0exUcfzUMwS9qiz8rinbmcdRfn/O231bmCROOyzrChOhY9EfDYYzxlhZxkLqzzoXrbCops0RuhN9goKOADK8sXgNNH74dbrpsDB/QHzI6FMWPsN5os9LquG1kchPi6DZoSj9CvXcvrK86FV49VgXxsQcdcnTgRuP123nMZcB85KqjrJqyBpp3iCqhF001IxfZZWda5lI9PHLdbw7RXXfyum5wc/VQXulx6KTBpEs/sKYfuevHOO/bfcmOscd2kCIsW8eiAZIdW3n03zykuGlu/+UY9Wo4Xqtdj4bpJpEXvTE0rxwjrNsbKN4HIUy6EvmFDe6oGISjivIgQvk6dvMsVZWVnW3VSxXc7z6OO60ZGXufkk4FRo6yHcFhCHxaqgUi8egY78UqKxpgl9EHqIh4qOtlO5fMaFpdcwnVB1X6jwjngi46PXs5SGiJJaKlIE846i1sbQXvGhU1eHv/euZNf8LJo6Qq9ar2CAv5JpEVfVORu9epa9HLdxYNCCP0jj9gHinZa4ccfz4c3DCIqok46Fn1QoVfFS4v9uQlS0HaMsPGz6N06PKmiYOTjlwca8cNp0es8/PzOa0ngfHBnZPDkcAUFVliuTJ06/mMbxIiWRU9E44loMxEpx5MjzmgiWk1Ei4molbTsCiJaFfm4tDilCSUwWruNRx+10v0C0f73eBpRb7iBD6eWSIveKfQyuo2x8jGKkaicA4IIVHlEjj3W3aerIlaLXlWeM6+6SjT9LPpkoeu6cYvYUnX1lxHzdUJ7Y7HoxbVV0uGVXmRk8AFo3n7b3vNaHN/11ycsAk7XdTMBgGIAy0P0BHBC5DMYwEsAQETVwQcTbwegLYARRKQ5KrUhKneL8wJnDPjll/j2kZMDDBsWXxluFBZ63+g6jbHy9mLMWSH0IqLl55+BJ58M1nYhi+r//Z817SX0Xha9SrCcg4/7WfSpJPSCWC164Yv2etNZudIaUN4L50NHx70hrq1kCr3KolfhNl5viGgJPWNsNgCvscl6A3iTceYBqEpERwLoDuBzxtg2xth2AJ/D+4Fh8EIl9PJrbCzdvcuUsWLww6a42P1G03Xd1K7Nw+YAqyewU+hbtADuuitYQ6S4qRo0sPcwleOxnXgJvRgqUHD11fbQQkCdNyZVhV7VESwsoRfnsVGjYAOXi/336aNOrSwzaBD/Pukk//JLCr/rM6yGdFXRIZVTF8Cf0u+8yDy3+VEQ0WAimk9E87fohFwdjjiFvrjYfiPG4s+VBa15c/uyp59Wpz3WJSzXzWOP8Zu9Y0c+T8TjO9MsBxFKVfigTFDXjSz0Igme2z6B1LfoVRFLQVw3qg5RsR6f06InsjJ9ujFoEN9O50GSKJzH63b8KjdZyKRM1A1jbBxjLJcxllvLOThwMpEv9GRH4KgsevniiKV+sqA5t+/SRd3NXpfCwviFXnVM55/P47BVvQ/dtnHD7ebTEXr52OROaX6RKPK2qdBoqEJX6P3GBgjj4aUSwgSKYsLwq3OyXTcabAAgv6fWi8xzm58+yH7kVBT6eC8O2SJzc03EKvbFxe4Xdyxx9ILatXkP4GbN7PNjsejjEXr53F1yCf8A7i40lUUvRtlq0ya8G/3KK4Fx4+IrQ9XeEcSiV/UAFQS9j3RDPVONoD76NLDopwEYGIm+aQ9gJ2PsLwAzAXQjomqRRthukXmpjzOfDBB9gT7zDPDddyVXJ2fjXhhC75UPXAh9rO4br56aOhb9DTfwRtZE4HfedHz0zgZY0WM3iNBfeCGwYwfQtm14Qj9+PB8oJx5U17/b4Ogq/Lr6ByFdhd5Jqlv0RDQRwPcATiSiPCK6moiGEJEYVXg6gLUAVgN4FcD1AMAY2wbgEQA/RT4PR+alD/KF7rTobr9dv5eck927g49YJUYpkusTpIxzz43uyi27DLwaG2fO5I1gfowaxdPqrloFnHNO8MbYV16xpl94ITpE0YtYXDZBLHo/oRfH6iZ+bhEsosdwKvnodS16t//Xy6IPyscfA717+4/rmmqkkEWv5RhkjCkGXbQtZwBucFk2HsD44FVLEbws+lgR2RUHDQLeeCP2cr79Vi+mWDBtGrBtmz08TY7a8RL6bt14GOMHH/AY4L171fu4/Xb+EXiFV6qE3i2PTdj4NcaqxNr5oHcKvThfQSx6P265RW+9sFFZ9OJc1akD/P239/aicdor6kaXrl35RyYdhN5Jqlv0hzXFxdaFGVaWR2GFT5jAO0H99RcX3IMHeW9PlYgyFp0KFgDGjtXbp0gH4LyYZIvLS+jlbTt25PV13nwqvFw3Dz8cPV9naDk3YrlR3LZRNY76CX2sFr1bXa691j7eaUni1RirI9Tjx/Oorfbto8sMg3QQ+hSy6NPgbCWZsC36Xbvs7pZBg3gESa9ePCTvwQd5w5xTwIuL1QLi7B3qxuLF/Nt5MXk1Njujn+TwtrJl3ZOLyXi5boYPVw9QURL4uW68cp4LnON7ioeDWwSNn4/ZWZdU6OwTq9BXrw7cdlvirNR0EHonOsNNJogUi+lKQdx89LGI/rp1PHnWU09Z8yZO5N+zZvEPACxfzhsiZVTWL8BHl9JBiLLzYvJ6S3GKrtNv++KLvFPQM8+4l+F2Q+qk9Y2VMB7IKpEtaYs+lYV+1Sr1mMUlRToIvbHo0wg3i1fHjfP778Ctt1rrrl7Nv99/P3g9li61BhiRmT8/WDleQi8fn2ocWee2tWrx13OBKh1DULEqKdeN380Vi9CLsmL10aeS0ItjUUXdMMYTxp15Zmxlh/EgTgehd2J89CmIqjFKnvYLDRwwgCfUeu45ntr0wAGeEVNn20TivNjchF68aai2dbsgGzSInleSQh8EcdzxuG6cbx9ieViNsakm9EHHQXByuPnonRiLPoVxE0K/sMZ337WmiXiaYd1tE4muRa8illwdQcUqnh6iQQTIL5GUn9C/+2600MujRalId9eN10AuJU06CL1q3FwvjEWfRGK16GWI4s9JExZeQu/njvK7EFUXsnOeSrxefNGajiUxm0C0Q8iRHn4EEXr5/PRXRBz7PTzkBmDVOro+3ZIgkUIf1virAA/7TVWcx+nMJeVcL9lx9GlPfj4XkFhExM3iDSLWxcX2MMZEWfS5udE+++bNrdBKIPpiUr2lfPWVunw/142ORb9sWXQP3+uv56mC16+PT+jr1uWx/mJQcC9ieXvxG8jbzx3kNeapartkdqBKhNCHeTxEvEFYjD2biojr4bTTgNmzTdRNwqlQgXfHX7Ag+LY6Fv2GDVxk3LjkErt4LlsWvB4Az2Fy+eVWfhSZatW4X52IC8qff/LGX2dvWi+LXgi9jutBd7lT1Bo1UufOCWvotxYt9NaLpTFWoDr/gDUwilvkkDj3ui6ZZAq9yh8vOrPJHeKSyfHHJ7sG3sjXmNd/aSz6EHFakbroWPT16nlbOatXu1vJQRg+3P3i/uUXe/7zhg2B00+PXk++4Jo2tQ9g7Cf0Ybhu3HDuO9EiF0tj7FNP8cbiiy9WbyP6NLiNKaoaR1UmlVIgqIIRypQJx+2SCj7+ksDvGnNifPRJRHazxOqjB6yBEOJBDI6tQtcSlkXmrbfsbg5xTH4WfTyuGzeeeYYL5BFHuJcVJrE0xtauDbz6anRHKYGf0PtZ9Kkk9Kqom3hJpeMrCXRHjhLnOIGN70bo3RB/TmGh5WphDNiyhacpcI7fKlsp2xKUt61qVfdl8hiUXsgXnTMOPF7XjeqC1hXsfv14r2Hh9tAZLi4Mggi9H7oWvU7PWa+6lQSpFGGTrugKfVhuSw+M0PuxZ481XVzM8488+KDVi1Ug8tM88YS+SOmkEJDxsugrV9YrIx6hj6UxKahYZWfz9A+JTv8ci0XvR506/Fu8lTgR+3JL3GaEvnRhLPo0Qo6WmTfPstadrputW3le8Xvv1S/bb9zLROAl9LquGyezZgFDh8ZdtUMMHZr4hrZYfPR+vPQS8N//8rYPFWJfum9fySTezlFeHC4PD91GViP0KYCIpACAESMsl41T2H75JbjfXvePbd0a+OEH67fzwpk8Odh+BWG5bs44Qz+LZqqQCIu+cmWgb1/35eL86Vr0yUTVGBtWmYcLuo2x4r4zQp9EZKEH3POw9+4dfJAF3T+2fXs+ApHg+++BwYOt37FaSEEt+tJ0o+p2bgqToK6b/Pzw66CLcd3Ej7HoS4Cvvwaefz7+cpxCv2SJ+7o7dgQrW/5j27RxX88Zl922rX0kplhxS1kca9RNOlKSxyLOn5vrxlkX3RTUiUD04gzSy1iXw+XhoWvRi/WSneuGiHoQ0UoiWk1E9yiWP0tEiyKf34hoh7SsSFo2LczKe9KlCx+dJ95Xz7VrE7MuYAl9y5buIXuAewccLzdBLMQbdZNO6DaUhYmfRe8kmULfqRPvqXzppeGVKUadkvt7lGZ0LXpx3yWzwxQRZQJ4EcBZAPIA/ERE0xhjh7p3MsZulda/CYA8MOk+xphmd8UEECTdwKZNvLvy22/zaQB49FH97c85J1jdMjN5VE92Nh9nFeDifc459rj7sAXdD93wv3SmWTPe0e3xx0tun+JGdrrMBKlk0QNA/frhlnfuucCUKcB554VbbqoS1KJPoOtGJ3CzLYDVjLG1AEBEkwD0BuDWj78/gBHhVC8EgjSQTpnCe7EGfV397LPYkitlZlqv8eLPHjwYOO44Omk6XAAAIABJREFUa52lS4HGjYOXHQ+Hg+umYkWeJqIkEedNtw0kNzex9SlpiIALL0x2LUqOoHH0SU6BUBeAfEfkAWinWpGIjgHQEIDc378sEc0HUAjgCcbYVJdtBwMYDAD1w7QkdBtI9+8PPoiHQJWHXQf5j5VT3MpuHK9OFIkS3MPBdZMMxPlzy7kv/s9jjwVmzODfhvSlFMfR9wMwhTEm+0uOYYzlArgUwHNEdJxqQ8bYOMZYLmMst5ZzrNJ4UEXJ/PAD8MYb9nm33cYH644FZwa9c891X3fKFGta/mNloRe+TOc6iUa4FBIRiaIasepwQ9eiz8zkid8S2FPSUAIEjbpJcmPsBgBy60m9yDwV/QDYhiZijG2IfK8FMAt2/33ikXu2Ctq351kdifho9QAPWYyV8uXtVpocVdCpk33dVq3UOU9koZdvcJ2bPawohoUL+QDlbsR6ITKmHrHqcMNP6AXJHHDEEB5pFnXzE4ATiKghEZUBF/Oo6BkiOglANQDfS/OqEVFOZLomgA5w9+0nBpXQy9x7Lxcir/QCfhCp89B07w7873/W78WLeVZJVV5yt3wXJXnTn3QScPXV7su3b+ffVaqUTH1KG365bsQDIJkjkBnCI4Usel9zkTFWSEQ3ApgJIBPAeMbYUiJ6GMB8xpgQ/X4AJjFmMy9PBvAKERWDP1SekKN1wqSwEJj89gEcf2wx2jbYbC3wE/rNm7k1rtto26WLOuXwZmmfV17JUyK8+qo9Zlp0jc/K4jezSuidwp5Kr+9r1vDvVM8Dnuq4/aci7HCD2wuzIa1IIYteS0UYY9MBTHfMe9Dxe6Riu7kAXBJ/hEvm7h249MpIkjA59teZk33fvuiNg0Tm9OxpF3pVx6WaNYG5c63fZ51lr4cQc5XrJojQi0RmJTWg9jXXAB99xBO7GYLjl6VQCL3cRmNIX9Ipjj5doGpVMa9KN7Tf+Zl9YG6ZuXOBDh3i29FVVwF33mn9llMRCJydtD77zP5bdG2vXj16G6fQe7lunn6aR/yUVFxyw4bePYNLA08+aX9Ih4mf5ZaZCXz6KXehGdKfUhx1k1Q+vv5TnIXP3FfQEXm/iJ/q1Xl++oIC90ZQv8ZRETbXsKE1zykCwl/rZdFXqQLcf39sF0gqj7WZTO66C5iqjACOH/GfuoT8zp0LUI/u+C7vmMTs31CyBBX6ZKdASBce+zfh6olnoTs+xQu4AV3xefBCdPzPJ5/snpYA8G9M+/ZbYNEi+wXg/LMT6Y5ZsoQ3DBtKFnHNOPMnRfg8crnOnFlC9TEklp49eae3kSO91ysB102pEnqAh2s/v7w7bsIL+BJdcS3GAQDeRX+8ft2P/gXoDi7thV9+nSOPtJJGCS64gH+LQUu++goYMkQ/L0oQmjQpuRGcDBY+Qi9e3oJmuzakKFWqAD/95O+KM66b2DjpJJ42/q23gNdwLQgMA/AurnmlDZY/OQ0b6nukOHjySeCBB3jaYecoR25DxMnUrw+0U3Yc9ubxx/kwhcJv37YtH8iiNKQbMHB8hF7c5ya68jAjVaJu0pHsbGDAAD6qm5yGpvHd56IbymAmegALFvBBPQRHH83F/OGHrXlbtvBXsPnz9YbrW78+tgpnZvJoHUPpxQh9atOtGw+fLmlM1E18EPHIxvnzgdtvB775hs//DN2xftUBHHN8Nrfa9+0Dli8HevWKLqRmTZ4a4ZRTvDsKff554sc5NaQ3IsWEi5IboU8yyWocqV2b649XqvI4KdVCL2jdmoexC6EHgN/zsrFsFdCz56l8xplnuhcgInGuusp9na5d+cdgcOPCC4HZs10b54yP/jBl8mTg44/tWWtD5rAQeoBHIb7wgtWLv29f3nl1yRLeNulJ7do8OZpbHnGDQYecHODll10XG6E/TKld2zv1SAiUysZYFWXK2MeY2LqVf/tlSDhE+fKmYdSQUIzrxpAoDhuhB3jP/b/+ss97+OHkD+RjMABG6A2J47AS+owMoE4d+7zp04HLLktOfQwGGeG6MUJvCJvDSugFv/0GrFpl/Z42jYeyvvQSj648XAapN6QWwqI3PnpD2ByWQn/CCdGZDj7+GLj+eu6zF/57wf79QI8efFwON/78kwdUGAyxYlw3hkRx2ETdqJD7S51/vjV/zhwrI8FTT/GG3Jkzecp5N7Fv0oT7+s3bgCFWRFu/EXpD2ByWFr2gVSu7C0fQpw/wYyQtzl13AbfcwqdVqewFokHXCL3Bi9mz3YVc9IQ3Qm8Im8Na6AHuwlGllN+0KVq0Cwr8y/N6GBgOb2bPBs44A/j3v9XLhdAbH70hbLSEnoh6ENFKIlpNRPcolg8ioi1EtCjyuUZadgURrYp8rgiz8mEhj/8hOO+86NQTOkJvQjUNbuTl8e+lS9XLhdD7JT81GILi66MnokwALwI4C0AegJ+IaJpi7Nf3GGM3OratDmAEgFwADMCCyLbbQ6l9SPz9t956Oq/Uu3fzRGoGgxPhg/cbr8YIvSFsdCz6tgBWM8bWMsYOAJgEoLdm+d0BfM4Y2xYR988B9Iitqonjzjv1UkHr+N+NRW+IFWPRJ5cmTXibXGlER+jrAvhT+p0XmefkQiJaTERTiOjogNuCiAYT0Xwimr9lyxaNaoVH48Y8f/0llwCjRrmvpxNV4xT6/HzeIWvDBmve119z6+6339zLmTGDJ2IzDXOHD0bok8uyZTzKrjQSVmPsxwAaMMaagVvt/wlaAGNsHGMslzGWW8tv3NYEkJEBTJrE0xm7sX8/j7UfMsR9JD6n0E+dCrzzDnD33da8t9/m315x95dcwsM8E/GG8P33fFAWQ8niMlTsIYTAm8gtQ9joCP0GAEdLv+tF5h2CMbaVMSZGU3gNQGvdbVORKyJNxqr09C+/DLzyCu9ApWLjRp66fv58+/xly3h5+fnB6nLgQLD1dTj1VGDgwPDLNXgjIrLchDzVLfqDB/mb6LPPJrsmhqDoCP1PAE4gooZEVAZAPwDT5BWI6Ejp53kAlkemZwLoRkTViKgagG6ReSnNq68Cs2YBt97Kfz/+eHSG4r/+4qMOMmZPlDZ4MI+quOcenv9epEX++Wfujvn2W706CDHQifQxpAd+D/lUF3rxdikPwGZID3yFnjFWCOBGcIFeDmAyY2wpET1MROdFVhtGREuJ6BcAwwAMimy7DcAj4A+LnwA8HJmX0mRn83jnrl2BHTu4aJ92WvR699wDHHsscNRR0cvWrgU6dQJuvNE+v3x5S8S9XuXFOqkUl799O3DDDd5tCwZ3xH/pJuSpLvSiXiZbtz5z5/q3s61Zw7UkkS47LR89Y2w6Y6wRY+w4xthjkXkPMsamRabvZYw1YYw1Z4x1ZoytkLYdzxg7PvJ5IzGHkTjE6IGTJwNt2kQvX7dOvd3vv6vnv/km8EbkLOzezX/PmhW9XioK/Zw5wNix/KI0BEd0hHLrEJXqQi8MEyP0esyZA3ToYB8HQ0WfPtw7sHJl4upy2PeM1aVqVZ7d0o3//levnFdftaZ37eLtAZ0789/yDS5e871cN0VF/GKKlaAWhLjRg7YzGDjCsnP7T1Nd6BPRXlSaEZF2v/7qvZ64rxL5vxuhD0Dr1rzHrEC2bPr2DV7ezp3W9Acf8Hzkv/7KM2EKvCz6xx/nIZixZs10uo7y84E//nBfX1iiqSpEqY44f27/aap3mBJCbyx6Pfw6yAlED3wj9CnE229zPzUADBpkXxZ0EPnPP7emP/iAXxATJ/I8OwIvi150pRdd63WQL7r9++3L+vQBjjnG/cIUQmVi+2NDnDe3NyLZot+8OfXCLMX1YoReD10BN0KfglSqBIwZA7z3HvDii/Zl3bpxv/s33wBPPAH07Kkuo3t3oFkzu09OROc8/jjwr39Z8596yv2VWVwgQYRX9g87hV48qJzzndsaoY8Ncf78hP7333kajSefLJl66VKaLfpEiKy4P3Ut+kTeV0boY4AIuPhiHnL5wQe8QVVQsSJ3p9x9Nx+msHFja5nIgVOvXnSHq+nTrWlZjL/+mlvZgwfzi/GPP4DlkeBVMUBKkItUdtc4HyDigtu1y3tbI/SxoWvRb97Mv//3v8TXKQilWegTcU2L86Rr0bsZWGFghD5OLrgAuPxy9+VLl/In+vbtfHBygLfEB+Hvv3kj7saNXPQbNwaaN7cs8CAZI2Shd15Yoq+AKgoIiN1HP3o0f2ClEozxtyXnuRs6FDjuOPU2RUX8bS1WdIU+VTFCH4ygFn0iG7uN0JcQVavyDlhvvMEfDB98ELwMufFWfiO4807gn3/0yvAS+rJl+fcll6i3dRP6MWOAk05y3+fNNwNduujVr6RYsIAnsHL2EH75Zd4HQsXjj/O+EbGKvV9jrPO8ppqPPgwh2rEj/jISQTIterGesehLCdWr8wbcrCwevXPddTy/joxqxCvB8OHuy376iQu+X09aHaEH1Be+m49+2DDe3hDPzbJsGXd3lZS4if0EeRtasIB/O8cU1kW26FXHmS5CH6tFv2ABUK0ab99KNRIp9MaiP4zJzOTW4yWXAPXr83k5OdGDlstMm+a+rFcvnnmzYkXeCPzZZ9zSBnjIpsi5Lwv9Qw9xd5BAFnqVmPk1xsZjkfTuDfzf/9lDSxNJdjb/9ks0JiNcLs50GACvt3gQuCGfP9V+08V1ozt+gxOR/+nLL8OpzwcfBIs48yIVhN5Y9KWc9ev5je+0xr/6KnhZRUXAp5/yyJ5hw/i8Zs0s14osMB9+aI/skAVMdQP5Ncaq3iZ0b6BYc/ssWcLDXYOKpLi53HqpquYLoVdZtMceC+Tmeu9TPhcqP30qWfT79wMjRgB79ljzZIszlkzi4vizfIc70ivrwgt54EMYJELoRZnGojccQr74e/YEGjTgPWbnzeNpGAYMAJ5/3hJvwcUXe5crREn4952WpLi48vP5G4VAZXUJ8XMT44ICHiki9yfQFW6xb520zM8+y4+rqIg/0MaOtef710GcBzeLXuVHF/NUlpfOOK8qoWfMeotxCkIyhX7sWJ68bPRoa54sRLH0jhbnKAyhF/t3SzUSlEQKvZ8RIgY9SqRFH8IpN4SNHGrZrp29AevAAfvNV7GifrkixbJMQQGPCHKOm/vLL9Z0cTG37kRHrt9+AxYt4mGlcsrab74BLr2UT7vl6tm0CVi92h55NHUq99ED7qGdMrfdxr937LAyhwbN268j9JUq2eeJG9frhiwqch+tTH4YCKGaOJE/xGfPDs91s38/F1OdUdPcWL2af5cvb81LRaGP5xhlEiH04nhTwaI3Qp9mlCnDL/J+/bjIP/EEMGECcMcd3Afq5e4ZOjR63ubN/K3BybJl/G2hcWPu/3/0UWsZY0DLltHbCJEHeNvD2LHRFn1uLncLMcYfFh9+aE97K0cWyfvbt88uOgCwbZt1E4mH4ebNQI0a7gKwZg1vs6hZk/92E3ovIfN6S9mzx0qE50QWE/EA/Okn/j1/fnium7JlgTPPBL74IrbtAcs1U6OGNU8WonnzeMe+pUuBusox46IRwheGOO/dG15ZQHIteuOjNygpVw746CM+clWtWvyCevJJ7icOyurVdhEXLF/OE7U99BB/qwjK5Mm8cVi26A8csHz/hYU8DbQzt/nGjTwEVc4K+uijQIUK0Q+BbVLCa2HdH3GE+ngELVvyvg9C4AsLuWgQ8fMpEEKiwkvo1693X6ay6EW7SH5+uI2xYTV4yuIuTz/2GP8/ZszQLytMi14k81MJfUEBD0QIIt6JtOh1hd746A1adOzIv2fP1o+r/+03njNb5oUX3MWsYkV9K+qJJ+z1kN0ye/aoLdbvvgOuugo46yxrnhj20OmHl4X+7LOt0NRPP3Wvk3DxiJQTBQWWOD/0kLWe6s1CcPPN9jxFMs2bu7cXyGKybh3w/vveQq/j908UqthuVWNshQr6ZYqHq7h+du3SS8P98888dFi+XkSeqQyFgj3yCG/LcoYuA/y/+fHH6Pkl1Ri7YIG9Jz1gLHpDQC67jFvMHTvyV+5du4DKldXjw44Z4/4G0KKF+z6ys4Pd3PJNJQv9li3qm/Tdd/n36tX8eP780xJw58Nr+3a7H12Eim7ezIXqzjt524IqVFSECO7cyd1egN3SFA+CDRush54cGdStm3t7gluHq6Ii3nEO4K63vn2tMlRCHyT0UxCPVbhxo3WMfkIv6u1nnX/zjTV+gXiLEQ+wmjW9rzXBGWfwt0NVO4zK6BAPIdX6vXrxN1TnA0YW+rBEX5QjP7B79eKpyUWaC8B0mDIEJCPD7i+tVIkL2YAB3Lcvu2BuvBF45hnr95gx3P3z2288HNON7dutm/ymm/zrJI9GJfLuA0CjRv5vHe+8Yx+0wRm/vW2bXRynTOHfIopl1CjuGho8OHoMX7ks4X6Qb8jt27no1avnHtnkNtLW5s38gXjllcAnn1jzCwuj/dnCRbRvX7TQb9yo3+g5ahR/eDhdTozxz5Ah/G3JjXnzeN3Eg1aIj/xmp3qIyPWTj+GHH/jDsFMnfl0dPGjV7ddfeZ0OHtQbrUzsQ7xlyfUQQs8Y74exerVVB5UhIVyCzlxTsrh7Ca6XS8+JKkpNTK9ZE71e0l03RNSDiFYS0WoiihpfiIhuI6JlRLSYiL4komOkZUVEtCjy8ejyY0gURPzinTePD2r+wAN8frduXARXr+bCf9ddwAkn8AeEsMTffJOvJzp1lSljlfvgg9a029vB2LHWtFeuezfkTmJ//22/aW66yX7jff89/3Zawh98wEcH27bN6hSmGhlM7pW8datl1X/8MXc1yZFIoozt2/lymb59+c07YQJwzjm8/aCggDeOVqlij5QSll1+frSob91qubD27uWupb17ednt2vGe1QAXyzvv5D1Oe/e2l1GxItC+PY+2Ug2HKRAhsUIAhWXvZtELRJz9/v28sfzee/nv006zu7c2bbL+q+nT7cv8QnCFCIv/Q47tr1eP1/V//+M9q887D3j9db5MJfSiQd9p7XtldQX4/UDEz6duSKcqt5F4gMrXrZhOZHoIX6EnokwALwLoCaAxgP5E1Nix2s8AchljzQBMAfB/0rJ9jLEWkc95MCSVGTOsBtBy5bgAqJJ4tWnDGzcvu4yLwLp1vIF1wQKeoGzOHCtyBeDWq+xXlx8I8SD7u//6S/0WIPLoyL18VchtD+Kh4MbMmfbXa2f/BYCLV9++9sFoVOTl8beqoiL+UD3ySGuZLPQ7dkSPPzx3LhfPf/8bGDmSPzwmT+bCM24cF78TT7TWd444lp+v9kk7ERamMxrJKfTOyKe9e7nQCveYGIXN2b7w1192ceve3ZrWFTixnhwlVqECf9CK/0BkdpWPQUa8AZxzDo942r2bvzVedpm1zv79vCOe/IYljyDnlaaksJD3dykosM7BkiVc4GXjQj4X4kHw0kuJywyrY9G3BbCaMbaWMXYAwCQANruBMfY1Y0w8t+YBqBduNQ3JoE4dywIhAi66CDjlFP467rQOf/8deO01Lohbt0b7qcWN7dWxSfiv3XjiCfsNKWjsNDtcGDHCmhZx4p99Fr1elSr8YSbX1dmABvC3IJ3ey02bWknstm3j51UghPn997lFLgafkV1WTzxh9Z3YsMHeUOzs/yCj6sHbpg1/M5kzh4vRQw/xxmvxf+3axR9yH37If+/ezR9mCxZwcZI71QH8DSAjw+qfIaxo576dQi+jK/Tbt3NxPvtsa15+vnvaDLG//futtwC5c9Idd/D2g+HD7a6cBQv4f9avn3WudQMQ3noLuOUWoFUre38YwO4+lK18eTrW9BK+MMY8PwD6AnhN+n05gBc81n8BwP3S70IA88EfAOd7bDc4st78+vXrM0N68Prr3At83XX2+QcP8vkdOjD25ZeMbd7M2Nq1fNn69Yy98AJjdesydvzxjFWrxtjw4YwtXy48yvxTqZI1nZFhXyZ/3n3Xmr75Zvf1VJ/166PnXXgh/27aNFhZzs8zz6jn9+vnvR1jjBUXM9a6dfSyAQMYu+OO+Oql+hxzDP8mss//17+ClVOtGv+vnfNffpmxM85grFEj+7GI7+Jixt57j7Gnn2Zs/37rOtq501r/pZcYGzXK+l2/PmNHHcXYiy+q63LUUbzc007jv/fvZ6xePfs63btHb/fKK9Z0kya8HsOHW/MmT+bzfv2Vsa++4vsQyPVzfiZOZKxKFT796quMffEFL7dOHWud776L/V4EMJ8xF311W3BohQBCD+CyiKDnSPPqRr6PBbAOwHF++2zdunXsR2soUYqLGXvjDcZ27AinPLebpLiYserVLQF99FFr2R9/WNN//snYjz8y9tprjN1+O2MnnWTd9GKdu+6ypjds4AIzZgxjtWrxeePGqevQqBFjTz7pXscNGxi76SY+/eijjC1eHL3OhRcy9s473mIpmD/ffZ0aNazpzz5jbNIkxqZP1xdk5ycrK/Zt5U/Fiu4P5aOOYqxzZ+v3qlXq9Z59lrH77mPs99+t/xzg8267zfp9yy283g895F0fMV22rN4xOMWaMcZ69bJ+d+7MWOPG1u/x463/bMQI93LHj7eE/vnnrfk5Odb0c8/Fc+/EJ/T/AjBT+n0vgHsV63UFsBxAbY+yJgDo67dPI/SHL3/8wS0mgLGxYxmbM4ex99/ny155hbGePRnbvZv/HjOGsYED+bT8QHCyYgVj69bx5WPGMLZypbX+rl3Wevv2Mfb44/xbWLa3387YKacw9tFHfJ2FC9U3cbt2fN/ffst/v/8+L1te5/zzuYVaUMBY796MffIJn3/EEfw4AMY+/9yqz/797qLxyCPWG4/ML7/wZTVrsigBz862prt1Cy7i4vP00/rryvsU51MW0N9+sy9v1kyvrA8/5A8Et3WnTQt2TMcdp66jEGivbcU1yJj1ZiQ+rVpZ023b+tfjootivnXiFvosAGsBNARQBsAvAJo41mkJYA2AExzzqwnrHkBNAKsANPbbpxF6Q1C++Ya/2nuxd6/1IFi4kIuyG4sW8VdzFW++ydjGjYzt2cPdRlu22Jdv2mTtZ9Eibplu28ZYYWF0WStXMvb33+712LSJscGD+Z0qW4uFhYxt3crfIlRs3MhFlDH+8PzhBz49cyZ3s+Xn84fOpZdaZV57rbcIrV/PH5iM8YfY5s1cvFQupssv5/8JY7yuZ5zB3UC//spddKI+jDHWvz/fZtQoxhYsUO+7cmVr+qyz+HZLlljzypVj7MABxj79lLF58/hDsmdP7lpctCi6vOuus6arVuX/16+/Bns4ZGZyYT/33OiHzqefMrZ6Na+nTlktW/K3j5NPdr8W/IhL6Pn26AXgt4iY3xeZ9zCA8yLTXwDYBGBR5DMtMv9UAL9GHg6/ArhaZ39G6A0GNXPncoELk717ub943z7+FrV9O3dNXH89Y2vW8IfCokXu2xcWciU59VTuh168OL765OXx4/zzT/4wZczeDjN2rLXu7Nl8Xo8e3mUW/H979x9aVR3Gcfz9YUslg7lVyHIjlaSQoJRRSoHZDzOJQhBJgrSE/gnSCELpDwn8J5TMMMToF0RYZFKyP5LS+a+pFLac5oaWijqNZRAkak9/fL/X3c1Nd7e5uz0+L7jsnu/56r7Pfe6ec8/3nHPPv53HkxYuNLt0yWzNmlSkd+3q7LdkSc+FuKoqbdiamtIeGKTpsvnzr+w7a1bX3336dNroLl+ePuGfPZum+HbuTBvJuroU64YNacPb015pX1yt0CutH14aGhpsb/crXEIIw9bRo+l7l0q5arpU58+ni75mz+56Vk9LSzr7aPz4a/8fHR3pFNHuZw8VmKUzktrb0zUna9em2032dp1IW1s67bWiAurr0ymZq1eX5766kvaZWY93RYhCH0IIDlyt0MdXIIQQgnNR6EMIwbko9CGE4FwU+hBCcC4KfQghOBeFPoQQnItCH0IIzkWhDyEE54blBVOSzgC/9/Of3wb08dbYbkTMN4aI2b+BxHunmd3e04phWegHQtLe3q4O8ypivjFEzP5dr3hj6iaEEJyLQh9CCM55LPQflHsAZRAx3xgiZv+uS7zu5uhDCCF05fETfQghhCJR6EMIwTk3hV7SXEmHJLVKWlHu8QwWSfWSmiQdkPSrpGW5vUbS95IO55/VuV2S3suvw35J08sbQf9JqpD0k6TGvDxJ0u4c25eSRuX20Xm5Na+fWM5x95ekcZK2SDooqUXSTO95lvRafl83S9osaYy3PEv6WFK7pOaitpLzKmlx7n9Y0uJSxuCi0EuqAN4HngKmAoskTS3vqAbNReB1M5sKzABeybGtAHaY2RRgR16G9BpMyY+XgY1DP+RBswxoKVp+G1hnZncBHcDS3L4U6Mjt63K/kWg98J2Z3QPcR4rdbZ4lTQBeBRrM7F6gAngOf3n+FJjbra2kvEqqAVYBDwIPAKsKG4c+6e1msiPpAcwEthctrwRWlntc1ynWb4EngENAbW6rBQ7l55uARUX9L/cbSQ+gLv8BPAo0AiJdMVjZPefAdmBmfl6Z+6ncMZQYbxVwpPu4PecZmAAcA2py3hqBJz3mGZgINPc3r8AiYFNRe5d+13q4+ERP5xum4HhucyXvqk4DdgPjzexkXnUKKNwa2ctr8S7wBvBfXr4V+MvMLubl4rgux5zXn8v9R5JJwBngkzxd9aGksTjOs5mdANYCfwAnSXnbh+88F5Sa1wHl20uhd0/SLcDXwHIz+7t4naVNvJvzZCU9DbSb2b5yj2UIVQLTgY1mNg34h87decBlnquBZ0kbuTuAsVw5xeHeUOTVS6E/AdQXLdflNhck3UQq8p+b2dbcfFpSbV5fC7Tndg+vxUPAM5KOAl+Qpm/WA+MkVeY+xXFdjjmvrwL+HMoBD4LjwHEz252Xt5AKv+c8Pw4cMbMzZnYB2ErKvec8F5Sa1wHl20uh3wNMyUfrR5EO6Gwr85gGhSQBHwEtZvZO0aptQOHI+2LS3H2h/YX1HZP2AAABDElEQVR89H4GcK5oF3FEMLOVZlZnZhNJudxpZs8DTcCC3K17zIXXYkHuP6I++ZrZKeCYpLtz02PAARznmTRlM0PSzfl9XojZbZ6LlJrX7cAcSdV5T2hObuubch+kGMSDHfOA34A24M1yj2cQ43qYtFu3H/g5P+aR5iZ3AIeBH4Ca3F+kM5DagF9IZzSUPY4BxP8I0JifTwZ+BFqBr4DRuX1MXm7N6yeXe9z9jPV+YG/O9TdAtfc8A28BB4Fm4DNgtLc8A5tJxyAukPbclvYnr8BLOfZW4MVSxhBfgRBCCM55mboJIYTQiyj0IYTgXBT6EEJwLgp9CCE4F4U+hBCci0IfQgjORaEPIQTn/gc2yd4QuPnI2QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr7FnQaGNUoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1au0vyVrNVdp",
        "colab": {}
      },
      "source": [
        "def plot_train_history_accuray(history, title):\n",
        "  accuray = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "  \n",
        "  epochs = range(len(accuracy))\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, accuracy, 'b', label='Training accuray')\n",
        "  plt.plot(epochs, val_accuracy, 'r', label='Validation accuray')\n",
        "  plt.title(title)\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT4sJ6dcNsKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_train_history_accuray(history, \"Accuray for ANN\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re0I_erHdYVF",
        "colab_type": "text"
      },
      "source": [
        "## Test the trained model on test data :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82WVFZQEH_bW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = NN_model.predict(test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80CG8FxGIHM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# round predictions\n",
        "rounded = [round(x[0]) for x in predictions]\n",
        "predictions = rounded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAqyEdrZJFcx",
        "colab_type": "code",
        "outputId": "990a7a6e-fb3d-4420-c2fa-e832a381150d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "score = accuracy_score(test_y ,predictions)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.732620320855615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yeml3ntaU02Q",
        "colab_type": "code",
        "outputId": "7daedc43-134e-423b-91ef-44659e1ed228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "train_X.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>months_as_customer</th>\n",
              "      <th>age</th>\n",
              "      <th>accd_month_since</th>\n",
              "      <th>capital-gains</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>total_claim_amount</th>\n",
              "      <th>injury_claim</th>\n",
              "      <th>property_claim</th>\n",
              "      <th>vehicle_claim</th>\n",
              "      <th>auto_year</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>...</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>272</td>\n",
              "      <td>43</td>\n",
              "      <td>234.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>81070</td>\n",
              "      <td>7370</td>\n",
              "      <td>14740</td>\n",
              "      <td>58960</td>\n",
              "      <td>2006</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>228</td>\n",
              "      <td>44</td>\n",
              "      <td>8.0</td>\n",
              "      <td>66000</td>\n",
              "      <td>-46000</td>\n",
              "      <td>6500</td>\n",
              "      <td>1300</td>\n",
              "      <td>650</td>\n",
              "      <td>4550</td>\n",
              "      <td>2009</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>97</td>\n",
              "      <td>28</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-32600</td>\n",
              "      <td>45270</td>\n",
              "      <td>10060</td>\n",
              "      <td>10060</td>\n",
              "      <td>25150</td>\n",
              "      <td>2006</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1366</th>\n",
              "      <td>108</td>\n",
              "      <td>29</td>\n",
              "      <td>87.0</td>\n",
              "      <td>71400</td>\n",
              "      <td>0</td>\n",
              "      <td>61380</td>\n",
              "      <td>11160</td>\n",
              "      <td>5580</td>\n",
              "      <td>44640</td>\n",
              "      <td>2012</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "      <td>141.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>66880</td>\n",
              "      <td>6080</td>\n",
              "      <td>12160</td>\n",
              "      <td>48640</td>\n",
              "      <td>1996</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 116 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      months_as_customer  age  accd_month_since  ...    0    1    2\n",
              "312                  272   43             234.0  ...  0.0  0.0  1.0\n",
              "8                    228   44               8.0  ...  1.0  0.0  0.0\n",
              "464                   97   28              59.0  ...  0.0  0.0  1.0\n",
              "1366                 108   29              87.0  ...  0.0  1.0  0.0\n",
              "479                    4   34             141.0  ...  0.0  0.0  1.0\n",
              "\n",
              "[5 rows x 116 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iNUUkzpdppT",
        "colab_type": "text"
      },
      "source": [
        "The accuracy of this neural network model is 73%, we notice that using random forest gives us a higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipjdtz2Mu2RI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}